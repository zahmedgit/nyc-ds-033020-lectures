{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "Slack me the answer to the following questions:\n",
    "\n",
    "What is the difference between a bagging and boosting algorithm?\n",
    "\n",
    "What is a reason you might choose one algorithm over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Practicum with Class Imbalance\n",
    "\n",
    "Agenda:\n",
    "- Review class imbalance\n",
    "- Review code for different ways to handle class imbalance\n",
    "- Review code for Random Forest with gridsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /anaconda3/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: imbalanced-learn in /anaconda3/lib/python3.7/site-packages (from imblearn) (0.6.2)\r\n",
      "Requirement already satisfied: numpy>=1.11 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.16.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.14.1)\r\n",
      "Requirement already satisfied: scipy>=0.17 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.4.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /anaconda3/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.22.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-mhtn-ds-042219-lectures/master/Module_4/cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline  \n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.countplot(y, alpha =.80, palette= ['grey','lightgreen'])\n",
    "plt.title('Survivors vs Non-Survivors')\n",
    "plt.ylabel('# Passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Dummy Classifier for Baseline Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6175478065241844"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n",
    "\n",
    "# DummyClassifier to predict only target 0\n",
    "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- What do you think the accuracy score will be for this model?\n",
    "- What do you think the F-1 Score will be for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score:  0.6547085201793722\n",
      "Test F1 score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, dummy_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Class Imbalance\n",
    "\n",
    "In this guide, we will cover 5 tactics for handling imbalanced classes in machine learning:\n",
    "\n",
    "1. Up-sample the minority class\n",
    "2. Down-sample the majority class\n",
    "3. Change your performance metric\n",
    "4. Penalize algorithms (cost-sensitive training)\n",
    "5. Use tree-based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a classification Model with class imbalance\n",
    "\n",
    "Before we start to implement different ways to handle class imbalance, let's fit a basic model to have a better point of comparison.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score:  0.7847533632286996\n",
      "Test F1 score:  0.6619718309859155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = lr_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "results['imbalanced'] = (accuracy_score(y_test, y_pred_test), f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data for handling class imbalances\n",
    "\n",
    "We are goign to change the training dataset to which we fit our model, so we want to bring our training data back together before we make those changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate our training data back together\n",
    "training  = pd.concat([X_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate minority and majority classes\n",
    "deceased = training[training.Survived==0]\n",
    "survived = training[training.Survived==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deceased count: 403\n",
      "survived count: 263\n"
     ]
    }
   ],
   "source": [
    "# Get a class count to understand the class imbalance.\n",
    "print('deceased count: '+ str(len(deceased)))\n",
    "print('survived count: '+ str(len(survived)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling \n",
    "You can change the dataset that you use to build your predictive model to have more balanced data.\n",
    "\n",
    "This change is called sampling your dataset and there are two main methods that you can use to even-up the classes:\n",
    "\n",
    "You can add copies of instances from the under-represented class called over-sampling (or more formally sampling with replacement), or\n",
    "You can delete instances from the over-represented class, called under-sampling.\n",
    "These approaches are often very easy to implement and fast to run. They are an excellent starting point.\n",
    "\n",
    "\n",
    "**Some Rules of Thumb:**\n",
    "- Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)\n",
    "- Consider testing over-sampling when you don’t have a lot of data (tens of thousands of records or less)\n",
    "- Consider testing random and non-random (e.g. stratified) sampling schemes.\n",
    "- Consider testing different resampled ratios (e.g. you don’t have to target a 1:1 ratio in a binary classification problem, try other ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/resampling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "survived_upsampled = resample(survived,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(deceased), # match number in majority class\n",
    "                          random_state=23) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    403\n",
       "0    403\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([deceased, survived_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "upsampled.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have balanced classes, lets see how this can affect the performance of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score:  0.7713004484304933\n",
      "Test F1 score:  0.6577181208053691\n"
     ]
    }
   ],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train = upsampled.Survived\n",
    "X_train = upsampled.drop('Survived', axis=1)\n",
    "\n",
    "\n",
    "# upsampled_dt = DecisionTreeClassifier(max_depth=5)\n",
    "upsampled_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# upsampled_dt.fit(X_train, y_train)\n",
    "upsampled_lr.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# upsampled_pred = upsampled_dt.predict(X_test)\n",
    "upsampled_pred = upsampled_lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, upsampled_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, upsampled_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['upsampled'] = (accuracy_score(y_test, upsampled_pred), f1_score(y_test, upsampled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample majority\n",
    "survived_downsampled = resample(deceased,\n",
    "                                replace = False, # sample without replacement\n",
    "                                n_samples = len(survived), # match minority n\n",
    "                                random_state = 23) # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    263\n",
       "0    263\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine minority and downsampled majority\n",
    "downsampled = pd.concat([survived_downsampled, survived])\n",
    "\n",
    "# checking counts\n",
    "downsampled.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy score:  0.7802690582959642\n",
      "Test F1 score:  0.6711409395973155\n"
     ]
    }
   ],
   "source": [
    "# trying logistic regression again with the balanced dataset\n",
    "y_train = downsampled.Survived\n",
    "X_train = downsampled.drop('Survived', axis=1)\n",
    "\n",
    "\n",
    "# downsampled_dt = DecisionTreeClassifier(max_depth=5)\n",
    "downsampled_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# downsampled_dt.fit(X_train, y_train)\n",
    "downsampled_lr.fit(X_train, y_train)\n",
    "\n",
    "# downsampled_pred = upsampled_dt.predict(X_test)\n",
    "downsampled_pred = downsampled_lr.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, downsampled_pred))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, downsampled_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['downsampled'] = (accuracy_score(y_test, downsampled_pred), f1_score(y_test, downsampled_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling: SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "\n",
    "![alt text](images/smote.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_to_object_array' from 'sklearn.utils' (/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0b052d41b57a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mModule\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mallowing\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcreate\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mscikit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearn\u001b[0m \u001b[0mestimators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/imblearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_docstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubstitution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_neighbors_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_to_object_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_to_object_array' from 'sklearn.utils' (/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sm = SMOTE(sampling_strategy=1.0, random_state=23)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(model, x_train, y_train, test):\n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(test)\n",
    "    # checking accuracy\n",
    "    print('Test Accuracy score: ', accuracy_score(test, prediction))\n",
    "\n",
    "    # checking accuracy\n",
    "    print('Test F1 score: ', f1_score(test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote_dt = DecisionTreeClassifier(max_depth=5)\n",
    "smote_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# smote_dt.fit(X_train, y_train)\n",
    "smote_lr.fit(X_train, y_train)\n",
    "\n",
    "# smote_pred = smote_dt.predict(X_test)\n",
    "smote_pred = smote_lr.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, smote_pred))\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, smote_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['smote'] = (accuracy_score(y_test, smote_pred), f1_score(y_test, smote_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under-sampling: Tomek links\n",
    "\n",
    "Tomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/tomek.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks # doctest: +NORMALIZE_WHITESPACE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TomekLinks()\n",
    "X_res, y_res = tl.fit_resample(X_train, y_train)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.sample_indices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove Tomek links\n",
    "tl = TomekLinks()\n",
    "X_resampled, y_resampled = tl.fit_sample(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "idx_samples_removed = np.setdiff1d(np.arange(X_train.shape[0]),\n",
    "                                   tl.sample_indices_)\n",
    "idx_class_0 = y_resampled == 0\n",
    "plt.scatter(X_resampled[idx_class_0, 1], X_resampled[idx_class_0, 4],\n",
    "            alpha=.8, label='Class #0')\n",
    "plt.scatter(X_resampled[~idx_class_0, 1], X_resampled[~idx_class_0, 4],\n",
    "            alpha=.8, label='Class #1')\n",
    "plt.scatter(X_train[idx_samples_removed, 1], X_train[idx_samples_removed, 4],\n",
    "             alpha=.8, label='Removed samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(idx_samples_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomek_lr = LogisticRegression(solver='liblinear')\n",
    "\n",
    "tomek_lr.fit(X_resampled, y_resampled)\n",
    "\n",
    "tomek_pred = tomek_lr.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, tomek_pred))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, tomek_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['tomek'] = (accuracy_score(y_test, tomek_pred), f1_score(y_test, tomek_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalize Algorithms (Cost-Sensitive Training)\n",
    "The next tactic is to use penalized learning algorithms that increase the cost of classification mistakes on the minority class.\n",
    "\n",
    "During training, we can use the argument `class_weight='balanced'`  to penalize mistakes on the minority class by an amount proportional to how under-represented it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf_weighted = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\n",
    "lr_clf_weighted.fit(X_train, y_train)\n",
    "\n",
    "y_weighted_test = lr_clf_weighted.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, y_weighted_test))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(y_test, y_weighted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['weighted'] = (accuracy_score(y_test, y_weighted_test), f1_score(y_test, y_weighted_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Algorithms\n",
    "\n",
    "Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier using 200 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state = 23, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model to the training data\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "#use the fitted model to predict on the test data\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test Accuracy score: ', accuracy_score(y_test, rfc_pred))\n",
    "\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', f1_score(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['rfc'] = (accuracy_score(y_test, rfc_pred), f1_score(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Your Performance Metric\n",
    "\n",
    "Accuracy is not the metric to use when working with an imbalanced dataset. We have seen that it is misleading.\n",
    "\n",
    "There are metrics that have been designed to tell you a more truthful story when working with imbalanced classes.\n",
    "\n",
    "- Precision: A measure of a classifiers exactness.\n",
    "- Recall: A measure of a classifiers completeness\n",
    "- F1 Score (or F-score): A weighted average of precision and recall.\n",
    "\n",
    "- Kappa (or Cohen’s kappa): Classification accuracy normalized by the imbalance of the classes in the data.\n",
    "- ROC Curves: Like precision and recall, accuracy is divided into sensitivity and specificity and models can be chosen based on the balance thresholds of these values.\n",
    "\n",
    "When using a cross-validation method, you can utilize one of these as the scoring metric when comparing across multiple methods.  \n",
    "\n",
    "This will not change the way a model is fitted, it will just choose a different model as the **best_estimator** based on the scoring metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of all the parameters you want to tune\n",
    "param_grid = { \n",
    "    'n_estimators': [100,200,300,400],\n",
    "    param2: list_of_options,\n",
    "    param3: list_of_options,\n",
    "    param4: list_of_options,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a grid search object and fit it to the data\n",
    "\n",
    "CV_rfc = GridSearchCV(_______, ________, cv=____,n_jobs=____, scoring= ____)\n",
    "CV_rfc.fit(_____, _____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the best params \n",
    "print(CV_rfc.____)\n",
    "\n",
    "\n",
    "#Identify the best score during fitting with cross-validation\n",
    "print(CV_rfc.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test set\n",
    "\n",
    "rfc_pred = ____.best_estimator_.____(_____)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', accuracy_score(_____, _____))\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test F1 score: ', f1_score(_____, _____))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to results dictionary\n",
    "\n",
    "results[____] = (accuracy_score(y_test, _____), f1_score(y_test, _____))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reframe as Anomaly Detection\n",
    "\n",
    "If your class imbalance is very extreme (less than 0.1%), it might be better to treat this as an anomay detection problem than a classification problem.  \n",
    "**Anomaly detection**, a.k.a. outlier detection, is for detecting outliers and rare events. Instead of building a classification model, you'd have a \"profile\" of a normal observation. If a new observation strays too far from that \"normal profile,\" it would be flagged as an anomaly.\n",
    "\n",
    "https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
