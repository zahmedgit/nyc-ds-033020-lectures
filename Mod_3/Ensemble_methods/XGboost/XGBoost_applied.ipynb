{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=10000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.771300\n",
      "F1: 0.666667\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.639739</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010608</td>\n",
       "      <td>0.616502</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000850           0.660168   \n",
       "1              0.639739           0.006761           0.640184   \n",
       "2              0.627790           0.006070           0.629019   \n",
       "3              0.614868           0.010608           0.616502   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   39.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  1.8min finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.08074217, 0.3073236 , 0.32059283, 0.05680747, 0.1521759 ,\n",
       "        0.2489316 , 0.05245428, 0.14621992, 0.26092114, 0.05916171,\n",
       "        0.18126316, 0.29810915, 0.06413178, 0.17080822, 0.28642745,\n",
       "        0.0615541 , 0.17577572, 0.2918725 , 0.07036743, 0.21007729,\n",
       "        0.35554872, 0.07381392, 0.21593823, 0.34495492, 0.0725069 ,\n",
       "        0.20234075, 0.36804018, 0.09416919, 0.26110635, 0.43025203,\n",
       "        0.09022455, 0.27511497, 0.42966766, 0.08905864, 0.24195356,\n",
       "        0.39676661, 0.09199181, 0.28928776, 0.45447063, 0.09376168,\n",
       "        0.25758085, 0.44070764, 0.08887296, 0.26678214, 0.40805478,\n",
       "        0.0604362 , 0.16371646, 0.26815901, 0.05686426, 0.16095386,\n",
       "        0.27086124, 0.05946035, 0.16380143, 0.27954741, 0.068151  ,\n",
       "        0.19939294, 0.31912742, 0.06394157, 0.21026173, 0.37331767,\n",
       "        0.09450197, 0.22900677, 0.35988202, 0.08302183, 0.25342746,\n",
       "        0.43980899, 0.10650196, 0.24532533, 0.41756196, 0.08561864,\n",
       "        0.23976755, 0.38212409, 0.08687692, 0.26698656, 0.47741752,\n",
       "        0.09579272, 0.26726022, 0.46337943, 0.10122447, 0.27088032,\n",
       "        0.43526697, 0.10076141, 0.31508803, 0.51071882, 0.10944624,\n",
       "        0.26300063, 0.43373227, 0.08649569, 0.28000703, 0.49616995,\n",
       "        0.0751338 , 0.2093925 , 0.31828218, 0.0685658 , 0.19185066,\n",
       "        0.34123216, 0.07536755, 0.20354748, 0.31633263, 0.07375269,\n",
       "        0.24435096, 0.34114642, 0.0681426 , 0.19423342, 0.32998261,\n",
       "        0.07048726, 0.21651773, 0.33186746, 0.0751996 , 0.22651129,\n",
       "        0.39845567, 0.07778649, 0.26100187, 0.37973323, 0.07696629,\n",
       "        0.21102386, 0.38213577, 0.09619093, 0.26505661, 0.44348707,\n",
       "        0.08949294, 0.30128288, 0.43528976, 0.0868659 , 0.28548775,\n",
       "        0.45942416, 0.1068964 , 0.32500596, 0.55893602, 0.11755471,\n",
       "        0.31356621, 0.51896896, 0.10520368, 0.31334233, 0.51780481,\n",
       "        0.07315335, 0.21154056, 0.34455781, 0.0675889 , 0.20649223,\n",
       "        0.29734545, 0.06218123, 0.17436886, 0.30012908, 0.06797771,\n",
       "        0.22279778, 0.37262478, 0.08047075, 0.21195297, 0.39323626,\n",
       "        0.08459568, 0.21366997, 0.35911274, 0.07785101, 0.27506752,\n",
       "        0.42890773, 0.09168162, 0.22458286, 0.42762971, 0.09369102,\n",
       "        0.24259515, 0.38429632, 0.08714838, 0.30095673, 0.51273603,\n",
       "        0.10290623, 0.32733755, 0.50320754, 0.09744067, 0.30530043,\n",
       "        0.58267059, 0.17438674, 0.34268093, 0.56946945, 0.11277299,\n",
       "        0.3414928 , 0.54176512, 0.11003118, 0.30800133, 0.46381507,\n",
       "        0.06983538, 0.19190125, 0.34224672, 0.07571111, 0.18843532,\n",
       "        0.33868561, 0.07325034, 0.22787676, 0.34815025, 0.08130975,\n",
       "        0.23669176, 0.34853835, 0.07028289, 0.20948677, 0.36684046,\n",
       "        0.08395061, 0.20536842, 0.32879853, 0.07945147, 0.22727895,\n",
       "        0.39051814, 0.08092012, 0.2272027 , 0.39361997, 0.08427877,\n",
       "        0.23360605, 0.37284942, 0.0894455 , 0.26096067, 0.4212862 ,\n",
       "        0.08540673, 0.2490274 , 0.4164072 , 0.08102646, 0.24981461,\n",
       "        0.42480655, 0.10129361, 0.26546555, 0.46123486, 0.09358897,\n",
       "        0.27291341, 0.44319377, 0.08818221, 0.27379808, 0.48668332,\n",
       "        0.06621656, 0.18521557, 0.3090888 , 0.06448956, 0.18913302,\n",
       "        0.31024923, 0.06502895, 0.18551707, 0.3157949 , 0.07377057,\n",
       "        0.20853567, 0.33469329, 0.07000351, 0.19972692, 0.33607721,\n",
       "        0.07108078, 0.20448217, 0.34050169, 0.0786284 , 0.22215829,\n",
       "        0.39235902, 0.08167977, 0.22934704, 0.39753981, 0.08821421,\n",
       "        0.23003182, 0.37398024, 0.08683362, 0.30578098, 0.46724682,\n",
       "        0.09258304, 0.27770138, 0.47075486, 0.10196095, 0.39866853,\n",
       "        0.56571398, 0.11385984, 0.32737451, 0.53933525, 0.12328415,\n",
       "        0.27100315, 0.45001779, 0.09054074, 0.28239512, 0.44055576,\n",
       "        0.06220036, 0.1806458 , 0.29321423, 0.0608099 , 0.17471676,\n",
       "        0.28710093, 0.06336575, 0.17259555, 0.35023775, 0.10172653,\n",
       "        0.20094614, 0.33031845, 0.06946163, 0.199193  , 0.33803635,\n",
       "        0.07156839, 0.20197692, 0.32344656, 0.07936306, 0.22663903,\n",
       "        0.39081059, 0.08603439, 0.229423  , 0.36985679, 0.07722192,\n",
       "        0.22284675, 0.39295797, 0.09055543, 0.2854506 , 0.43044691,\n",
       "        0.08929982, 0.25134659, 0.40334435, 0.08658342, 0.250982  ,\n",
       "        0.41062655, 0.09558525, 0.27613144, 0.45643258, 0.09690275,\n",
       "        0.26710997, 0.42858467, 0.09448433, 0.26553221, 0.44260864,\n",
       "        0.06316457, 0.19492278, 0.30077324, 0.06308379, 0.17256403,\n",
       "        0.27901163, 0.06051903, 0.17014666, 0.28594761, 0.06937938,\n",
       "        0.20294199, 0.33070478, 0.06955986, 0.19604058, 0.32121501,\n",
       "        0.06820245, 0.20204697, 0.34984775, 0.08505926, 0.23060889,\n",
       "        0.38103137, 0.07714162, 0.22737036, 0.3856626 , 0.07835546,\n",
       "        0.25582614, 0.40920234, 0.09970922, 0.26952186, 0.48452587,\n",
       "        0.09804063, 0.25658283, 0.40203657, 0.08470049, 0.24696016,\n",
       "        0.40496325, 0.09716148, 0.28106799, 0.44905081, 0.09146867,\n",
       "        0.25524936, 0.43232517, 0.09260859, 0.26118188, 0.40788856,\n",
       "        0.06186976, 0.17137489, 0.31209135, 0.06460776, 0.20848389,\n",
       "        0.32827115, 0.07144618, 0.17128439, 0.30190778, 0.07316785,\n",
       "        0.23705645, 0.37394557, 0.07415628, 0.2090219 , 0.32957382,\n",
       "        0.06885777, 0.20231276, 0.34894638, 0.08306079, 0.22803202,\n",
       "        0.3751616 , 0.08109317, 0.21887074, 0.36803298, 0.07620454,\n",
       "        0.23635087, 0.38803682, 0.09374609, 0.24431539, 0.40477366,\n",
       "        0.08398442, 0.24399281, 0.40617876, 0.0835783 , 0.23850212,\n",
       "        0.38588977, 0.09200306, 0.26722784, 0.48168898, 0.09214287,\n",
       "        0.26899953, 0.43201303, 0.08708115, 0.26196651, 0.47222481,\n",
       "        0.06930451, 0.18249922, 0.30233421, 0.06754231, 0.19672866,\n",
       "        0.31000414, 0.06219492, 0.19756651, 0.32081399, 0.08067441,\n",
       "        0.22523494, 0.38488774, 0.07958679, 0.2405478 , 0.38815084,\n",
       "        0.07905984, 0.22719102, 0.36607404, 0.09563341, 0.2610373 ,\n",
       "        0.44292464, 0.09749317, 0.26105638, 0.45147891, 0.09433632,\n",
       "        0.27079058, 0.41242728, 0.0925992 , 0.28177013, 0.45031857,\n",
       "        0.092873  , 0.26261816, 0.43935018, 0.0914238 , 0.2476408 ,\n",
       "        0.42802839, 0.09712787, 0.33916802, 0.58340545, 0.11842313,\n",
       "        0.3389255 , 0.58533139, 0.12206688, 0.31992512, 0.57756987,\n",
       "        0.07064838, 0.23924122, 0.42805247, 0.09695973, 0.2090097 ,\n",
       "        0.3392242 , 0.07404714, 0.2215838 , 0.39906526, 0.09824224,\n",
       "        0.24667182, 0.39647398, 0.08261971, 0.24089723, 0.47961144,\n",
       "        0.10507073, 0.3112823 , 0.50300436, 0.12382784, 0.34247446,\n",
       "        0.6113822 , 0.13269362, 0.36129985, 0.5965457 , 0.1224782 ,\n",
       "        0.39621987, 0.59246197, 0.13206415, 0.37552242, 0.58920007,\n",
       "        0.11682701, 0.36415992, 0.57624459, 0.12255869, 0.31853509,\n",
       "        0.54972477, 0.13703175, 0.43783092, 0.69904327, 0.1279336 ,\n",
       "        0.4373383 , 0.72435722, 0.15676522, 0.42210045, 0.58512321,\n",
       "        0.09044299, 0.23662744, 0.50132656, 0.09652863, 0.30411777,\n",
       "        0.46754689, 0.09710331, 0.26489596, 0.42880001, 0.10338869,\n",
       "        0.26932993, 0.47565541, 0.10190201, 0.25607805, 0.40857687,\n",
       "        0.08089948, 0.27298455, 0.41150832, 0.09166522, 0.28136091,\n",
       "        0.47600341, 0.11002851, 0.24020953, 0.36732326, 0.07739573,\n",
       "        0.22748942, 0.37147965, 0.0856092 , 0.23738041, 0.38365698,\n",
       "        0.08047738, 0.23375268, 0.42278767, 0.09606566, 0.22523537,\n",
       "        0.38407989, 0.08656425, 0.23921051, 0.41086268, 0.08176069,\n",
       "        0.27258625, 0.41791139, 0.08834867, 0.22154102, 0.34873371,\n",
       "        0.05461159, 0.16637511, 0.28810906, 0.06109538, 0.1855494 ,\n",
       "        0.26734548, 0.05497565, 0.15791001, 0.25936909, 0.0621295 ,\n",
       "        0.19383717, 0.3606492 , 0.07801924, 0.20729804, 0.34700737,\n",
       "        0.06752138, 0.1947958 , 0.34350114, 0.07627101, 0.23963161,\n",
       "        0.36587806, 0.07667446, 0.1985086 , 0.33984866, 0.06936417,\n",
       "        0.22458978, 0.35600586, 0.07607341, 0.23864503, 0.4123466 ,\n",
       "        0.0854845 , 0.24933419, 0.41365762, 0.08953652, 0.25660129,\n",
       "        0.4195128 , 0.08723497, 0.26834016, 0.41581364, 0.08177004,\n",
       "        0.25354238, 0.41872072, 0.09046888, 0.22727265, 0.38435268,\n",
       "        0.06022081, 0.18125916, 0.28111105, 0.06448116, 0.17178712,\n",
       "        0.2807364 , 0.05949559, 0.17996039, 0.30708246, 0.07220879,\n",
       "        0.20761709, 0.35499244, 0.07183704, 0.23239188, 0.39124537,\n",
       "        0.08314338, 0.22692881, 0.33945284, 0.07986798, 0.2259398 ,\n",
       "        0.42280946, 0.08805132, 0.25593228, 0.38207378, 0.08215981,\n",
       "        0.22909122, 0.36137738, 0.08132052, 0.24154515, 0.42368307,\n",
       "        0.08947182, 0.23964329, 0.39968076, 0.07894316, 0.23524556,\n",
       "        0.39724302, 0.09059072, 0.27686958, 0.50666609, 0.10011358,\n",
       "        0.32069516, 0.4286901 , 0.08806343, 0.27681112, 0.48273783,\n",
       "        0.09950323, 0.24050336, 0.39348621, 0.07955394, 0.2246603 ,\n",
       "        0.31982298, 0.06371284, 0.19333329, 0.32991834, 0.08569164,\n",
       "        0.22607541, 0.34835277, 0.07197781, 0.21745749, 0.40740056,\n",
       "        0.09349337, 0.20918465, 0.34246354, 0.08101869, 0.23506017,\n",
       "        0.38591809, 0.08260899, 0.22715626, 0.40666041, 0.08219924,\n",
       "        0.26721363, 0.41968832, 0.09525743, 0.30291924, 0.47951689,\n",
       "        0.09070897, 0.307409  , 0.44855533, 0.08486071, 0.27573729,\n",
       "        0.4153029 , 0.09360785, 0.27984667, 0.50952268, 0.10316162,\n",
       "        0.27160649, 0.41353564, 0.08366642, 0.24690714, 0.34201016]),\n",
       " 'std_fit_time': array([0.01008225, 0.03187989, 0.04434765, 0.00437219, 0.00460429,\n",
       "        0.00517122, 0.00125032, 0.00411098, 0.01669783, 0.00310165,\n",
       "        0.01520896, 0.00870733, 0.0030559 , 0.00178721, 0.00205688,\n",
       "        0.00165132, 0.00318912, 0.00310803, 0.00285659, 0.00574298,\n",
       "        0.00551453, 0.00469272, 0.00316434, 0.00433886, 0.00196644,\n",
       "        0.00451983, 0.00835346, 0.01078406, 0.00189822, 0.00663624,\n",
       "        0.00273536, 0.0065106 , 0.0093398 , 0.00578986, 0.00640966,\n",
       "        0.00819413, 0.00127348, 0.00727984, 0.00709285, 0.00244929,\n",
       "        0.00528032, 0.03205663, 0.00146816, 0.01378348, 0.00478278,\n",
       "        0.0011486 , 0.00262998, 0.00475479, 0.00184715, 0.00331158,\n",
       "        0.00545872, 0.00231436, 0.00246811, 0.00743757, 0.00150615,\n",
       "        0.00643788, 0.01562829, 0.00107658, 0.01174867, 0.00783049,\n",
       "        0.03523924, 0.01640533, 0.00799359, 0.00529308, 0.00597312,\n",
       "        0.00784264, 0.01701218, 0.00673013, 0.01927892, 0.00372193,\n",
       "        0.01576481, 0.01375591, 0.0039589 , 0.01121704, 0.03419644,\n",
       "        0.01677344, 0.00660926, 0.00516844, 0.0059198 , 0.00645941,\n",
       "        0.0050884 , 0.00559906, 0.00497147, 0.01073964, 0.00923008,\n",
       "        0.00560902, 0.02972914, 0.0015887 , 0.02669758, 0.01042964,\n",
       "        0.00363451, 0.01507834, 0.00710707, 0.00663826, 0.00734521,\n",
       "        0.00903786, 0.00337303, 0.00516042, 0.00668348, 0.00697362,\n",
       "        0.03997116, 0.00744662, 0.00117203, 0.00366366, 0.01531566,\n",
       "        0.00518715, 0.00621494, 0.00627309, 0.00302406, 0.00647647,\n",
       "        0.01109473, 0.00224101, 0.00416922, 0.0071553 , 0.00629907,\n",
       "        0.00239296, 0.00442296, 0.00611537, 0.00368427, 0.02424834,\n",
       "        0.00812654, 0.03073565, 0.02206346, 0.00204905, 0.01092776,\n",
       "        0.00785956, 0.00827552, 0.00971074, 0.00794516, 0.00833469,\n",
       "        0.01336674, 0.01665736, 0.00807352, 0.0037246 , 0.00305287,\n",
       "        0.00250474, 0.00599835, 0.00564623, 0.00616438, 0.00852491,\n",
       "        0.00929456, 0.00105153, 0.00324006, 0.0134679 , 0.00122462,\n",
       "        0.00172481, 0.01327998, 0.0067079 , 0.00827536, 0.0030303 ,\n",
       "        0.00265358, 0.01111449, 0.00673004, 0.00515402, 0.00655569,\n",
       "        0.0128946 , 0.00473762, 0.00719337, 0.0100945 , 0.00395945,\n",
       "        0.01877097, 0.01478725, 0.00170121, 0.01536897, 0.03056961,\n",
       "        0.01060431, 0.01010787, 0.0232858 , 0.01541481, 0.0083011 ,\n",
       "        0.02886232, 0.04068   , 0.00479442, 0.00749769, 0.00864143,\n",
       "        0.00688675, 0.01118662, 0.00556394, 0.01193584, 0.00587084,\n",
       "        0.00459735, 0.00833635, 0.0041382 , 0.00634229, 0.00500726,\n",
       "        0.00660852, 0.01197107, 0.00926208, 0.00887942, 0.00376985,\n",
       "        0.0065925 , 0.01087727, 0.00363115, 0.00559251, 0.02604062,\n",
       "        0.02547575, 0.00468532, 0.00531362, 0.00208974, 0.00347888,\n",
       "        0.01019973, 0.00364827, 0.00434991, 0.00512157, 0.00716297,\n",
       "        0.00358706, 0.01049015, 0.00291105, 0.00518545, 0.00983304,\n",
       "        0.0020858 , 0.02524099, 0.02109404, 0.00220262, 0.00324742,\n",
       "        0.00656452, 0.00604215, 0.00726101, 0.01941419, 0.00354556,\n",
       "        0.0073788 , 0.00455511, 0.00153915, 0.03906802, 0.02317995,\n",
       "        0.00705442, 0.00775211, 0.02491969, 0.00379906, 0.00462005,\n",
       "        0.00820416, 0.0025547 , 0.00955754, 0.00517504, 0.00438434,\n",
       "        0.00654524, 0.00285065, 0.00222923, 0.00247519, 0.0093389 ,\n",
       "        0.00412534, 0.0025303 , 0.01259831, 0.00156589, 0.00483386,\n",
       "        0.00548641, 0.00339233, 0.00537979, 0.00404886, 0.00710738,\n",
       "        0.00749639, 0.00710496, 0.0020405 , 0.0246585 , 0.03197804,\n",
       "        0.00297995, 0.00591628, 0.0190541 , 0.00351315, 0.07388498,\n",
       "        0.05215827, 0.01241207, 0.0095329 , 0.02092664, 0.01256832,\n",
       "        0.01389206, 0.02162379, 0.00196414, 0.01214003, 0.0133357 ,\n",
       "        0.00292488, 0.010124  , 0.00418365, 0.00286409, 0.00453278,\n",
       "        0.0057687 , 0.00376706, 0.00431216, 0.00403298, 0.0155551 ,\n",
       "        0.00281707, 0.00263067, 0.00166491, 0.00286706, 0.00433173,\n",
       "        0.00381131, 0.01198952, 0.00574923, 0.00055084, 0.00483819,\n",
       "        0.01000676, 0.01136581, 0.00959182, 0.00451036, 0.00113562,\n",
       "        0.00555511, 0.01096542, 0.00371046, 0.0029715 , 0.01691855,\n",
       "        0.00085488, 0.00167192, 0.0045218 , 0.00237918, 0.01201332,\n",
       "        0.01046997, 0.00669249, 0.00931298, 0.01110453, 0.00160398,\n",
       "        0.0055366 , 0.00207472, 0.01107082, 0.01782136, 0.0045145 ,\n",
       "        0.00253402, 0.00444089, 0.00529169, 0.00251025, 0.00328995,\n",
       "        0.00265603, 0.00144754, 0.00275869, 0.00628818, 0.00274611,\n",
       "        0.00469075, 0.00585419, 0.00181923, 0.00471743, 0.0040935 ,\n",
       "        0.00209908, 0.00310851, 0.00465   , 0.00369734, 0.01686969,\n",
       "        0.00888739, 0.00106581, 0.00424479, 0.01140802, 0.00429566,\n",
       "        0.01678536, 0.01834114, 0.00766037, 0.00586317, 0.02441992,\n",
       "        0.00268385, 0.00956504, 0.00536561, 0.00115526, 0.00610211,\n",
       "        0.00879457, 0.00475542, 0.00355478, 0.01128592, 0.00252666,\n",
       "        0.00427711, 0.00497295, 0.00252925, 0.00361211, 0.00248523,\n",
       "        0.00361344, 0.00384139, 0.00986027, 0.00657586, 0.00218147,\n",
       "        0.00693709, 0.00876167, 0.00374909, 0.01065208, 0.00600302,\n",
       "        0.00843383, 0.00943803, 0.0044919 , 0.00372824, 0.00788734,\n",
       "        0.00185703, 0.00658884, 0.01429018, 0.00418541, 0.0086138 ,\n",
       "        0.00463485, 0.00583977, 0.00523484, 0.00996488, 0.00213673,\n",
       "        0.00718698, 0.0057918 , 0.00233589, 0.00223809, 0.00342219,\n",
       "        0.00149388, 0.0038639 , 0.00357334, 0.00379544, 0.00179684,\n",
       "        0.00520058, 0.00320523, 0.00588241, 0.01985292, 0.00216518,\n",
       "        0.00367788, 0.00793339, 0.00088501, 0.01377611, 0.00581691,\n",
       "        0.00964406, 0.00955012, 0.01044064, 0.00854228, 0.00493786,\n",
       "        0.00731188, 0.00272431, 0.00327919, 0.00365733, 0.00601169,\n",
       "        0.01153869, 0.00368575, 0.00414218, 0.00846783, 0.01141022,\n",
       "        0.00869354, 0.0053116 , 0.01403592, 0.0059267 , 0.00921071,\n",
       "        0.00698259, 0.00572414, 0.00586052, 0.00711507, 0.00285402,\n",
       "        0.01416577, 0.00613101, 0.00389228, 0.00376911, 0.00375732,\n",
       "        0.00624802, 0.00525086, 0.01242122, 0.00609706, 0.00217814,\n",
       "        0.01645647, 0.00274498, 0.0296535 , 0.01097811, 0.00512251,\n",
       "        0.00872058, 0.00672481, 0.00390542, 0.0046286 , 0.01432595,\n",
       "        0.003021  , 0.01068028, 0.00680799, 0.01225748, 0.00410696,\n",
       "        0.00534654, 0.00507345, 0.01322627, 0.00588376, 0.00727996,\n",
       "        0.00836907, 0.00340486, 0.00186727, 0.00883307, 0.0233544 ,\n",
       "        0.01009287, 0.00333055, 0.00540741, 0.00595592, 0.01591753,\n",
       "        0.01214774, 0.00517392, 0.00414593, 0.01458378, 0.00858913,\n",
       "        0.01449287, 0.01961485, 0.01141668, 0.00763352, 0.01543086,\n",
       "        0.00465989, 0.00430131, 0.02781793, 0.00610607, 0.00589561,\n",
       "        0.0184441 , 0.00854736, 0.00948529, 0.01013244, 0.00936472,\n",
       "        0.01268896, 0.01214556, 0.00902171, 0.01655956, 0.02072021,\n",
       "        0.00408252, 0.01036124, 0.00537034, 0.0136116 , 0.00716504,\n",
       "        0.00788284, 0.00591789, 0.00319314, 0.01331243, 0.00688729,\n",
       "        0.00608586, 0.04267127, 0.00340593, 0.02998896, 0.0121218 ,\n",
       "        0.00471422, 0.00531587, 0.00421151, 0.00557165, 0.00687441,\n",
       "        0.00879324, 0.01032652, 0.00535271, 0.00336597, 0.00220755,\n",
       "        0.00613558, 0.00413005, 0.00358834, 0.01101021, 0.00651221,\n",
       "        0.0060166 , 0.01183838, 0.00271628, 0.00723864, 0.01081491,\n",
       "        0.00696059, 0.01128847, 0.00540516, 0.02018011, 0.00323356,\n",
       "        0.00347859, 0.02331134, 0.00372355, 0.00161637, 0.00244422,\n",
       "        0.0006145 , 0.00473094, 0.00471036, 0.00613915, 0.00844675,\n",
       "        0.00820679, 0.0013104 , 0.00328929, 0.00825939, 0.00116519,\n",
       "        0.00410805, 0.0039051 , 0.00386444, 0.01688204, 0.01201521,\n",
       "        0.00519403, 0.01031561, 0.01537249, 0.00260098, 0.00442342,\n",
       "        0.00892858, 0.0084235 , 0.00587596, 0.00993875, 0.0023501 ,\n",
       "        0.00514842, 0.01564621, 0.00199074, 0.00773038, 0.00388132,\n",
       "        0.00624755, 0.00893655, 0.007965  , 0.00367832, 0.01503515,\n",
       "        0.03092849, 0.00622518, 0.01069947, 0.00734018, 0.00130006,\n",
       "        0.01800134, 0.00273731, 0.00236857, 0.00599649, 0.00496976,\n",
       "        0.00385468, 0.01037895, 0.00444193, 0.0104456 , 0.00385297,\n",
       "        0.00291702, 0.00171055, 0.00331739, 0.00382306, 0.00338741,\n",
       "        0.00494465, 0.00707647, 0.00205293, 0.0074176 , 0.00915307,\n",
       "        0.00822457, 0.00670828, 0.00520813, 0.0025036 , 0.00342847,\n",
       "        0.02671968, 0.00372154, 0.01580138, 0.00871777, 0.00917011,\n",
       "        0.00798208, 0.00165522, 0.00106006, 0.00162853, 0.00216225,\n",
       "        0.00473525, 0.00881162, 0.00985542, 0.00143781, 0.00561567,\n",
       "        0.00615852, 0.00410272, 0.00548581, 0.01843821, 0.01094682,\n",
       "        0.01791048, 0.0157431 , 0.0039349 , 0.05654503, 0.01995963,\n",
       "        0.02129134, 0.01310605, 0.0094631 , 0.00684711, 0.0078823 ,\n",
       "        0.00686494, 0.00158142, 0.00620252, 0.01339319, 0.01161194,\n",
       "        0.00707075, 0.00472057, 0.00370435, 0.00758448, 0.00861049,\n",
       "        0.00945019, 0.00424327, 0.00907063, 0.00091672, 0.02269861,\n",
       "        0.00383275, 0.00535846, 0.00289836, 0.00890074, 0.00507137,\n",
       "        0.00431199, 0.00833782, 0.00248876, 0.0092943 , 0.00304658,\n",
       "        0.00813336, 0.02319372, 0.01338555, 0.00165266, 0.00728675,\n",
       "        0.00375171, 0.0084151 , 0.02069098, 0.01165453, 0.00247737,\n",
       "        0.00850348, 0.0075871 , 0.00203542, 0.00651408, 0.01089503]),\n",
       " 'mean_score_time': array([0.00522728, 0.00902009, 0.00447674, 0.00318446, 0.00377502,\n",
       "        0.00452466, 0.00291958, 0.0037087 , 0.00548472, 0.00313311,\n",
       "        0.00499802, 0.00708809, 0.0029006 , 0.00410075, 0.00525141,\n",
       "        0.00282459, 0.00428424, 0.00540366, 0.00298162, 0.00502114,\n",
       "        0.00831871, 0.00346713, 0.00672765, 0.00594392, 0.00332055,\n",
       "        0.00443916, 0.00713229, 0.00368156, 0.00581932, 0.00916123,\n",
       "        0.00371079, 0.0053618 , 0.00738487, 0.00350075, 0.0054008 ,\n",
       "        0.00735841, 0.00395403, 0.0064817 , 0.00841694, 0.00356741,\n",
       "        0.0057457 , 0.00990677, 0.00347867, 0.00529842, 0.00708294,\n",
       "        0.00300303, 0.00380111, 0.00474744, 0.00289068, 0.00400414,\n",
       "        0.00516658, 0.00294352, 0.00399795, 0.00494595, 0.0030551 ,\n",
       "        0.00435905, 0.0064189 , 0.00312309, 0.00594716, 0.00663624,\n",
       "        0.00327244, 0.00637045, 0.00839281, 0.00491686, 0.00647759,\n",
       "        0.00795398, 0.00343475, 0.00562186, 0.0072041 , 0.00417881,\n",
       "        0.00601797, 0.00623498, 0.00345888, 0.00689549, 0.00848789,\n",
       "        0.00395942, 0.00825882, 0.00782709, 0.00577388, 0.00627661,\n",
       "        0.00836482, 0.00398116, 0.00909319, 0.00858831, 0.0039834 ,\n",
       "        0.00550752, 0.008109  , 0.00386982, 0.00888295, 0.00876274,\n",
       "        0.00464034, 0.00478377, 0.00478897, 0.00382042, 0.00612197,\n",
       "        0.00871277, 0.00370626, 0.00444293, 0.0056356 , 0.00341692,\n",
       "        0.00507889, 0.00611176, 0.00460401, 0.0047267 , 0.00678172,\n",
       "        0.00307398, 0.00549479, 0.00651932, 0.00312748, 0.00582781,\n",
       "        0.00874906, 0.00434432, 0.00570331, 0.00655422, 0.00320115,\n",
       "        0.0047606 , 0.00696864, 0.0060781 , 0.00561781, 0.00849829,\n",
       "        0.00534492, 0.00592017, 0.00913777, 0.00374627, 0.00747023,\n",
       "        0.00955324, 0.00443306, 0.00868354, 0.01038055, 0.00474663,\n",
       "        0.00617785, 0.00912709, 0.00636644, 0.00655122, 0.0097425 ,\n",
       "        0.00479183, 0.00467048, 0.00558968, 0.00355935, 0.00436759,\n",
       "        0.00517244, 0.00311007, 0.00413499, 0.00583968, 0.00320678,\n",
       "        0.00469027, 0.00659089, 0.00459976, 0.00669518, 0.00639739,\n",
       "        0.00405426, 0.00440459, 0.00720458, 0.00329871, 0.00689864,\n",
       "        0.0072156 , 0.00329518, 0.00513349, 0.00882044, 0.00489297,\n",
       "        0.00496621, 0.00775604, 0.00358419, 0.00736685, 0.01255074,\n",
       "        0.00522742, 0.00828967, 0.01103349, 0.00437703, 0.00664306,\n",
       "        0.01027184, 0.00417109, 0.00716648, 0.01094174, 0.00462303,\n",
       "        0.00653863, 0.00943303, 0.00405526, 0.00689526, 0.00873423,\n",
       "        0.00405273, 0.00492802, 0.00629005, 0.0038795 , 0.00428743,\n",
       "        0.00575128, 0.00386567, 0.00460243, 0.00716429, 0.00402212,\n",
       "        0.00475044, 0.00655947, 0.00326691, 0.00451937, 0.00630536,\n",
       "        0.00665498, 0.0045433 , 0.00669847, 0.0032083 , 0.00494618,\n",
       "        0.00772042, 0.00320501, 0.005689  , 0.00661807, 0.00322313,\n",
       "        0.00493617, 0.00662217, 0.00339046, 0.00551181, 0.00736918,\n",
       "        0.00334826, 0.00598235, 0.00758138, 0.0036756 , 0.00554657,\n",
       "        0.0072928 , 0.00342236, 0.00552726, 0.00787897, 0.00352931,\n",
       "        0.00550771, 0.00753503, 0.00319958, 0.00675926, 0.00766253,\n",
       "        0.00310659, 0.00400195, 0.00622969, 0.00306444, 0.00746732,\n",
       "        0.00569859, 0.0032382 , 0.00673594, 0.00575871, 0.0038487 ,\n",
       "        0.00462232, 0.00658388, 0.00341396, 0.00448251, 0.00597105,\n",
       "        0.00322309, 0.00593882, 0.00708594, 0.0032764 , 0.00518227,\n",
       "        0.0068687 , 0.00363483, 0.00520768, 0.00809278, 0.00342307,\n",
       "        0.00591669, 0.00744653, 0.00427027, 0.00634503, 0.01065478,\n",
       "        0.00481071, 0.00736814, 0.01779432, 0.00440545, 0.01888433,\n",
       "        0.00821257, 0.00491824, 0.00806336, 0.00905848, 0.00393872,\n",
       "        0.00557628, 0.01054363, 0.00381303, 0.00537086, 0.00925341,\n",
       "        0.00431218, 0.00505619, 0.00544448, 0.00344067, 0.004248  ,\n",
       "        0.00515327, 0.0030448 , 0.00403681, 0.005264  , 0.00516362,\n",
       "        0.00459304, 0.00609059, 0.00320439, 0.00596657, 0.00601664,\n",
       "        0.00344276, 0.00463004, 0.00596294, 0.00341024, 0.00506058,\n",
       "        0.0071506 , 0.00345016, 0.00503263, 0.0068758 , 0.00339837,\n",
       "        0.00523701, 0.00738106, 0.00363283, 0.00580368, 0.0086442 ,\n",
       "        0.00371442, 0.00590882, 0.00987291, 0.00368681, 0.00542622,\n",
       "        0.0071713 , 0.00345659, 0.00608234, 0.00858808, 0.00346403,\n",
       "        0.00559888, 0.00778003, 0.00360641, 0.00595999, 0.00762043,\n",
       "        0.00309496, 0.0051281 , 0.0052856 , 0.00311317, 0.00405588,\n",
       "        0.00560398, 0.00301161, 0.00402832, 0.00506215, 0.00314989,\n",
       "        0.00448346, 0.00603704, 0.0031702 , 0.00444341, 0.00652623,\n",
       "        0.00318975, 0.00461121, 0.00595121, 0.00373435, 0.00502086,\n",
       "        0.00686622, 0.00431137, 0.00597   , 0.00869231, 0.00334015,\n",
       "        0.00613327, 0.00853944, 0.00402932, 0.0066802 , 0.00831304,\n",
       "        0.0039577 , 0.00543098, 0.0077632 , 0.00332127, 0.00728574,\n",
       "        0.00691228, 0.00367918, 0.0061934 , 0.00834637, 0.00353317,\n",
       "        0.00538568, 0.00966778, 0.00342493, 0.00562615, 0.00708666,\n",
       "        0.00296459, 0.00394273, 0.00684609, 0.0050036 , 0.00631623,\n",
       "        0.00507622, 0.00366263, 0.00406356, 0.00549836, 0.00440631,\n",
       "        0.00531621, 0.00670257, 0.00339904, 0.0044939 , 0.00599942,\n",
       "        0.00336838, 0.00542307, 0.00585985, 0.0035378 , 0.00502262,\n",
       "        0.00697699, 0.0032485 , 0.00488868, 0.00724936, 0.00328679,\n",
       "        0.00595756, 0.00708957, 0.00380392, 0.0053566 , 0.00743761,\n",
       "        0.00363927, 0.00601578, 0.00806746, 0.00330362, 0.00514083,\n",
       "        0.0075716 , 0.00345392, 0.00610862, 0.00935597, 0.00369277,\n",
       "        0.00557814, 0.00793858, 0.00336623, 0.0061213 , 0.00727234,\n",
       "        0.00606742, 0.00403857, 0.00640779, 0.00365047, 0.00451674,\n",
       "        0.00688133, 0.00342398, 0.00438824, 0.00673542, 0.0032742 ,\n",
       "        0.00587068, 0.00786695, 0.004247  , 0.00890536, 0.00639277,\n",
       "        0.00477777, 0.00657396, 0.00709729, 0.00390401, 0.00611043,\n",
       "        0.00884047, 0.00500913, 0.00612206, 0.00786037, 0.00446777,\n",
       "        0.00689645, 0.00730853, 0.00406785, 0.00666862, 0.00778594,\n",
       "        0.00362391, 0.00551453, 0.00831876, 0.00336637, 0.00532455,\n",
       "        0.00791826, 0.00359235, 0.00751538, 0.01247501, 0.00493436,\n",
       "        0.01049094, 0.01007423, 0.00484843, 0.00692744, 0.01189418,\n",
       "        0.00372553, 0.00677004, 0.00665994, 0.00630021, 0.00528307,\n",
       "        0.00654244, 0.00376205, 0.00867362, 0.00686378, 0.00543075,\n",
       "        0.00572176, 0.00801582, 0.00414329, 0.00628271, 0.0106935 ,\n",
       "        0.00503407, 0.00692258, 0.0092598 , 0.00566101, 0.0099268 ,\n",
       "        0.01279411, 0.00649858, 0.00788016, 0.0152153 , 0.00525031,\n",
       "        0.01133242, 0.01214108, 0.00567546, 0.00824103, 0.0119308 ,\n",
       "        0.00518484, 0.00844679, 0.01144805, 0.00534844, 0.00753455,\n",
       "        0.01420083, 0.00551863, 0.01288991, 0.01623945, 0.00585475,\n",
       "        0.01257362, 0.01628346, 0.00831518, 0.01114302, 0.01075435,\n",
       "        0.00435238, 0.0058836 , 0.01273947, 0.00579185, 0.00804148,\n",
       "        0.00903459, 0.00643673, 0.00745249, 0.00938101, 0.00482464,\n",
       "        0.00716658, 0.00846224, 0.00590739, 0.00565243, 0.00854812,\n",
       "        0.00397806, 0.00656862, 0.00922298, 0.00427933, 0.00705113,\n",
       "        0.0093976 , 0.00644331, 0.00581808, 0.00728869, 0.00363302,\n",
       "        0.00581598, 0.00705538, 0.00420055, 0.00610232, 0.0076438 ,\n",
       "        0.00363803, 0.00593543, 0.00747552, 0.00424786, 0.01132007,\n",
       "        0.00735493, 0.00460138, 0.00572495, 0.01188884, 0.00370173,\n",
       "        0.0077085 , 0.00767784, 0.00361958, 0.00615072, 0.00754085,\n",
       "        0.00296555, 0.00397186, 0.00593705, 0.00296569, 0.00390959,\n",
       "        0.00501032, 0.00299239, 0.0042429 , 0.00555849, 0.00321603,\n",
       "        0.00499277, 0.00671778, 0.00425673, 0.00487919, 0.00824876,\n",
       "        0.00324779, 0.00490127, 0.007651  , 0.00360217, 0.00638399,\n",
       "        0.00673528, 0.00537877, 0.00506349, 0.00729208, 0.00344443,\n",
       "        0.00620422, 0.00686378, 0.00363369, 0.00575457, 0.00795903,\n",
       "        0.00368271, 0.00831103, 0.00957079, 0.00452404, 0.00748115,\n",
       "        0.00732803, 0.0038784 , 0.00727272, 0.00960007, 0.00409255,\n",
       "        0.00646577, 0.00817761, 0.00451469, 0.00518851, 0.00774679,\n",
       "        0.00304198, 0.00699086, 0.00623713, 0.00387249, 0.0046227 ,\n",
       "        0.00591197, 0.00328722, 0.00432544, 0.00672889, 0.00348015,\n",
       "        0.00518055, 0.00759215, 0.00361376, 0.00626879, 0.00710287,\n",
       "        0.00400352, 0.00532002, 0.0066678 , 0.00381203, 0.00630646,\n",
       "        0.01077795, 0.00400934, 0.00558467, 0.00763192, 0.00390539,\n",
       "        0.005509  , 0.00728836, 0.0037065 , 0.00593038, 0.00887713,\n",
       "        0.00515556, 0.00585046, 0.00776258, 0.00363708, 0.00550199,\n",
       "        0.0090951 , 0.0039957 , 0.00626235, 0.02189703, 0.00435925,\n",
       "        0.00701919, 0.00842209, 0.00363135, 0.0070859 , 0.01229253,\n",
       "        0.00623136, 0.00933285, 0.007516  , 0.00381484, 0.00491371,\n",
       "        0.00749612, 0.00338979, 0.00460758, 0.00668364, 0.0047802 ,\n",
       "        0.00529003, 0.00669737, 0.00355821, 0.00607514, 0.00710454,\n",
       "        0.00628824, 0.00503654, 0.00660243, 0.00376673, 0.00594606,\n",
       "        0.00882931, 0.00359693, 0.0054059 , 0.0087626 , 0.00411596,\n",
       "        0.0057404 , 0.01036806, 0.00400496, 0.00881276, 0.01094556,\n",
       "        0.00410933, 0.00702481, 0.016467  , 0.00372305, 0.00640173,\n",
       "        0.00748935, 0.00415573, 0.00802555, 0.01451454, 0.00405369,\n",
       "        0.00633903, 0.00830584, 0.00357537, 0.00649915, 0.00588927]),\n",
       " 'std_score_time': array([3.34879228e-03, 8.83886454e-03, 1.04303199e-04, 6.17478429e-04,\n",
       "        4.85405369e-04, 1.42022760e-04, 4.99107383e-04, 3.08786306e-04,\n",
       "        1.72581438e-03, 8.29425395e-04, 9.49806014e-04, 3.01210346e-03,\n",
       "        1.67218640e-04, 3.83761892e-04, 2.34736380e-04, 2.02482565e-04,\n",
       "        7.30330594e-04, 1.92275655e-04, 9.02332147e-05, 3.76960641e-04,\n",
       "        3.74782132e-03, 7.16110694e-04, 2.89601381e-03, 1.42281257e-04,\n",
       "        5.04687940e-04, 1.16780078e-04, 9.80981059e-04, 4.97455497e-04,\n",
       "        4.25038825e-04, 1.22952157e-03, 1.70106295e-04, 9.86895111e-05,\n",
       "        2.33685121e-04, 1.08132265e-04, 3.79311884e-04, 4.31522443e-04,\n",
       "        5.19924411e-04, 6.97864375e-04, 6.46558718e-04, 3.54275568e-05,\n",
       "        3.46104998e-04, 4.32740242e-03, 8.19350830e-05, 1.76034265e-04,\n",
       "        2.65572399e-04, 7.55412403e-05, 1.01441880e-04, 9.60988843e-05,\n",
       "        6.69863039e-05, 4.24007253e-04, 6.80126404e-04, 1.00680640e-04,\n",
       "        1.91166485e-04, 1.00816998e-04, 5.52252779e-05, 1.23107073e-04,\n",
       "        1.18300865e-03, 2.75220164e-04, 2.19701308e-03, 1.24685139e-03,\n",
       "        7.81800078e-05, 1.57788017e-03, 3.30810710e-03, 3.49040531e-03,\n",
       "        1.06883143e-03, 2.64349952e-03, 1.61232993e-04, 9.63776953e-04,\n",
       "        8.69693807e-04, 1.14895015e-03, 1.18463834e-03, 7.87833406e-05,\n",
       "        2.17327427e-04, 1.73937595e-03, 7.80482924e-04, 7.65165984e-04,\n",
       "        3.01740021e-03, 5.67278467e-04, 1.94770284e-03, 1.54385224e-03,\n",
       "        1.50491890e-03, 4.34449247e-04, 2.48682162e-03, 3.91360361e-04,\n",
       "        3.88171356e-04, 1.00816660e-04, 5.29111682e-04, 6.84829094e-04,\n",
       "        2.90137817e-03, 1.37521323e-03, 2.20232712e-03, 5.13169341e-04,\n",
       "        5.87746309e-04, 7.88453997e-04, 1.70015911e-03, 6.90456320e-03,\n",
       "        1.00022412e-03, 6.71249699e-04, 1.71769201e-04, 5.85542723e-04,\n",
       "        6.14624352e-04, 6.52042090e-04, 2.96120672e-03, 5.86971733e-04,\n",
       "        9.21732525e-04, 3.65789408e-04, 9.45262881e-04, 1.19863264e-03,\n",
       "        1.35068702e-05, 2.07363155e-03, 1.87165581e-03, 2.23255958e-03,\n",
       "        8.10742210e-04, 1.46160398e-04, 9.80315735e-05, 1.85939261e-04,\n",
       "        1.19124370e-03, 5.24825486e-03, 2.64335116e-04, 6.34555115e-04,\n",
       "        2.08962814e-03, 5.43610652e-04, 1.89152603e-03, 4.19780174e-04,\n",
       "        1.49926079e-03, 2.20850068e-03, 9.42088087e-04, 2.72303148e-03,\n",
       "        1.15379902e-03, 1.15788028e-03, 3.64526037e-04, 1.26480222e-03,\n",
       "        2.74901926e-03, 8.13026825e-04, 2.65625684e-03, 1.98684310e-03,\n",
       "        8.67194777e-04, 6.20207528e-04, 8.00797476e-04, 3.52027822e-04,\n",
       "        2.37838774e-04, 1.81597884e-04, 1.15394947e-04, 6.21600347e-04,\n",
       "        2.01054892e-04, 3.64032196e-04, 8.29100647e-04, 2.84721266e-03,\n",
       "        2.01852679e-03, 8.47301735e-04, 6.84953091e-04, 7.98145356e-05,\n",
       "        1.17310880e-03, 1.40301618e-04, 1.12157451e-03, 9.51748854e-04,\n",
       "        7.46701716e-05, 3.20782416e-04, 2.57787315e-03, 1.04324888e-03,\n",
       "        2.16740265e-04, 1.05820161e-03, 1.52391471e-04, 2.33109793e-03,\n",
       "        2.44791208e-03, 2.25937429e-03, 2.76735288e-03, 1.76232017e-03,\n",
       "        1.41182517e-03, 1.06360554e-03, 4.26208013e-03, 4.27375858e-04,\n",
       "        9.45756151e-04, 2.22925123e-03, 1.00379404e-03, 4.65690970e-04,\n",
       "        1.14824755e-03, 2.91060441e-04, 1.44904515e-03, 1.65947713e-03,\n",
       "        1.15480657e-03, 1.04832389e-03, 1.29368416e-03, 9.61079806e-04,\n",
       "        1.19501271e-04, 2.29736788e-04, 8.04747710e-04, 3.75546302e-04,\n",
       "        2.18554767e-03, 1.65624672e-03, 3.47616362e-04, 9.29300677e-04,\n",
       "        6.09664978e-05, 1.56773195e-04, 3.55104784e-04, 3.13974655e-03,\n",
       "        2.53836637e-04, 1.72347101e-03, 8.36702617e-05, 8.98129338e-05,\n",
       "        7.10849756e-04, 1.00101329e-04, 1.13230460e-03, 3.14391847e-04,\n",
       "        3.77618904e-05, 1.02352782e-04, 2.16153424e-04, 1.62692939e-04,\n",
       "        1.46373894e-04, 1.72773810e-04, 8.93601740e-05, 1.61131463e-03,\n",
       "        5.06645676e-04, 6.63936090e-04, 7.97540916e-04, 4.20209899e-04,\n",
       "        6.33825109e-05, 1.15380010e-04, 5.76206755e-05, 2.73349703e-04,\n",
       "        2.39991578e-04, 7.58777232e-05, 2.14873162e-04, 2.70492853e-03,\n",
       "        4.52884058e-04, 4.16303581e-04, 1.26304307e-04, 1.51754472e-03,\n",
       "        1.56103071e-04, 5.42255969e-03, 3.50917667e-04, 5.43216984e-04,\n",
       "        2.11979652e-03, 8.77764651e-04, 1.26218935e-03, 1.23793722e-04,\n",
       "        1.46351797e-03, 5.52736943e-04, 1.60720155e-04, 3.59715934e-04,\n",
       "        1.25442852e-04, 1.91111317e-03, 2.86145809e-03, 7.35405035e-05,\n",
       "        1.40887992e-04, 9.58643840e-05, 4.68762373e-04, 3.72464692e-04,\n",
       "        1.91550693e-03, 1.85984134e-04, 1.33611181e-03, 1.49572718e-03,\n",
       "        1.42255621e-03, 8.25367587e-04, 3.86594174e-03, 1.81830429e-03,\n",
       "        1.69745423e-03, 1.89647483e-02, 1.07188134e-03, 2.15675180e-02,\n",
       "        1.55831885e-03, 1.21542681e-03, 3.24480309e-03, 1.24838823e-03,\n",
       "        3.37755692e-04, 3.62162973e-04, 2.45826501e-03, 7.21977203e-04,\n",
       "        1.68699636e-04, 4.14703149e-03, 2.45347844e-03, 1.51659464e-03,\n",
       "        6.83721077e-04, 6.37473865e-04, 2.47276578e-04, 4.75231964e-04,\n",
       "        3.34858534e-05, 1.76904696e-04, 1.17902594e-04, 3.75594829e-03,\n",
       "        2.21693774e-04, 2.72149117e-04, 9.44643656e-05, 2.57501428e-03,\n",
       "        5.00048740e-05, 4.41581821e-04, 1.72776823e-04, 1.07397511e-04,\n",
       "        1.30824652e-04, 1.28401153e-04, 3.60837785e-04, 2.82137539e-04,\n",
       "        1.11115295e-04, 3.03802582e-04, 2.11309750e-04, 2.99973689e-04,\n",
       "        4.42626994e-04, 2.43645190e-04, 6.05788658e-04, 1.07908370e-03,\n",
       "        4.01982028e-04, 1.40467334e-03, 5.45937038e-03, 6.49034293e-04,\n",
       "        4.10253588e-04, 4.40225134e-04, 1.87642541e-04, 7.49523528e-04,\n",
       "        1.08175349e-03, 5.58114214e-05, 1.75884642e-04, 3.76936332e-04,\n",
       "        1.94744440e-04, 6.83684638e-04, 1.01423214e-03, 1.82324560e-04,\n",
       "        9.14369367e-04, 4.61779272e-04, 2.89661858e-04, 1.20849822e-04,\n",
       "        8.32406658e-04, 4.92821296e-05, 2.67514570e-04, 8.24393943e-05,\n",
       "        1.01896609e-04, 1.11869493e-04, 2.57981700e-04, 6.35091604e-05,\n",
       "        7.77947804e-05, 9.42045746e-04, 9.55029240e-05, 3.22124497e-04,\n",
       "        1.19093304e-04, 5.17517872e-04, 1.08674107e-04, 1.88375665e-04,\n",
       "        1.24115824e-03, 1.88560089e-03, 1.04419475e-03, 5.68866130e-05,\n",
       "        1.60674356e-03, 2.31628651e-03, 3.06691244e-04, 1.70879608e-03,\n",
       "        1.25010155e-03, 2.67521387e-04, 1.83835190e-04, 5.09932774e-04,\n",
       "        4.50702184e-05, 2.21843163e-03, 1.44544075e-04, 1.83304080e-04,\n",
       "        5.13383643e-04, 4.38719816e-04, 5.77625583e-05, 6.48888303e-05,\n",
       "        2.51108437e-03, 1.47985888e-04, 5.96850261e-04, 1.35629337e-04,\n",
       "        6.12468698e-05, 1.31550808e-04, 1.83848478e-03, 1.70806108e-03,\n",
       "        2.38160600e-03, 8.47587136e-05, 1.06644912e-03, 1.35222090e-04,\n",
       "        4.77708626e-04, 1.43913404e-03, 5.38126217e-04, 1.26196600e-03,\n",
       "        1.37952097e-04, 3.01044032e-05, 1.86153379e-04, 5.35155513e-04,\n",
       "        1.01637046e-03, 9.88958427e-05, 1.35923233e-04, 7.83823116e-05,\n",
       "        2.25452501e-04, 9.54257548e-05, 1.55292078e-04, 6.39692074e-04,\n",
       "        1.88952848e-04, 1.19455524e-03, 1.03125633e-03, 4.89145484e-04,\n",
       "        5.24656105e-05, 1.19190272e-04, 6.23606049e-04, 1.31357698e-03,\n",
       "        1.45720448e-03, 3.58357004e-05, 1.80463393e-04, 1.37646417e-03,\n",
       "        8.13690721e-05, 4.59160922e-04, 1.45745758e-03, 2.87676597e-04,\n",
       "        1.56260752e-04, 7.68783204e-04, 6.25149700e-05, 8.03092160e-04,\n",
       "        4.74867247e-04, 4.34735775e-03, 8.97159958e-05, 1.71910680e-03,\n",
       "        6.50331652e-04, 4.05200848e-04, 2.71380728e-03, 5.84739409e-04,\n",
       "        2.30083866e-04, 1.63513312e-03, 1.37010841e-04, 1.54997528e-03,\n",
       "        1.69705716e-03, 1.13340441e-03, 7.53033225e-03, 3.79532467e-04,\n",
       "        1.26280819e-03, 3.72730957e-03, 1.82853171e-03, 3.61285062e-04,\n",
       "        7.55975321e-04, 1.61795689e-03, 1.93627275e-03, 1.24656927e-03,\n",
       "        7.85013733e-04, 1.72484530e-03, 3.57430157e-03, 3.55911536e-04,\n",
       "        7.84631456e-04, 1.52426442e-03, 5.53654144e-05, 1.37542965e-04,\n",
       "        5.14601194e-05, 6.39390865e-04, 2.35491938e-04, 5.35567737e-05,\n",
       "        4.14176501e-04, 6.85348876e-05, 8.22988934e-04, 1.93019892e-03,\n",
       "        1.02508843e-03, 2.52490914e-03, 6.44661505e-04, 1.37407863e-03,\n",
       "        6.75188710e-04, 2.90492135e-03, 1.48897318e-04, 1.83377948e-03,\n",
       "        1.71867909e-04, 2.72787783e-03, 3.60551752e-04, 1.75832886e-04,\n",
       "        8.79982789e-05, 1.93680967e-03, 1.40295865e-04, 1.59133380e-03,\n",
       "        1.44774970e-04, 4.40292939e-04, 2.20239640e-04, 6.20712880e-04,\n",
       "        2.68056787e-03, 9.45431091e-04, 1.16614509e-03, 1.10009066e-03,\n",
       "        1.07228678e-03, 1.94048796e-03, 2.03724850e-03, 1.03917086e-03,\n",
       "        6.36520230e-04, 5.83643160e-03, 3.90115065e-04, 6.90447234e-03,\n",
       "        1.93447666e-03, 3.21965823e-04, 4.81718650e-04, 1.24360011e-03,\n",
       "        1.92346464e-04, 5.13082955e-04, 7.95176762e-04, 1.61303023e-04,\n",
       "        3.61437577e-04, 1.67751616e-03, 1.57092726e-04, 7.63855747e-04,\n",
       "        2.57209982e-03, 1.10465822e-03, 5.42054383e-03, 4.63998559e-03,\n",
       "        2.02391855e-03, 2.65443959e-03, 1.39948960e-03, 9.74734232e-05,\n",
       "        5.05100878e-04, 4.34821219e-03, 1.29174922e-03, 3.69584284e-03,\n",
       "        2.73906697e-03, 4.29228845e-03, 2.22133368e-03, 3.26900538e-03,\n",
       "        3.48273647e-04, 2.18720485e-03, 6.62622279e-04, 2.67101577e-03,\n",
       "        2.12898580e-04, 1.15767751e-03, 2.50512127e-04, 7.22830721e-04,\n",
       "        2.51850273e-03, 6.53905511e-04, 2.36976727e-03, 1.37219434e-03,\n",
       "        3.17971573e-03, 5.63492018e-04, 2.10981395e-04, 8.24790458e-05,\n",
       "        7.69449843e-04, 4.41605212e-04, 9.46506164e-04, 1.28287864e-03,\n",
       "        3.58933247e-04, 2.01884485e-04, 5.22325473e-04, 2.99689616e-04,\n",
       "        8.97084638e-04, 1.11828039e-02, 4.45531578e-04, 2.10639995e-03,\n",
       "        1.96543369e-04, 4.27914901e-03, 4.07660198e-04, 2.18836908e-03,\n",
       "        6.34511914e-04, 2.08668680e-04, 1.66909399e-03, 1.18829943e-03,\n",
       "        6.42808678e-05, 1.07836636e-04, 1.13270492e-03, 9.46294123e-05,\n",
       "        1.05200371e-04, 1.40442783e-04, 4.22091124e-05, 5.90673896e-04,\n",
       "        1.10217254e-03, 3.18677411e-04, 5.54039870e-04, 8.42850657e-04,\n",
       "        1.20343388e-03, 3.25520261e-04, 3.32022594e-03, 1.45032622e-04,\n",
       "        6.23688176e-04, 1.68168633e-03, 2.81709993e-04, 1.91278959e-03,\n",
       "        2.38840083e-04, 3.01684108e-03, 3.77685776e-04, 1.22112220e-03,\n",
       "        3.68991031e-04, 1.07494450e-03, 4.54247811e-04, 4.56295949e-04,\n",
       "        7.83690488e-04, 8.72212206e-04, 2.39519467e-04, 3.72358666e-03,\n",
       "        2.57750804e-03, 1.16575211e-03, 3.23560652e-03, 5.47355203e-04,\n",
       "        4.17293991e-04, 1.11146526e-03, 2.19019608e-03, 1.08529057e-03,\n",
       "        7.70042356e-04, 1.24811847e-03, 1.33842056e-03, 7.36020049e-05,\n",
       "        5.62892513e-04, 7.60566478e-05, 4.12269795e-03, 1.01229027e-03,\n",
       "        8.83940058e-04, 7.14932130e-04, 9.95694363e-04, 2.53984733e-04,\n",
       "        6.31341041e-05, 1.68445652e-03, 2.03874759e-04, 3.49056440e-04,\n",
       "        1.19419297e-03, 3.24972111e-04, 2.64708261e-03, 1.23531386e-03,\n",
       "        7.00475392e-04, 8.11352954e-04, 4.87823742e-04, 2.26705325e-04,\n",
       "        1.43163188e-03, 3.13326601e-03, 5.32542886e-04, 5.81087726e-04,\n",
       "        5.31517647e-04, 5.20133059e-04, 4.00611145e-04, 3.12729447e-04,\n",
       "        1.19672771e-04, 4.30105832e-04, 1.67441427e-03, 1.25397694e-03,\n",
       "        3.46945963e-04, 3.55851038e-04, 9.96779009e-05, 1.57071492e-04,\n",
       "        1.17504820e-03, 6.10550598e-04, 4.52225065e-04, 9.67614044e-03,\n",
       "        8.09180505e-04, 1.23543416e-03, 4.73410198e-04, 3.72378987e-05,\n",
       "        1.04210570e-03, 3.68474118e-03, 3.51034732e-03, 3.72725425e-03,\n",
       "        1.23535116e-03, 1.29351306e-04, 3.47928852e-04, 2.94532033e-03,\n",
       "        5.82333951e-05, 1.39749135e-04, 9.18894370e-04, 1.80996397e-03,\n",
       "        3.40725325e-04, 2.52558609e-04, 1.16435237e-04, 8.49465236e-04,\n",
       "        6.90838008e-04, 5.00597893e-03, 1.08399613e-04, 1.57492867e-04,\n",
       "        1.82834962e-04, 8.96342397e-04, 1.32469820e-03, 3.06278880e-04,\n",
       "        8.82485042e-05, 1.13869144e-03, 8.25756663e-04, 2.12150098e-04,\n",
       "        2.28837320e-03, 2.41196518e-04, 1.22185764e-03, 2.78402169e-03,\n",
       "        2.61506619e-04, 1.87674424e-03, 1.28510717e-02, 6.24020449e-05,\n",
       "        1.29438807e-03, 2.17972097e-04, 5.62940540e-04, 2.23759311e-03,\n",
       "        1.02714101e-02, 3.20083939e-04, 3.69712775e-04, 6.70506455e-04,\n",
       "        2.22930597e-04, 1.67153598e-03, 3.31450623e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.73469388, 0.72164948, 0.7       , 0.72164948, 0.70833333,\n",
       "        0.72727273, 0.72164948, 0.71428571, 0.73469388, 0.70212766,\n",
       "        0.69387755, 0.70588235, 0.72340426, 0.72727273, 0.72      ,\n",
       "        0.71578947, 0.72916667, 0.75      , 0.69473684, 0.71428571,\n",
       "        0.67307692, 0.69473684, 0.69387755, 0.71287129, 0.72916667,\n",
       "        0.73469388, 0.76      , 0.70833333, 0.71287129, 0.69230769,\n",
       "        0.67368421, 0.74      , 0.7254902 , 0.72164948, 0.74226804,\n",
       "        0.76767677, 0.72164948, 0.70588235, 0.69902913, 0.68817204,\n",
       "        0.73267327, 0.7184466 , 0.70833333, 0.76      , 0.76767677,\n",
       "        0.72164948, 0.70103093, 0.71428571, 0.76767677, 0.72164948,\n",
       "        0.71428571, 0.74226804, 0.72916667, 0.72164948, 0.70212766,\n",
       "        0.70833333, 0.69387755, 0.71578947, 0.72727273, 0.69387755,\n",
       "        0.70833333, 0.70833333, 0.73469388, 0.66666667, 0.70833333,\n",
       "        0.72      , 0.65957447, 0.70833333, 0.72      , 0.71578947,\n",
       "        0.73469388, 0.75510204, 0.66666667, 0.72727273, 0.70588235,\n",
       "        0.69473684, 0.70103093, 0.74      , 0.71578947, 0.73469388,\n",
       "        0.75510204, 0.66666667, 0.72727273, 0.7184466 , 0.69387755,\n",
       "        0.72727273, 0.75247525, 0.71578947, 0.74226804, 0.7755102 ,\n",
       "        0.69387755, 0.71428571, 0.72727273, 0.72164948, 0.70833333,\n",
       "        0.74747475, 0.72164948, 0.73469388, 0.71428571, 0.69473684,\n",
       "        0.72916667, 0.72164948, 0.70833333, 0.70103093, 0.72164948,\n",
       "        0.70833333, 0.70833333, 0.70833333, 0.68041237, 0.70833333,\n",
       "        0.73267327, 0.67368421, 0.6875    , 0.69387755, 0.71578947,\n",
       "        0.70833333, 0.70833333, 0.69473684, 0.72164948, 0.72      ,\n",
       "        0.67368421, 0.68085106, 0.72      , 0.69473684, 0.70103093,\n",
       "        0.74747475, 0.68085106, 0.70833333, 0.72      , 0.68085106,\n",
       "        0.69473684, 0.72727273, 0.69473684, 0.70103093, 0.74747475,\n",
       "        0.70833333, 0.72164948, 0.72164948, 0.68817204, 0.72727273,\n",
       "        0.70833333, 0.70833333, 0.72164948, 0.70833333, 0.68085106,\n",
       "        0.69473684, 0.70833333, 0.68085106, 0.70103093, 0.71578947,\n",
       "        0.70212766, 0.70103093, 0.70833333, 0.69473684, 0.6875    ,\n",
       "        0.70833333, 0.6875    , 0.6875    , 0.71578947, 0.70103093,\n",
       "        0.70833333, 0.6875    , 0.68085106, 0.69473684, 0.69473684,\n",
       "        0.67368421, 0.67368421, 0.70833333, 0.70833333, 0.70103093,\n",
       "        0.6875    , 0.69473684, 0.69473684, 0.74747475, 0.66666667,\n",
       "        0.67368421, 0.6875    , 0.70103093, 0.70103093, 0.69473684,\n",
       "        0.67391304, 0.66666667, 0.70103093, 0.66666667, 0.65957447,\n",
       "        0.71428571, 0.66666667, 0.68041237, 0.6875    , 0.65957447,\n",
       "        0.67368421, 0.6875    , 0.65263158, 0.65957447, 0.70103093,\n",
       "        0.66666667, 0.65979381, 0.70103093, 0.66666667, 0.68041237,\n",
       "        0.69387755, 0.66666667, 0.6875    , 0.71428571, 0.6875    ,\n",
       "        0.67346939, 0.70103093, 0.69473684, 0.68686869, 0.69387755,\n",
       "        0.66666667, 0.66666667, 0.6875    , 0.6875    , 0.68041237,\n",
       "        0.6875    , 0.67368421, 0.68041237, 0.68041237, 0.65979381,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.67346939, 0.69387755,\n",
       "        0.72      , 0.72727273, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.69387755, 0.70103093, 0.71428571, 0.72164948, 0.6875    ,\n",
       "        0.71428571, 0.70588235, 0.70103093, 0.6875    , 0.70707071,\n",
       "        0.68041237, 0.68817204, 0.72916667, 0.6875    , 0.68      ,\n",
       "        0.70588235, 0.6875    , 0.6875    , 0.68686869, 0.70212766,\n",
       "        0.71578947, 0.74226804, 0.68041237, 0.71287129, 0.69902913,\n",
       "        0.65957447, 0.68041237, 0.73076923, 0.70833333, 0.72916667,\n",
       "        0.75510204, 0.69387755, 0.71287129, 0.68627451, 0.67368421,\n",
       "        0.72727273, 0.74509804, 0.6875    , 0.70103093, 0.72727273,\n",
       "        0.71428571, 0.72727273, 0.72727273, 0.69387755, 0.70833333,\n",
       "        0.72164948, 0.6875    , 0.70103093, 0.70103093, 0.70103093,\n",
       "        0.72916667, 0.69387755, 0.69387755, 0.70833333, 0.70833333,\n",
       "        0.6875    , 0.6875    , 0.70833333, 0.66666667, 0.70103093,\n",
       "        0.66666667, 0.69387755, 0.72164948, 0.70103093, 0.6875    ,\n",
       "        0.6875    , 0.69473684, 0.69387755, 0.71428571, 0.70588235,\n",
       "        0.6875    , 0.70103093, 0.7254902 , 0.6875    , 0.68085106,\n",
       "        0.72164948, 0.65306122, 0.7254902 , 0.69306931, 0.67368421,\n",
       "        0.70103093, 0.7254902 , 0.6875    , 0.67368421, 0.70103093,\n",
       "        0.67346939, 0.71428571, 0.74      , 0.67346939, 0.71428571,\n",
       "        0.70833333, 0.6875    , 0.71428571, 0.72164948, 0.68041237,\n",
       "        0.70833333, 0.72164948, 0.66666667, 0.70103093, 0.72164948,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.66666667, 0.70103093,\n",
       "        0.68041237, 0.68686869, 0.71428571, 0.70103093, 0.6875    ,\n",
       "        0.66666667, 0.6875    , 0.66666667, 0.70103093, 0.70707071,\n",
       "        0.67346939, 0.6875    , 0.70103093, 0.6875    , 0.66666667,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.71287129, 0.68041237,\n",
       "        0.70103093, 0.69387755, 0.6875    , 0.69387755, 0.71428571,\n",
       "        0.65979381, 0.72727273, 0.71428571, 0.67346939, 0.71428571,\n",
       "        0.70103093, 0.68041237, 0.71428571, 0.71428571, 0.64583333,\n",
       "        0.6875    , 0.70833333, 0.64583333, 0.69387755, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.70103093, 0.64583333, 0.68041237,\n",
       "        0.70103093, 0.64583333, 0.69387755, 0.71428571, 0.66666667,\n",
       "        0.68041237, 0.68041237, 0.65979381, 0.67346939, 0.70103093,\n",
       "        0.64583333, 0.69387755, 0.66666667, 0.68041237, 0.68041237,\n",
       "        0.69387755, 0.64583333, 0.69387755, 0.71428571, 0.64583333,\n",
       "        0.70103093, 0.69387755, 0.68041237, 0.68041237, 0.69387755,\n",
       "        0.59340659, 0.65979381, 0.65979381, 0.59340659, 0.66666667,\n",
       "        0.6875    , 0.6       , 0.67346939, 0.68041237, 0.61702128,\n",
       "        0.64583333, 0.65979381, 0.60215054, 0.64583333, 0.67346939,\n",
       "        0.62365591, 0.65979381, 0.70103093, 0.61702128, 0.64583333,\n",
       "        0.67346939, 0.63157895, 0.64583333, 0.68686869, 0.65263158,\n",
       "        0.67368421, 0.70103093, 0.63157895, 0.65979381, 0.68      ,\n",
       "        0.63157895, 0.65979381, 0.69387755, 0.65263158, 0.6875    ,\n",
       "        0.70103093, 0.63157895, 0.65979381, 0.68      , 0.63829787,\n",
       "        0.66666667, 0.69387755, 0.65263158, 0.6875    , 0.6875    ]),\n",
       " 'split2_test_score': array([0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.77669903, 0.75      , 0.74285714, 0.75      , 0.75      ,\n",
       "        0.75      , 0.76923077, 0.75728155, 0.76923077, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.76470588, 0.75      , 0.75      ,\n",
       "        0.74509804, 0.76470588, 0.74074074, 0.74509804, 0.73584906,\n",
       "        0.74285714, 0.76923077, 0.74285714, 0.75      , 0.75728155,\n",
       "        0.74509804, 0.74074074, 0.73786408, 0.73584906, 0.74285714,\n",
       "        0.75      , 0.75      , 0.74285714, 0.74509804, 0.75      ,\n",
       "        0.73394495, 0.72897196, 0.72897196, 0.72897196, 0.76923077,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75      , 0.73394495,\n",
       "        0.77227723, 0.7254902 , 0.74285714, 0.76470588, 0.76470588,\n",
       "        0.75728155, 0.77669903, 0.75      , 0.75728155, 0.75728155,\n",
       "        0.74285714, 0.73584906, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.74285714, 0.75728155, 0.75      , 0.72897196, 0.75728155,\n",
       "        0.75728155, 0.73786408, 0.7254902 , 0.73584906, 0.74285714,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74074074, 0.73076923, 0.72897196, 0.72897196, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73786408, 0.75      , 0.74074074,\n",
       "        0.77227723, 0.73076923, 0.73584906, 0.77227723, 0.73076923,\n",
       "        0.75      , 0.75247525, 0.75728155, 0.75      , 0.77227723,\n",
       "        0.75247525, 0.73076923, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75247525, 0.74509804, 0.74509804, 0.74509804, 0.75      ,\n",
       "        0.74285714, 0.73786408, 0.74509804, 0.74285714, 0.75      ,\n",
       "        0.74509804, 0.75728155, 0.71153846, 0.72380952, 0.73584906,\n",
       "        0.73786408, 0.73786408, 0.74285714, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.72380952, 0.72222222, 0.73584906, 0.73786408,\n",
       "        0.75      , 0.74285714, 0.75      , 0.75728155, 0.76190476,\n",
       "        0.76      , 0.76470588, 0.75471698, 0.76      , 0.76470588,\n",
       "        0.75      , 0.74747475, 0.75247525, 0.74509804, 0.75247525,\n",
       "        0.75728155, 0.77669903, 0.76470588, 0.75728155, 0.75728155,\n",
       "        0.73786408, 0.74509804, 0.74509804, 0.73786408, 0.73786408,\n",
       "        0.74285714, 0.73786408, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.74509804, 0.74509804, 0.73786408, 0.73076923, 0.72380952,\n",
       "        0.73786408, 0.74509804, 0.73786408, 0.73786408, 0.74509804,\n",
       "        0.73786408, 0.72380952, 0.71698113, 0.72897196, 0.72380952,\n",
       "        0.73076923, 0.75      , 0.72380952, 0.74509804, 0.75      ,\n",
       "        0.73469388, 0.74747475, 0.76      , 0.73469388, 0.74747475,\n",
       "        0.76      , 0.73469388, 0.76      , 0.77227723, 0.74747475,\n",
       "        0.76      , 0.76      , 0.74747475, 0.76      , 0.76470588,\n",
       "        0.74747475, 0.75      , 0.75728155, 0.74      , 0.74509804,\n",
       "        0.75728155, 0.76      , 0.74509804, 0.73786408, 0.74509804,\n",
       "        0.75      , 0.75      , 0.74      , 0.73786408, 0.72380952,\n",
       "        0.76767677, 0.74509804, 0.73786408, 0.73786408, 0.75      ,\n",
       "        0.75      , 0.74      , 0.73076923, 0.73786408, 0.76      ,\n",
       "        0.73076923, 0.73786408, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76923077, 0.71153846, 0.72222222, 0.76190476, 0.75728155,\n",
       "        0.75      , 0.75      , 0.75      , 0.75728155, 0.75728155,\n",
       "        0.7184466 , 0.73584906, 0.74285714, 0.75728155, 0.73584906,\n",
       "        0.76923077, 0.76923077, 0.76923077, 0.73584906, 0.74074074,\n",
       "        0.72222222, 0.75      , 0.74285714, 0.73584906, 0.75      ,\n",
       "        0.77358491, 0.76190476, 0.73786408, 0.71698113, 0.71559633,\n",
       "        0.74285714, 0.74285714, 0.74285714, 0.75      , 0.75      ,\n",
       "        0.74766355, 0.75471698, 0.72897196, 0.72897196, 0.73076923,\n",
       "        0.72897196, 0.72897196, 0.73786408, 0.74285714, 0.73394495,\n",
       "        0.77227723, 0.74074074, 0.74285714, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75      , 0.75728155, 0.73786408,\n",
       "        0.73076923, 0.74074074, 0.75      , 0.75      , 0.75      ,\n",
       "        0.75      , 0.75      , 0.75728155, 0.74509804, 0.71698113,\n",
       "        0.71698113, 0.74285714, 0.75      , 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75      , 0.71153846, 0.74766355, 0.72897196,\n",
       "        0.73786408, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.75      , 0.7184466 , 0.72897196, 0.72222222, 0.73786408,\n",
       "        0.72897196, 0.72897196, 0.73076923, 0.75      , 0.75      ,\n",
       "        0.76      , 0.76635514, 0.75471698, 0.76      , 0.75471698,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.74285714, 0.76      ,\n",
       "        0.73267327, 0.75728155, 0.74      , 0.75      , 0.75      ,\n",
       "        0.74509804, 0.74509804, 0.76470588, 0.76470588, 0.74766355,\n",
       "        0.73584906, 0.73267327, 0.75      , 0.74285714, 0.73786408,\n",
       "        0.75728155, 0.75      , 0.70588235, 0.74285714, 0.72897196,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.74509804, 0.75728155,\n",
       "        0.74285714, 0.70588235, 0.73394495, 0.74074074, 0.73786408,\n",
       "        0.74285714, 0.73584906, 0.73786408, 0.75      , 0.75      ,\n",
       "        0.74747475, 0.78431373, 0.75471698, 0.74747475, 0.75728155,\n",
       "        0.76190476, 0.74747475, 0.76923077, 0.75728155, 0.75247525,\n",
       "        0.76923077, 0.75471698, 0.74      , 0.75      , 0.75      ,\n",
       "        0.75247525, 0.76470588, 0.76923077, 0.73267327, 0.74766355,\n",
       "        0.74285714, 0.76      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.7184466 , 0.74074074,\n",
       "        0.75247525, 0.73076923, 0.73786408, 0.74509804, 0.75      ,\n",
       "        0.75      , 0.75247525, 0.73786408, 0.74766355, 0.75247525,\n",
       "        0.73786408, 0.74285714, 0.73786408, 0.75      , 0.76190476,\n",
       "        0.71578947, 0.74747475, 0.76      , 0.72340426, 0.74747475,\n",
       "        0.75247525, 0.7628866 , 0.74      , 0.77227723, 0.74747475,\n",
       "        0.76470588, 0.76470588, 0.74226804, 0.75247525, 0.75247525,\n",
       "        0.75510204, 0.75247525, 0.75728155, 0.76767677, 0.76470588,\n",
       "        0.75728155, 0.76      , 0.75247525, 0.73786408, 0.74747475,\n",
       "        0.74509804, 0.75      , 0.75510204, 0.75      , 0.73584906,\n",
       "        0.76      , 0.75247525, 0.74509804, 0.74509804, 0.74509804,\n",
       "        0.75728155, 0.74747475, 0.75728155, 0.73584906, 0.74747475,\n",
       "        0.75247525, 0.73076923, 0.75247525, 0.74509804, 0.75728155]),\n",
       " 'split3_test_score': array([0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78846154, 0.77358491, 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.8       , 0.78846154, 0.7961165 , 0.8       ,\n",
       "        0.77669903, 0.77358491, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.78846154, 0.7961165 , 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.76190476, 0.8       , 0.76190476, 0.76190476, 0.7961165 ,\n",
       "        0.78846154, 0.78095238, 0.78095238, 0.76923077, 0.76923077,\n",
       "        0.8       , 0.77669903, 0.78095238, 0.7961165 , 0.78846154,\n",
       "        0.78504673, 0.76923077, 0.76923077, 0.76923077, 0.78095238,\n",
       "        0.75728155, 0.76923077, 0.80769231, 0.78846154, 0.79245283,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.78095238, 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.8       , 0.8       ,\n",
       "        0.77358491, 0.77358491, 0.79245283, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.78846154, 0.76190476,\n",
       "        0.77358491, 0.80769231, 0.76923077, 0.77358491, 0.8       ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.75471698, 0.76190476,\n",
       "        0.80769231, 0.77358491, 0.78095238, 0.80769231, 0.78846154,\n",
       "        0.8       , 0.8       , 0.76190476, 0.76923077, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78846154, 0.76923077, 0.79245283,\n",
       "        0.78846154, 0.76923077, 0.8       , 0.8       , 0.78095238,\n",
       "        0.78846154, 0.77358491, 0.78095238, 0.78846154, 0.78846154,\n",
       "        0.77669903, 0.80769231, 0.7961165 , 0.8       , 0.79245283,\n",
       "        0.77358491, 0.7961165 , 0.78846154, 0.76923077, 0.7961165 ,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76190476, 0.75471698,\n",
       "        0.80769231, 0.77358491, 0.76923077, 0.80769231, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.80769231, 0.7961165 , 0.7961165 ,\n",
       "        0.76190476, 0.78846154, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.77227723, 0.78095238, 0.79245283, 0.77669903,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.78095238, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.78846154, 0.78095238, 0.8       , 0.78846154, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.80769231, 0.7961165 , 0.76190476,\n",
       "        0.80769231, 0.8       , 0.77358491, 0.7961165 , 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76190476, 0.80769231,\n",
       "        0.78504673, 0.77358491, 0.7961165 , 0.80769231, 0.7961165 ,\n",
       "        0.75510204, 0.75728155, 0.76923077, 0.75510204, 0.75728155,\n",
       "        0.76190476, 0.73267327, 0.75728155, 0.76190476, 0.75247525,\n",
       "        0.75728155, 0.78846154, 0.75728155, 0.75728155, 0.78095238,\n",
       "        0.73786408, 0.75728155, 0.76923077, 0.7961165 , 0.77669903,\n",
       "        0.7961165 , 0.7961165 , 0.78846154, 0.78846154, 0.75471698,\n",
       "        0.78095238, 0.78846154, 0.77227723, 0.77669903, 0.7961165 ,\n",
       "        0.77227723, 0.77669903, 0.8       , 0.76923077, 0.78846154,\n",
       "        0.8       , 0.7961165 , 0.78846154, 0.78846154, 0.78431373,\n",
       "        0.78846154, 0.8       , 0.76923077, 0.78846154, 0.80769231,\n",
       "        0.8       , 0.78504673, 0.79245283, 0.8       , 0.79245283,\n",
       "        0.8       , 0.81132075, 0.78846154, 0.81132075, 0.8       ,\n",
       "        0.79245283, 0.78095238, 0.80373832, 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.80769231, 0.80769231, 0.81904762, 0.77358491,\n",
       "        0.77358491, 0.81132075, 0.78095238, 0.77358491, 0.83018868,\n",
       "        0.78846154, 0.7961165 , 0.80769231, 0.76923077, 0.78095238,\n",
       "        0.81904762, 0.78095238, 0.78095238, 0.81904762, 0.80769231,\n",
       "        0.8       , 0.8       , 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.78095238, 0.77358491, 0.81904762, 0.8       , 0.8       ,\n",
       "        0.78846154, 0.8       , 0.78504673, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.79245283, 0.78846154, 0.81132075,\n",
       "        0.78095238, 0.78846154, 0.8       , 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.82242991, 0.78095238,\n",
       "        0.77358491, 0.80769231, 0.7961165 , 0.77358491, 0.81132075,\n",
       "        0.7961165 , 0.7961165 , 0.80769231, 0.76923077, 0.77358491,\n",
       "        0.81904762, 0.78504673, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.78846154, 0.80769231, 0.76923077, 0.76190476, 0.80769231,\n",
       "        0.79245283, 0.77358491, 0.81904762, 0.76923077, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.76470588, 0.79245283, 0.7961165 , 0.76470588,\n",
       "        0.8       , 0.79245283, 0.77669903, 0.79245283, 0.77669903,\n",
       "        0.77669903, 0.81904762, 0.80769231, 0.7961165 , 0.80769231,\n",
       "        0.77358491, 0.78095238, 0.8       , 0.77358491, 0.78095238,\n",
       "        0.81904762, 0.7961165 , 0.80769231, 0.78095238, 0.77358491,\n",
       "        0.80769231, 0.79245283, 0.79245283, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.76923077, 0.77669903, 0.80769231,\n",
       "        0.79245283, 0.78095238, 0.80769231, 0.81904762, 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.77227723, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.79245283, 0.76470588,\n",
       "        0.80373832, 0.79245283, 0.76470588, 0.79245283, 0.79245283,\n",
       "        0.76470588, 0.80373832, 0.81904762, 0.77669903, 0.80373832,\n",
       "        0.79245283, 0.76470588, 0.79245283, 0.79245283, 0.76923077,\n",
       "        0.79245283, 0.80769231, 0.77227723, 0.8       , 0.79245283,\n",
       "        0.77227723, 0.78846154, 0.79245283, 0.76470588, 0.8       ,\n",
       "        0.81904762, 0.78846154, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.8       , 0.78504673, 0.78846154, 0.81132075, 0.81904762,\n",
       "        0.77083333, 0.78      , 0.75728155, 0.75510204, 0.78      ,\n",
       "        0.75728155, 0.7628866 , 0.78      , 0.75728155, 0.76767677,\n",
       "        0.77227723, 0.75728155, 0.75510204, 0.77227723, 0.75728155,\n",
       "        0.74226804, 0.77227723, 0.75728155, 0.78      , 0.77227723,\n",
       "        0.78095238, 0.78      , 0.76470588, 0.78095238, 0.76767677,\n",
       "        0.76      , 0.78095238, 0.78350515, 0.77227723, 0.78846154,\n",
       "        0.75510204, 0.76470588, 0.78095238, 0.75510204, 0.76      ,\n",
       "        0.78095238, 0.79591837, 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.79245283, 0.75510204, 0.77227723, 0.78846154]),\n",
       " 'split4_test_score': array([0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76470588, 0.80769231, 0.8       , 0.74      , 0.76767677,\n",
       "        0.78      , 0.71428571, 0.75510204, 0.7755102 , 0.78787879,\n",
       "        0.80412371, 0.8       , 0.76      , 0.75510204, 0.76767677,\n",
       "        0.73267327, 0.75789474, 0.76767677, 0.82      , 0.82828283,\n",
       "        0.79591837, 0.78431373, 0.78787879, 0.77227723, 0.76      ,\n",
       "        0.75510204, 0.76      , 0.78787879, 0.80808081, 0.76767677,\n",
       "        0.78787879, 0.78787879, 0.78      , 0.76      , 0.75510204,\n",
       "        0.77227723, 0.80392157, 0.80808081, 0.79207921, 0.79207921,\n",
       "        0.78787879, 0.78      , 0.78      , 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.78846154, 0.80808081, 0.74509804, 0.76      ,\n",
       "        0.79207921, 0.74      , 0.74      , 0.7628866 , 0.76767677,\n",
       "        0.8125    , 0.7755102 , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.72727273, 0.74226804, 0.7628866 , 0.78787879, 0.82      ,\n",
       "        0.8       , 0.76767677, 0.78350515, 0.78787879, 0.75510204,\n",
       "        0.7755102 , 0.78      , 0.78787879, 0.82      , 0.8       ,\n",
       "        0.79207921, 0.78787879, 0.7755102 , 0.76      , 0.7628866 ,\n",
       "        0.76767677, 0.8       , 0.83168317, 0.8       , 0.79207921,\n",
       "        0.80392157, 0.79207921, 0.77227723, 0.75510204, 0.76      ,\n",
       "        0.74226804, 0.80392157, 0.80392157, 0.73469388, 0.7961165 ,\n",
       "        0.76      , 0.73469388, 0.73267327, 0.74747475, 0.76      ,\n",
       "        0.77227723, 0.79591837, 0.72727273, 0.7755102 , 0.75510204,\n",
       "        0.74509804, 0.72727273, 0.7628866 , 0.78      , 0.79591837,\n",
       "        0.80808081, 0.74747475, 0.76      , 0.79591837, 0.76      ,\n",
       "        0.74747475, 0.7628866 , 0.8       , 0.83168317, 0.80808081,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.76      , 0.75510204,\n",
       "        0.7628866 , 0.80392157, 0.82352941, 0.82      , 0.81188119,\n",
       "        0.78431373, 0.79207921, 0.77227723, 0.7628866 , 0.7628866 ,\n",
       "        0.74226804, 0.75247525, 0.80392157, 0.74226804, 0.7254902 ,\n",
       "        0.76      , 0.72164948, 0.73267327, 0.72340426, 0.75510204,\n",
       "        0.79207921, 0.78787879, 0.74      , 0.77669903, 0.77227723,\n",
       "        0.74509804, 0.73267327, 0.72727273, 0.76767677, 0.79207921,\n",
       "        0.82828283, 0.75510204, 0.78431373, 0.79591837, 0.76767677,\n",
       "        0.74      , 0.75      , 0.8       , 0.80392157, 0.82352941,\n",
       "        0.76767677, 0.7961165 , 0.80392157, 0.74747475, 0.76      ,\n",
       "        0.74226804, 0.8       , 0.8       , 0.83168317, 0.78787879,\n",
       "        0.81553398, 0.80392157, 0.7755102 , 0.78      , 0.7628866 ,\n",
       "        0.73913043, 0.75      , 0.74226804, 0.73913043, 0.75789474,\n",
       "        0.74226804, 0.7311828 , 0.73684211, 0.73469388, 0.73913043,\n",
       "        0.75      , 0.7628866 , 0.73913043, 0.74226804, 0.75510204,\n",
       "        0.7311828 , 0.75510204, 0.76      , 0.77083333, 0.78350515,\n",
       "        0.7755102 , 0.73684211, 0.7755102 , 0.77227723, 0.70967742,\n",
       "        0.74747475, 0.77227723, 0.77894737, 0.78      , 0.80392157,\n",
       "        0.77083333, 0.7755102 , 0.77227723, 0.7311828 , 0.75510204,\n",
       "        0.76      , 0.78723404, 0.77227723, 0.78846154, 0.77894737,\n",
       "        0.78      , 0.80392157, 0.71578947, 0.75510204, 0.78      ,\n",
       "        0.76      , 0.80392157, 0.8       , 0.74509804, 0.77227723,\n",
       "        0.76767677, 0.74509804, 0.73267327, 0.74747475, 0.78      ,\n",
       "        0.80412371, 0.81632653, 0.74      , 0.77227723, 0.78      ,\n",
       "        0.74      , 0.74747475, 0.74747475, 0.78431373, 0.80808081,\n",
       "        0.7755102 , 0.75247525, 0.76470588, 0.77227723, 0.74509804,\n",
       "        0.7755102 , 0.77227723, 0.78      , 0.79591837, 0.7755102 ,\n",
       "        0.76470588, 0.77227723, 0.77669903, 0.76      , 0.77227723,\n",
       "        0.78431373, 0.79207921, 0.79591837, 0.78787879, 0.78      ,\n",
       "        0.77227723, 0.77227723, 0.75247525, 0.74747475, 0.77227723,\n",
       "        0.74747475, 0.7961165 , 0.83168317, 0.73267327, 0.77669903,\n",
       "        0.74509804, 0.74      , 0.73786408, 0.75247525, 0.74747475,\n",
       "        0.8       , 0.80808081, 0.76      , 0.77227723, 0.76      ,\n",
       "        0.74747475, 0.7254902 , 0.73469388, 0.76470588, 0.8       ,\n",
       "        0.81632653, 0.74      , 0.76470588, 0.76470588, 0.74747475,\n",
       "        0.75247525, 0.77227723, 0.7961165 , 0.77227723, 0.79591837,\n",
       "        0.78431373, 0.76470588, 0.77227723, 0.76470588, 0.75728155,\n",
       "        0.78      , 0.80769231, 0.81188119, 0.78787879, 0.78431373,\n",
       "        0.76470588, 0.79207921, 0.78      , 0.77227723, 0.78      ,\n",
       "        0.75510204, 0.78846154, 0.7961165 , 0.75510204, 0.78095238,\n",
       "        0.75728155, 0.74226804, 0.7254902 , 0.73267327, 0.73469388,\n",
       "        0.77669903, 0.79591837, 0.72727273, 0.7961165 , 0.76      ,\n",
       "        0.76      , 0.73076923, 0.74      , 0.73267327, 0.77669903,\n",
       "        0.82474227, 0.74747475, 0.74509804, 0.77227723, 0.73469388,\n",
       "        0.75      , 0.74747475, 0.76470588, 0.80392157, 0.8       ,\n",
       "        0.74747475, 0.75728155, 0.78      , 0.74747475, 0.74509804,\n",
       "        0.74      , 0.78846154, 0.77227723, 0.8       , 0.77227723,\n",
       "        0.76470588, 0.77227723, 0.76      , 0.74509804, 0.75247525,\n",
       "        0.73333333, 0.76      , 0.78846154, 0.73913043, 0.74509804,\n",
       "        0.8       , 0.7311828 , 0.73267327, 0.7254902 , 0.74468085,\n",
       "        0.75728155, 0.77669903, 0.73684211, 0.73786408, 0.77669903,\n",
       "        0.72164948, 0.7184466 , 0.73076923, 0.74226804, 0.78095238,\n",
       "        0.79207921, 0.73469388, 0.76190476, 0.77669903, 0.72916667,\n",
       "        0.73786408, 0.75      , 0.74226804, 0.77669903, 0.81188119,\n",
       "        0.73469388, 0.75471698, 0.76923077, 0.72164948, 0.73786408,\n",
       "        0.75247525, 0.74226804, 0.76190476, 0.80392157, 0.75510204,\n",
       "        0.77358491, 0.76923077, 0.74226804, 0.73786408, 0.75247525,\n",
       "        0.68965517, 0.74725275, 0.72916667, 0.6744186 , 0.73913043,\n",
       "        0.72727273, 0.66666667, 0.72916667, 0.72727273, 0.68817204,\n",
       "        0.73469388, 0.74      , 0.70212766, 0.73469388, 0.74      ,\n",
       "        0.65217391, 0.74226804, 0.73267327, 0.69565217, 0.74747475,\n",
       "        0.74      , 0.70212766, 0.74747475, 0.74509804, 0.67391304,\n",
       "        0.74      , 0.75247525, 0.70967742, 0.74747475, 0.75247525,\n",
       "        0.7173913 , 0.73469388, 0.74509804, 0.67391304, 0.74      ,\n",
       "        0.76      , 0.70967742, 0.74747475, 0.77669903, 0.7032967 ,\n",
       "        0.74226804, 0.76470588, 0.68131868, 0.72727273, 0.75247525]),\n",
       " 'mean_test_score': array([0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.78155639, 0.78443445, 0.77583743, 0.77111749, 0.77765161,\n",
       "        0.77907359, 0.77358221, 0.77445474, 0.78818719, 0.77290935,\n",
       "        0.77274226, 0.76460172, 0.77840962, 0.770205  , 0.76987138,\n",
       "        0.76563523, 0.78100533, 0.77800326, 0.7837393 , 0.77965078,\n",
       "        0.76237049, 0.78391369, 0.76817744, 0.76640095, 0.78498353,\n",
       "        0.77722012, 0.77532892, 0.77223649, 0.77219668, 0.75890046,\n",
       "        0.77486162, 0.78346458, 0.77285024, 0.77764973, 0.77804011,\n",
       "        0.78041659, 0.77562854, 0.7662427 , 0.75786221, 0.77638391,\n",
       "        0.77501194, 0.76972595, 0.78541872, 0.78100975, 0.77934056,\n",
       "        0.76674167, 0.7700608 , 0.77685426, 0.78138358, 0.78021883,\n",
       "        0.78349853, 0.78053291, 0.77691026, 0.78144045, 0.77175383,\n",
       "        0.77832886, 0.76499511, 0.76973222, 0.77538507, 0.76217687,\n",
       "        0.76646635, 0.77767684, 0.78106949, 0.76962101, 0.77916807,\n",
       "        0.78036533, 0.76874205, 0.7708413 , 0.77296092, 0.77818363,\n",
       "        0.78747771, 0.78807395, 0.77009461, 0.77786478, 0.76911914,\n",
       "        0.78145069, 0.77194414, 0.77873773, 0.78070169, 0.77665362,\n",
       "        0.78133136, 0.7720362 , 0.77859398, 0.76713939, 0.77563795,\n",
       "        0.78187455, 0.78679402, 0.7775984 , 0.7795711 , 0.78173769,\n",
       "        0.75953072, 0.77628281, 0.78197476, 0.76691721, 0.7780834 ,\n",
       "        0.78226418, 0.76530685, 0.77580353, 0.77542902, 0.76396953,\n",
       "        0.78155306, 0.77746132, 0.75714424, 0.77200192, 0.77447118,\n",
       "        0.75889737, 0.77243656, 0.77391547, 0.77079905, 0.78188993,\n",
       "        0.78398824, 0.75902791, 0.7670857 , 0.76348356, 0.77137149,\n",
       "        0.77248145, 0.77473492, 0.77079352, 0.78035841, 0.7740264 ,\n",
       "        0.77016788, 0.76839968, 0.77729137, 0.76704894, 0.77187847,\n",
       "        0.78272414, 0.77355192, 0.77496018, 0.77678153, 0.77853151,\n",
       "        0.76975786, 0.78180617, 0.77193157, 0.77704802, 0.78397355,\n",
       "        0.76015779, 0.77345843, 0.77923837, 0.76072616, 0.76998127,\n",
       "        0.77470625, 0.7572939 , 0.76842386, 0.76308846, 0.75797393,\n",
       "        0.77515615, 0.78332173, 0.75810776, 0.77182023, 0.78161867,\n",
       "        0.76035776, 0.7643879 , 0.77243656, 0.76662578, 0.77167913,\n",
       "        0.78446075, 0.75963064, 0.77636275, 0.78403921, 0.76711869,\n",
       "        0.77277376, 0.76881983, 0.77093806, 0.77765785, 0.77334513,\n",
       "        0.76538347, 0.77773703, 0.77949806, 0.7636143 , 0.77306128,\n",
       "        0.76582665, 0.77158437, 0.7688311 , 0.78430396, 0.76520946,\n",
       "        0.77576411, 0.77555031, 0.76729343, 0.77763804, 0.77162178,\n",
       "        0.74477841, 0.75163153, 0.76420292, 0.74332913, 0.75179204,\n",
       "        0.76538867, 0.73474545, 0.75425414, 0.75693174, 0.73973098,\n",
       "        0.75554009, 0.7694666 , 0.74351419, 0.75117175, 0.77005522,\n",
       "        0.73663766, 0.75178242, 0.76550865, 0.75642543, 0.76448986,\n",
       "        0.77425413, 0.75613558, 0.76901093, 0.77227468, 0.7368453 ,\n",
       "        0.75900675, 0.77035394, 0.75719229, 0.7636333 , 0.77089197,\n",
       "        0.76632413, 0.75845135, 0.76752826, 0.75250247, 0.76342264,\n",
       "        0.76812745, 0.76607362, 0.76004064, 0.76469647, 0.76568315,\n",
       "        0.7601966 , 0.77385713, 0.74902065, 0.76203404, 0.77494142,\n",
       "        0.77455204, 0.77585293, 0.77379215, 0.77360675, 0.7766596 ,\n",
       "        0.77373944, 0.77236373, 0.76851268, 0.77897388, 0.76806311,\n",
       "        0.77189951, 0.77161159, 0.76675605, 0.77253264, 0.772203  ,\n",
       "        0.76872844, 0.7755909 , 0.77994367, 0.77233237, 0.76358809,\n",
       "        0.75622425, 0.77113299, 0.76383053, 0.7575255 , 0.77635666,\n",
       "        0.78154301, 0.7815036 , 0.76657837, 0.76661936, 0.75344091,\n",
       "        0.76978604, 0.76617361, 0.77712934, 0.77547619, 0.78490416,\n",
       "        0.78664663, 0.77843178, 0.76773511, 0.75454447, 0.77097817,\n",
       "        0.77444388, 0.77824385, 0.77025118, 0.76649686, 0.7720836 ,\n",
       "        0.77083648, 0.77981629, 0.784991  , 0.76064997, 0.77668936,\n",
       "        0.77269016, 0.75946726, 0.77102685, 0.77292678, 0.76424398,\n",
       "        0.77905144, 0.77385118, 0.76711214, 0.77304519, 0.77073093,\n",
       "        0.75939314, 0.76501029, 0.772474  , 0.76448598, 0.76678318,\n",
       "        0.76170214, 0.76322203, 0.77512183, 0.76506322, 0.76542129,\n",
       "        0.76790543, 0.77570304, 0.77107573, 0.76768174, 0.76625613,\n",
       "        0.77661887, 0.76735559, 0.77294284, 0.76798954, 0.76973471,\n",
       "        0.77564125, 0.76537849, 0.77574227, 0.75612181, 0.77325988,\n",
       "        0.76772935, 0.77657427, 0.77433716, 0.76446701, 0.77327045,\n",
       "        0.75383841, 0.77852636, 0.78427631, 0.76335112, 0.77877861,\n",
       "        0.77193573, 0.75552224, 0.76652951, 0.7717362 , 0.75266831,\n",
       "        0.77053142, 0.78433423, 0.75012768, 0.77879384, 0.77090047,\n",
       "        0.76127148, 0.76638594, 0.76921041, 0.75357093, 0.77524461,\n",
       "        0.77321475, 0.7542997 , 0.77275054, 0.76718081, 0.75130886,\n",
       "        0.76558946, 0.7638373 , 0.74977376, 0.77437986, 0.77055297,\n",
       "        0.75821777, 0.76464576, 0.77025847, 0.76065981, 0.7662215 ,\n",
       "        0.76820619, 0.76007721, 0.76392423, 0.77468966, 0.77219822,\n",
       "        0.77050639, 0.76459124, 0.76331716, 0.77468156, 0.77107353,\n",
       "        0.74657582, 0.77671548, 0.77467361, 0.75554252, 0.76815099,\n",
       "        0.78137473, 0.74802047, 0.7686592 , 0.76877585, 0.74153906,\n",
       "        0.76988676, 0.77731422, 0.73909243, 0.76346634, 0.77491034,\n",
       "        0.73463481, 0.76788138, 0.77431274, 0.73949473, 0.76888996,\n",
       "        0.77823304, 0.74431192, 0.76973079, 0.77923653, 0.73713447,\n",
       "        0.75830805, 0.77162627, 0.75001933, 0.76005964, 0.77784859,\n",
       "        0.74600439, 0.76386209, 0.76411666, 0.74155683, 0.76165529,\n",
       "        0.7756291 , 0.74907294, 0.76902631, 0.78196176, 0.75332289,\n",
       "        0.77279301, 0.77075146, 0.74898488, 0.76454689, 0.77853796,\n",
       "        0.70504803, 0.74940426, 0.74362464, 0.70207529, 0.7508649 ,\n",
       "        0.75290591, 0.70680258, 0.74779252, 0.75667955, 0.71726046,\n",
       "        0.74268574, 0.75235625, 0.71588521, 0.74188068, 0.75434221,\n",
       "        0.6968622 , 0.73794019, 0.75435934, 0.72526153, 0.75005824,\n",
       "        0.7566773 , 0.73561089, 0.74536315, 0.75815664, 0.71887686,\n",
       "        0.74294012, 0.76322834, 0.73176219, 0.74752532, 0.75535717,\n",
       "        0.73026127, 0.74559907, 0.7610052 , 0.71752285, 0.74322064,\n",
       "        0.75985297, 0.7269299 , 0.75390639, 0.76020192, 0.72691292,\n",
       "        0.7510766 , 0.76201766, 0.72215166, 0.74561327, 0.7634803 ]),\n",
       " 'std_test_score': array([0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03273069, 0.04599057, 0.0547373 , 0.04642303, 0.05482315,\n",
       "        0.04388121, 0.05455261, 0.04758839, 0.04336797, 0.04544344,\n",
       "        0.05196629, 0.03863991, 0.04263694, 0.03580744, 0.03887191,\n",
       "        0.04689809, 0.04361717, 0.03641738, 0.0585227 , 0.05019065,\n",
       "        0.05516163, 0.05667653, 0.05294913, 0.03971985, 0.05312597,\n",
       "        0.04641209, 0.03241223, 0.04700957, 0.04488938, 0.0422178 ,\n",
       "        0.0622573 , 0.04326594, 0.03772502, 0.05009109, 0.04129586,\n",
       "        0.03561877, 0.04925861, 0.04377258, 0.03839692, 0.0524212 ,\n",
       "        0.04382682, 0.04032782, 0.05489659, 0.03767369, 0.03345849,\n",
       "        0.03593443, 0.05351025, 0.04430119, 0.03545541, 0.05066606,\n",
       "        0.04956074, 0.0444541 , 0.05045162, 0.04877281, 0.04350458,\n",
       "        0.05124428, 0.05035532, 0.04439227, 0.04623476, 0.04733755,\n",
       "        0.05361763, 0.05445022, 0.04158462, 0.06333664, 0.05529273,\n",
       "        0.05041946, 0.06389614, 0.04411639, 0.04814579, 0.04997856,\n",
       "        0.04767595, 0.04617896, 0.06782458, 0.04910404, 0.04478427,\n",
       "        0.05642483, 0.05084327, 0.04128784, 0.05029965, 0.04306147,\n",
       "        0.03658008, 0.0672276 , 0.04971728, 0.03900358, 0.05508906,\n",
       "        0.04526825, 0.03775959, 0.04945065, 0.04179275, 0.03756962,\n",
       "        0.04128302, 0.04971162, 0.04665737, 0.03999018, 0.05441136,\n",
       "        0.04403269, 0.04459091, 0.04615469, 0.0526542 , 0.03863129,\n",
       "        0.04637101, 0.0517326 , 0.03698051, 0.05241084, 0.0464639 ,\n",
       "        0.0343877 , 0.06068138, 0.05034438, 0.05621873, 0.05152372,\n",
       "        0.0473631 , 0.05620849, 0.05466779, 0.04257872, 0.04081701,\n",
       "        0.05418446, 0.04655117, 0.05709616, 0.05728324, 0.04880323,\n",
       "        0.05902526, 0.0604705 , 0.04688203, 0.04978578, 0.05222449,\n",
       "        0.04104733, 0.06201488, 0.05562067, 0.04858247, 0.06149663,\n",
       "        0.04915566, 0.04466579, 0.04835385, 0.05475766, 0.03733395,\n",
       "        0.039115  , 0.03963317, 0.03907409, 0.05161929, 0.04603162,\n",
       "        0.05157095, 0.04538594, 0.0474633 , 0.05031648, 0.04646517,\n",
       "        0.04665978, 0.048993  , 0.04818755, 0.04564168, 0.04885164,\n",
       "        0.04634543, 0.05071825, 0.06068138, 0.04854137, 0.05750185,\n",
       "        0.05357197, 0.04918693, 0.0569363 , 0.04843405, 0.04866569,\n",
       "        0.05532952, 0.05932907, 0.05420968, 0.05889785, 0.06208822,\n",
       "        0.05749608, 0.06625892, 0.05713068, 0.04293706, 0.05194328,\n",
       "        0.06049999, 0.05274671, 0.05679524, 0.04832773, 0.06218835,\n",
       "        0.06887819, 0.05799551, 0.04993215, 0.05239078, 0.05272984,\n",
       "        0.04707405, 0.0538766 , 0.0482042 , 0.04929275, 0.0562234 ,\n",
       "        0.04492711, 0.04492886, 0.05020161, 0.04618762, 0.0453435 ,\n",
       "        0.05166539, 0.05187702, 0.05384478, 0.05640146, 0.04751109,\n",
       "        0.04256625, 0.05607714, 0.04431303, 0.05064044, 0.05133231,\n",
       "        0.05047416, 0.0533002 , 0.05282524, 0.04607996, 0.0349273 ,\n",
       "        0.05493688, 0.04561167, 0.03668677, 0.04967747, 0.0533113 ,\n",
       "        0.05950718, 0.05310962, 0.0521797 , 0.04953592, 0.05314409,\n",
       "        0.05203417, 0.05493604, 0.05062006, 0.05098347, 0.06019372,\n",
       "        0.05331866, 0.05424989, 0.04567577, 0.05534024, 0.05087692,\n",
       "        0.03539035, 0.05116443, 0.04820586, 0.05362337, 0.0600453 ,\n",
       "        0.05408139, 0.05398968, 0.05063795, 0.04879661, 0.04477185,\n",
       "        0.04697878, 0.04457835, 0.05153355, 0.05454508, 0.05082509,\n",
       "        0.05575448, 0.05927299, 0.0421926 , 0.05435914, 0.04957194,\n",
       "        0.03643849, 0.05715325, 0.0506946 , 0.04409704, 0.05686039,\n",
       "        0.04421478, 0.03187124, 0.05246155, 0.04759593, 0.03862123,\n",
       "        0.06923244, 0.0564495 , 0.04313838, 0.04787171, 0.04791829,\n",
       "        0.03532593, 0.05234509, 0.04335553, 0.04313881, 0.0647045 ,\n",
       "        0.0492686 , 0.04947873, 0.05944671, 0.04878066, 0.03807703,\n",
       "        0.03939238, 0.04000173, 0.04495972, 0.04857093, 0.05015794,\n",
       "        0.05118102, 0.05088318, 0.05909302, 0.05407741, 0.04626874,\n",
       "        0.04676504, 0.05099904, 0.04681398, 0.04539126, 0.04803786,\n",
       "        0.04551337, 0.05933248, 0.05238183, 0.05794512, 0.05050671,\n",
       "        0.06248551, 0.04985415, 0.04163115, 0.04639299, 0.04887407,\n",
       "        0.0524112 , 0.05601322, 0.05849578, 0.03949834, 0.04392981,\n",
       "        0.05890431, 0.04703614, 0.04037017, 0.05113429, 0.05752677,\n",
       "        0.03913168, 0.06923447, 0.04605049, 0.04403761, 0.06400556,\n",
       "        0.05212723, 0.05010053, 0.05979136, 0.05852062, 0.04988251,\n",
       "        0.04365701, 0.03704079, 0.03446061, 0.0567517 , 0.04520231,\n",
       "        0.05220157, 0.05060508, 0.04698921, 0.05335357, 0.04639818,\n",
       "        0.04551531, 0.04419545, 0.0572254 , 0.05113588, 0.04166054,\n",
       "        0.04001545, 0.05593426, 0.05468323, 0.0506997 , 0.04885236,\n",
       "        0.06130245, 0.04596595, 0.0492026 , 0.04747878, 0.04366233,\n",
       "        0.05960979, 0.05068092, 0.05539451, 0.04902497, 0.04879298,\n",
       "        0.05638443, 0.05179689, 0.04542236, 0.04688625, 0.06313016,\n",
       "        0.0545367 , 0.0630284 , 0.04738557, 0.04539552, 0.06174471,\n",
       "        0.05032727, 0.04866165, 0.04899342, 0.0603474 , 0.04496437,\n",
       "        0.05245753, 0.03197261, 0.03976912, 0.05552985, 0.04321543,\n",
       "        0.04943542, 0.04426913, 0.04562528, 0.05071827, 0.05146541,\n",
       "        0.04880906, 0.04782464, 0.05313867, 0.05072887, 0.05040496,\n",
       "        0.03768981, 0.05513288, 0.05522477, 0.05267206, 0.0521186 ,\n",
       "        0.0543099 , 0.05592615, 0.04873345, 0.04945336, 0.03897559,\n",
       "        0.04879844, 0.06098662, 0.05445694, 0.0569764 , 0.05081743,\n",
       "        0.05847043, 0.05359018, 0.06190358, 0.03908591, 0.05460859,\n",
       "        0.05890778, 0.05794964, 0.05371772, 0.04729104, 0.05982197,\n",
       "        0.05158125, 0.05542055, 0.0415503 , 0.05721266, 0.05883313,\n",
       "        0.06276121, 0.05090296, 0.04969962, 0.06273879, 0.05101707,\n",
       "        0.05007935, 0.06405857, 0.04830281, 0.05464566, 0.05782756,\n",
       "        0.05222741, 0.05758243, 0.06195087, 0.0532675 , 0.05589468,\n",
       "        0.0510153 , 0.04034404, 0.04028842, 0.06171576, 0.05739267,\n",
       "        0.05180196, 0.06202219, 0.05544577, 0.05075878, 0.04637382,\n",
       "        0.03976787, 0.04280033, 0.05652069, 0.0489542 , 0.04762808,\n",
       "        0.05413167, 0.0507862 , 0.04827062, 0.04508706, 0.03169567,\n",
       "        0.03323744, 0.05496032, 0.05470154, 0.04831559, 0.05143786,\n",
       "        0.05091324, 0.04678491, 0.0463078 , 0.03729708, 0.04734603]),\n",
       " 'rank_test_score': array([ 57,  18, 182, 323, 143, 105, 247, 229,   1, 265, 273, 479, 120,\n",
       "        350, 363, 457,  76, 132,  32,  92, 516,  30, 400, 441,  13, 156,\n",
       "        204, 288, 292, 553, 217,  36, 267, 145, 130,  82, 194, 447, 566,\n",
       "        174, 210, 372,  10,  74,  98, 431, 356, 163,  67,  88,  34,  80,\n",
       "        161,  65, 309, 122, 474, 369, 202, 518, 439, 139,  72, 374, 103,\n",
       "         84, 390, 334, 261, 126,   5,   3, 354, 134, 379,  63, 299, 111,\n",
       "         78, 170,  70, 295, 113, 418, 191,  49,   7, 149,  94,  53, 545,\n",
       "        179,  44, 427, 128,  42, 468, 184, 200, 493,  59, 151, 573, 297,\n",
       "        227, 555, 280, 239, 337,  47,  26, 549, 423, 504, 320, 277, 219,\n",
       "        339,  86, 237, 352, 397, 154, 425, 305,  40, 249, 212, 165, 116,\n",
       "        366,  51, 302, 159,  28, 536, 251, 100, 527, 360, 221, 569, 395,\n",
       "        514, 564, 207,  38, 562, 307,  55, 531, 487, 280, 433, 312,  16,\n",
       "        543, 176,  24, 420, 270, 387, 329, 141, 253, 465, 137,  96, 501,\n",
       "        258, 453, 318, 385,  21, 470, 186, 197, 415, 147, 315, 634, 610,\n",
       "        490, 640, 606, 463, 659, 594, 575, 648, 586, 376, 638, 613, 358,\n",
       "        656, 608, 460, 579, 483, 235, 582, 382, 286, 654, 551, 346, 571,\n",
       "        499, 332, 444, 557, 412, 603, 508, 403, 451, 540, 476, 455, 534,\n",
       "        241, 623, 520, 214,  57,  18, 182, 323, 143, 105, 247, 229,   1,\n",
       "        265, 273, 479, 120, 350, 363, 457,  76, 132,  32,  92, 516,  30,\n",
       "        400, 441,  13, 156, 204, 288, 292, 553, 217,  36, 267, 145, 130,\n",
       "         82, 194, 447, 566, 174, 210, 372,  10,  74,  98, 431, 356, 163,\n",
       "         67,  88,  34,  80, 161,  65, 309, 122, 474, 369, 202, 518, 439,\n",
       "        139,  72, 374, 103,  84, 390, 334, 261, 126,   5,   3, 354, 134,\n",
       "        379,  63, 299, 111,  78, 170,  70, 295, 113, 418, 191,  49,   7,\n",
       "        149,  94,  53, 545, 179,  44, 427, 128,  42, 468, 184, 200, 493,\n",
       "         59, 151, 573, 297, 227, 555, 280, 239, 337,  47,  26, 549, 423,\n",
       "        504, 320, 277, 219, 339,  86, 237, 352, 397, 154, 425, 305,  40,\n",
       "        249, 212, 165, 116, 366,  51, 302, 159,  28, 536, 251, 100, 527,\n",
       "        360, 221, 569, 395, 514, 564, 207,  38, 562, 307,  55, 531, 487,\n",
       "        280, 433, 312,  16, 543, 176,  24, 420, 270, 387, 329, 141, 253,\n",
       "        465, 137,  96, 501, 258, 453, 318, 385,  21, 470, 186, 197, 415,\n",
       "        147, 315, 634, 610, 490, 640, 606, 463, 659, 594, 575, 648, 586,\n",
       "        376, 638, 613, 358, 656, 608, 460, 579, 483, 235, 582, 382, 286,\n",
       "        654, 551, 346, 571, 499, 332, 444, 557, 412, 603, 508, 403, 451,\n",
       "        540, 476, 455, 534, 241, 623, 520, 214, 226, 181, 244, 246, 169,\n",
       "        245, 284, 394, 108, 405, 304, 317, 430, 276, 290, 392, 196,  90,\n",
       "        285, 503, 581, 322, 498, 568, 178,  61,  62, 436, 435, 599, 365,\n",
       "        450, 158, 199,  15,   9, 119, 409, 590, 328, 231, 124, 349, 438,\n",
       "        294, 336,  91,  12, 530, 168, 275, 547, 327, 264, 489, 107, 243,\n",
       "        422, 260, 342, 548, 473, 279, 485, 429, 523, 513, 209, 472, 462,\n",
       "        407, 189, 325, 411, 446, 172, 414, 263, 406, 368, 190, 467, 188,\n",
       "        584, 256, 410, 173, 233, 486, 255, 597, 118,  23, 510, 110, 301,\n",
       "        588, 437, 311, 602, 344,  20, 617, 109, 331, 525, 443, 378, 598,\n",
       "        206, 257, 593, 272, 417, 612, 459, 497, 620, 232, 343, 560, 478,\n",
       "        348, 529, 449, 399, 538, 495, 223, 291, 345, 481, 511, 224, 326,\n",
       "        629, 167, 225, 585, 402,  69, 626, 393, 389, 647, 362, 153, 651,\n",
       "        507, 216, 661, 408, 234, 650, 384, 125, 636, 371, 102, 653, 559,\n",
       "        314, 619, 539, 136, 630, 496, 492, 646, 524, 193, 622, 381,  46,\n",
       "        600, 269, 341, 625, 482, 115, 673, 621, 637, 674, 616, 601, 672,\n",
       "        627, 577, 670, 644, 605, 671, 645, 592, 675, 652, 591, 666, 618,\n",
       "        578, 658, 633, 561, 668, 643, 512, 662, 628, 589, 663, 632, 526,\n",
       "        669, 642, 542, 664, 596, 533, 665, 615, 522, 667, 631, 506],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7881871942204813"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793722\n",
      "F1: 0.680556\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a267f25f8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
