{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='binary:logistic', random_state=None, reg_alpha=None,\n",
       "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "              tree_method=None, validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=10000, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=1,\n",
       "              reg_lambda=1, scale_pos_weight=0.38245219347581555, subsample=0.5,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.690647\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789238\n",
      "F1: 0.680272\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660379</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.661152</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.643254</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.010106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630917</td>\n",
       "      <td>0.009163</td>\n",
       "      <td>0.633101</td>\n",
       "      <td>0.010630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.619612</td>\n",
       "      <td>0.012002</td>\n",
       "      <td>0.622553</td>\n",
       "      <td>0.011074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.614089</td>\n",
       "      <td>0.011281</td>\n",
       "      <td>0.617477</td>\n",
       "      <td>0.010654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.361463</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.426287</td>\n",
       "      <td>0.039331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.361390</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.426189</td>\n",
       "      <td>0.039451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.360983</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>0.426244</td>\n",
       "      <td>0.039909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.360753</td>\n",
       "      <td>0.011790</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.039985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.360338</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.426116</td>\n",
       "      <td>0.040178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.660379           0.000837           0.661152   \n",
       "1              0.643254           0.008338           0.644139   \n",
       "2              0.630917           0.009163           0.633101   \n",
       "3              0.619612           0.012002           0.622553   \n",
       "4              0.614089           0.011281           0.617477   \n",
       "..                  ...                ...                ...   \n",
       "148            0.361463           0.011803           0.426287   \n",
       "149            0.361390           0.011806           0.426189   \n",
       "150            0.360983           0.011734           0.426244   \n",
       "151            0.360753           0.011790           0.426230   \n",
       "152            0.360338           0.011897           0.426116   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001650  \n",
       "1            0.010106  \n",
       "2            0.010630  \n",
       "3            0.011074  \n",
       "4            0.010654  \n",
       "..                ...  \n",
       "148          0.039331  \n",
       "149          0.039451  \n",
       "150          0.039909  \n",
       "151          0.039985  \n",
       "152          0.040178  \n",
       "\n",
       "[153 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8754\n",
      "AUC Score (Train): 0.924280\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.703448\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  3.3min finished\n",
      "C:\\Users\\Drago\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
      "  \"removed in 0.24.\", FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estim...\n",
       "                                     reg_lambda=None, scale_pos_weight=None,\n",
       "                                     subsample=None, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.07417645, 0.17414451, 0.31109972, 0.07057652, 0.19134007,\n",
       "        0.28890786, 0.0653801 , 0.18824272, 0.30450926, 0.07677355,\n",
       "        0.22552824, 0.31249905, 0.07317719, 0.18574028, 0.30050282,\n",
       "        0.06677747, 0.26156993, 0.32349982, 0.08037357, 0.23152623,\n",
       "        0.37658277, 0.10857387, 0.24982185, 0.41436   , 0.09857316,\n",
       "        0.20693293, 0.33469191, 0.08397274, 0.25371766, 0.4724196 ,\n",
       "        0.0974834 , 0.25051932, 0.39187355, 0.08117323, 0.24692059,\n",
       "        0.38807549, 0.09177027, 0.25711703, 0.43495431, 0.10542226,\n",
       "        0.2497746 , 0.40366325, 0.0863718 , 0.23487506, 0.3916985 ,\n",
       "        0.07072577, 0.1841929 , 0.38311982, 0.07967825, 0.16894522,\n",
       "        0.27792168, 0.06417861, 0.17419376, 0.28180699, 0.06659818,\n",
       "        0.20377922, 0.30896878, 0.07097797, 0.19153829, 0.30989995,\n",
       "        0.06877809, 0.18534045, 0.30300412, 0.07517896, 0.21463604,\n",
       "        0.34109025, 0.07637615, 0.26318026, 0.51314754, 0.08423562,\n",
       "        0.22096453, 0.3371222 , 0.09000249, 0.24354453, 0.38954444,\n",
       "        0.08577166, 0.23953261, 0.38171163, 0.08017402, 0.23333559,\n",
       "        0.37192349, 0.08677225, 0.25111804, 0.42126565, 0.09376907,\n",
       "        0.25871634, 0.39680285, 0.08762836, 0.23520594, 0.45049825,\n",
       "        0.0617794 , 0.17114601, 0.3007246 , 0.0648386 , 0.16951408,\n",
       "        0.27254124, 0.06143265, 0.17917752, 0.27455964, 0.06997681,\n",
       "        0.19866781, 0.33494673, 0.06958804, 0.19313755, 0.34070139,\n",
       "        0.07907796, 0.22054033, 0.37787843, 0.09217005, 0.22052875,\n",
       "        0.3622839 , 0.0795752 , 0.22772837, 0.34969788, 0.07517514,\n",
       "        0.21253181, 0.33989067, 0.08417373, 0.24231143, 0.38591552,\n",
       "        0.08438859, 0.23698702, 0.39167409, 0.08497314, 0.22292733,\n",
       "        0.39097681, 0.0881762 , 0.2580606 , 0.44103956, 0.08777242,\n",
       "        0.24372134, 0.39995999, 0.10117159, 0.23792343, 0.41046753,\n",
       "        0.06377993, 0.1705452 , 0.2789185 , 0.06378188, 0.17414541,\n",
       "        0.27671113, 0.05938077, 0.20893369, 0.31409774, 0.07737446,\n",
       "        0.22162991, 0.33379679, 0.07317672, 0.18873887, 0.31769791,\n",
       "        0.07137628, 0.19013953, 0.30950089, 0.07797546, 0.22949224,\n",
       "        0.34956317, 0.07697511, 0.21353154, 0.35849838, 0.07997365,\n",
       "        0.22355499, 0.35654254, 0.08520865, 0.23965673, 0.40700684,\n",
       "        0.08277307, 0.23832345, 0.40929146, 0.07918053, 0.2239399 ,\n",
       "        0.38827448, 0.09157076, 0.26291575, 0.42806273, 0.09157066,\n",
       "        0.24492116, 0.40547032, 0.08577304, 0.25171843, 0.43565998,\n",
       "        0.07337551, 0.20313497, 0.32709479, 0.07917409, 0.17214408,\n",
       "        0.27811046, 0.06317925, 0.18074164, 0.28531322, 0.06977844,\n",
       "        0.1948123 , 0.32426186, 0.0699018 , 0.18981819, 0.32205443,\n",
       "        0.08625345, 0.18813429, 0.34176812, 0.0921699 , 0.24397144,\n",
       "        0.37840929, 0.08141088, 0.22217002, 0.37604899, 0.0863728 ,\n",
       "        0.22937698, 0.367979  , 0.08677201, 0.25438223, 0.47852268,\n",
       "        0.10342865, 0.28685679, 0.41503158, 0.09846854, 0.25371637,\n",
       "        0.40260653, 0.10664144, 0.35482354, 0.58202143, 0.11060357,\n",
       "        0.38586464, 0.42486243, 0.08477445, 0.25761957, 0.48496017,\n",
       "        0.10476952, 0.1977365 , 0.31349826, 0.0723763 , 0.20973225,\n",
       "        0.39517636, 0.07346749, 0.25036635, 0.28231936, 0.08157344,\n",
       "        0.22232866, 0.39967251, 0.06657786, 0.21133137, 0.38027725,\n",
       "        0.10976629, 0.2287262 , 0.32189717, 0.07777615, 0.30330729,\n",
       "        0.42326951, 0.08317218, 0.2357254 , 0.38228264, 0.08117371,\n",
       "        0.24132152, 0.38987479, 0.10456672, 0.26671362, 0.41606636,\n",
       "        0.08657155, 0.23552399, 0.43006902, 0.10536718, 0.22752757,\n",
       "        0.43746128, 0.10117383, 0.25131927, 0.41715245, 0.09037032,\n",
       "        0.24572124, 0.40187082, 0.08477263, 0.23852334, 0.37747836,\n",
       "        0.06817832, 0.18394027, 0.29710245, 0.06277933, 0.17714276,\n",
       "        0.28680768, 0.0663784 , 0.18706541, 0.28690724, 0.0793757 ,\n",
       "        0.18913941, 0.33949156, 0.08077402, 0.20843401, 0.32919559,\n",
       "        0.0753756 , 0.19193783, 0.30810165, 0.07677507, 0.24952126,\n",
       "        0.40447268, 0.08817105, 0.22912636, 0.34798956, 0.07777514,\n",
       "        0.23163104, 0.4202651 , 0.09516778, 0.24384108, 0.40022955,\n",
       "        0.08637204, 0.25531712, 0.36808214, 0.08297296, 0.2532403 ,\n",
       "        0.39027462, 0.09037099, 0.25311832, 0.42206388, 0.08997045,\n",
       "        0.23832583, 0.43306203, 0.10036774, 0.24072232, 0.48984919,\n",
       "        0.06857729, 0.17554321, 0.28170896, 0.06137977, 0.25693574,\n",
       "        0.29550877, 0.07197633, 0.22132897, 0.33829155, 0.07277603,\n",
       "        0.26431541, 0.53089204, 0.20947299, 0.21597061, 0.34566102,\n",
       "        0.08407865, 0.20453391, 0.3001893 , 0.07824306, 0.22604151,\n",
       "        0.34037447, 0.07662048, 0.220437  , 0.3669713 , 0.07478113,\n",
       "        0.19839416, 0.34579091, 0.08918767, 0.24259248, 0.41146803,\n",
       "        0.08557348, 0.23072543, 0.37578549, 0.0884304 , 0.23277707,\n",
       "        0.3720468 , 0.09541125, 0.24532967, 0.41802311, 0.08977823,\n",
       "        0.23235636, 0.38198252, 0.08387556, 0.24052315, 0.44016337,\n",
       "        0.07267928, 0.22572641, 0.31689768, 0.0729763 , 0.17674284,\n",
       "        0.28830872, 0.06577926, 0.17094321, 0.28270907, 0.07057762,\n",
       "        0.20873199, 0.320298  , 0.06897769, 0.19613657, 0.32469549,\n",
       "        0.06777768, 0.1859406 , 0.31989732, 0.09237022, 0.21293106,\n",
       "        0.37447953, 0.09796877, 0.21313138, 0.34450045, 0.07317629,\n",
       "        0.21433067, 0.38107805, 0.13535719, 0.37327976, 0.39027457,\n",
       "        0.08277373, 0.25151982, 0.3834765 , 0.08377137, 0.23452253,\n",
       "        0.37687888, 0.09137073, 0.25621982, 0.47946329, 0.10938125,\n",
       "        0.28976946, 0.44286423, 0.08697243, 0.25634289, 0.43287773,\n",
       "        0.06038399, 0.18754005, 0.30311236, 0.06437759, 0.18596845,\n",
       "        0.29217243, 0.06717873, 0.18724666, 0.31921759, 0.07797494,\n",
       "        0.19833555, 0.32869444, 0.09336963, 0.29190512, 0.31609826,\n",
       "        0.07517447, 0.20213571, 0.31789756, 0.07857461, 0.21433125,\n",
       "        0.38278499, 0.07481194, 0.24356656, 0.45305409, 0.07477608,\n",
       "        0.23532424, 0.35778599, 0.0897718 , 0.24242597, 0.47014537,\n",
       "        0.10628915, 0.37793174, 0.44425716, 0.07937427, 0.22793083,\n",
       "        0.3789525 , 0.08798609, 0.25594263, 0.44537888, 0.09977036,\n",
       "        0.25921822, 0.42926493, 0.08277588, 0.25503931, 0.45995593,\n",
       "        0.07157702, 0.19433703, 0.26432743, 0.0713799 , 0.15874968,\n",
       "        0.26971364, 0.06257911, 0.16074805, 0.25511813, 0.06517878,\n",
       "        0.19193821, 0.29791842, 0.06617842, 0.17834244, 0.29990315,\n",
       "        0.06178041, 0.17194476, 0.29431543, 0.08097243, 0.20293474,\n",
       "        0.32549505, 0.06977773, 0.21852884, 0.32449503, 0.06977696,\n",
       "        0.24132376, 0.37190547, 0.08413434, 0.20953741, 0.35732379,\n",
       "        0.08362379, 0.19893632, 0.32709489, 0.08037305, 0.20339937,\n",
       "        0.32018681, 0.08177357, 0.23982425, 0.37098589, 0.09137073,\n",
       "        0.26951256, 0.36548247, 0.07517567, 0.2185298 , 0.42686248,\n",
       "        0.07117705, 0.15455017, 0.2571177 , 0.05678163, 0.17294307,\n",
       "        0.25291858, 0.06058035, 0.16694636, 0.26451683, 0.06837664,\n",
       "        0.17154498, 0.28590899, 0.06257968, 0.25315399, 0.3171566 ,\n",
       "        0.07881913, 0.18238959, 0.29270597, 0.06717925, 0.19093776,\n",
       "        0.35550718, 0.09562969, 0.21594129, 0.36119146, 0.08201213,\n",
       "        0.23261333, 0.37794824, 0.07857504, 0.21394911, 0.33835592,\n",
       "        0.07157631, 0.20375085, 0.32350893, 0.07280254, 0.19614997,\n",
       "        0.3192977 , 0.09156914, 0.21852994, 0.37328324, 0.07557583,\n",
       "        0.43106289, 0.44107313, 0.11098022, 0.25215006, 0.3940702 ,\n",
       "        0.06257982, 0.21103525, 0.31512365, 0.05978031, 0.15834985,\n",
       "        0.27172141, 0.05939188, 0.15574942, 0.2504724 , 0.07191257,\n",
       "        0.22680187, 0.36768155, 0.06937566, 0.18918285, 0.29486322,\n",
       "        0.0744175 , 0.18881793, 0.35175786, 0.08687768, 0.20133491,\n",
       "        0.33531199, 0.06899347, 0.1829021 , 0.3115912 , 0.07399182,\n",
       "        0.18554535, 0.3129899 , 0.07797613, 0.20733547, 0.3341136 ,\n",
       "        0.07699575, 0.20597544, 0.3288929 , 0.07277575, 0.1983356 ,\n",
       "        0.32910767, 0.08518772, 0.2169281 , 0.37267895, 0.08257203,\n",
       "        0.23387117, 0.35848532, 0.07677593, 0.21773009, 0.32649503,\n",
       "        0.0573823 , 0.15854921, 0.29190626, 0.065978  , 0.19633975,\n",
       "        0.29390879, 0.06519051, 0.20374093, 0.27790952, 0.07397399,\n",
       "        0.20833321, 0.32760849, 0.07117586, 0.19565163, 0.37689562,\n",
       "        0.07177663, 0.19553833, 0.29070621, 0.0749764 , 0.18993907,\n",
       "        0.31969709, 0.07117648, 0.2081347 , 0.34509063, 0.07197633,\n",
       "        0.26181564, 0.33072195, 0.07577415, 0.23432531, 0.37048163,\n",
       "        0.08077407, 0.21033206, 0.34869738, 0.07297635, 0.21170611,\n",
       "        0.34002337, 0.08660731, 0.21453061, 0.37952518, 0.09704142,\n",
       "        0.22892499, 0.38472533, 0.08857951, 0.25313163, 0.43907599,\n",
       "        0.06078057, 0.1659471 , 0.27871008, 0.05877995, 0.15974836,\n",
       "        0.27652454, 0.05538225, 0.19262309, 0.43554735, 0.11596227,\n",
       "        0.19233794, 0.30750432, 0.07897472, 0.24433064, 0.30630426,\n",
       "        0.0819766 , 0.19554639, 0.28850722, 0.07237597, 0.19373741,\n",
       "        0.35208697, 0.0829721 , 0.19553776, 0.35752659, 0.12375989,\n",
       "        0.22532516, 0.31329927, 0.07937388, 0.22472558, 0.34668922,\n",
       "        0.07537532, 0.19993734, 0.35048728, 0.07077708, 0.19913602,\n",
       "        0.34768882, 0.08617282, 0.23612776, 0.43061986, 0.134166  ,\n",
       "        0.21974783, 0.36668096, 0.07677364, 0.20833259, 0.35109534]),\n",
       " 'std_fit_time': array([0.00607791, 0.00780553, 0.01939912, 0.00605258, 0.01166762,\n",
       "        0.01722095, 0.00215303, 0.00846444, 0.01219175, 0.0038676 ,\n",
       "        0.02262622, 0.01488955, 0.0040691 , 0.00365503, 0.00560555,\n",
       "        0.00256056, 0.03340139, 0.02541002, 0.00402852, 0.0117724 ,\n",
       "        0.04959604, 0.0439178 , 0.03407047, 0.05965952, 0.04329302,\n",
       "        0.00328527, 0.00970135, 0.00389786, 0.01841571, 0.01134622,\n",
       "        0.00974695, 0.01727523, 0.0169774 , 0.00256072, 0.01169147,\n",
       "        0.02282593, 0.00483142, 0.01365518, 0.02132105, 0.0271294 ,\n",
       "        0.01242064, 0.01687583, 0.0034401 , 0.01077658, 0.02405728,\n",
       "        0.01342288, 0.01723148, 0.05991097, 0.01339226, 0.00509723,\n",
       "        0.01596277, 0.00917045, 0.01467572, 0.0057163 , 0.00431994,\n",
       "        0.01017445, 0.00205337, 0.00309715, 0.00711391, 0.00809726,\n",
       "        0.00391765, 0.00422188, 0.00902497, 0.00597303, 0.00514521,\n",
       "        0.00721985, 0.00879783, 0.01974421, 0.04511566, 0.00213826,\n",
       "        0.01106645, 0.00911091, 0.00662472, 0.01150227, 0.00669501,\n",
       "        0.00696558, 0.01280577, 0.00834161, 0.00386562, 0.01483882,\n",
       "        0.00488234, 0.0047062 , 0.01372752, 0.00830624, 0.00713715,\n",
       "        0.0119688 , 0.00975201, 0.00242681, 0.01140636, 0.03197515,\n",
       "        0.00331028, 0.00693892, 0.01728999, 0.00542485, 0.0056759 ,\n",
       "        0.00989338, 0.00564511, 0.00847606, 0.01573252, 0.00497814,\n",
       "        0.01626103, 0.01133008, 0.00267258, 0.00573943, 0.03090183,\n",
       "        0.00667346, 0.00463708, 0.01897841, 0.00785572, 0.00560504,\n",
       "        0.01411864, 0.00417481, 0.0139874 , 0.01045138, 0.00370917,\n",
       "        0.00658886, 0.00632275, 0.00292458, 0.01373443, 0.00699102,\n",
       "        0.00288987, 0.0104619 , 0.00649072, 0.00468922, 0.00547412,\n",
       "        0.0114365 , 0.00376959, 0.01152375, 0.01727707, 0.00584233,\n",
       "        0.01008431, 0.02053641, 0.01418632, 0.00891901, 0.01274878,\n",
       "        0.00411698, 0.00588342, 0.01872822, 0.00563705, 0.01092055,\n",
       "        0.0097623 , 0.00307168, 0.01405353, 0.00994511, 0.00174325,\n",
       "        0.00713723, 0.01562846, 0.00676207, 0.00499407, 0.00526685,\n",
       "        0.00344002, 0.00624004, 0.01062796, 0.00589659, 0.00748644,\n",
       "        0.00739168, 0.00468885, 0.00688542, 0.02726872, 0.00952611,\n",
       "        0.01769508, 0.02153544, 0.00661848, 0.01293332, 0.01497897,\n",
       "        0.00312363, 0.01692421, 0.0132999 , 0.00462866, 0.0077876 ,\n",
       "        0.01743695, 0.00531197, 0.00740146, 0.00970128, 0.00508143,\n",
       "        0.00606418, 0.00844965, 0.00292529, 0.00892819, 0.03764146,\n",
       "        0.00906517, 0.01058739, 0.00822814, 0.01283876, 0.00636565,\n",
       "        0.01274512, 0.0048733 , 0.01014446, 0.01554421, 0.00495463,\n",
       "        0.00611361, 0.01424892, 0.00234284, 0.00989688, 0.01809529,\n",
       "        0.01041562, 0.00775277, 0.02725762, 0.00193884, 0.0147818 ,\n",
       "        0.01150795, 0.00674127, 0.00763527, 0.02242815, 0.01566281,\n",
       "        0.01795471, 0.01117593, 0.00767631, 0.01396275, 0.04714076,\n",
       "        0.01490315, 0.01191003, 0.01711083, 0.00938101, 0.00359937,\n",
       "        0.02159212, 0.01409052, 0.02069866, 0.07798463, 0.00693967,\n",
       "        0.10105678, 0.00993737, 0.00470718, 0.01383378, 0.06840062,\n",
       "        0.00884151, 0.01411511, 0.01644479, 0.00825758, 0.06572167,\n",
       "        0.05189115, 0.01579804, 0.02132612, 0.00265101, 0.00788686,\n",
       "        0.06459417, 0.05615288, 0.00667952, 0.00801241, 0.05795545,\n",
       "        0.02562553, 0.03670017, 0.01635265, 0.00515225, 0.07081382,\n",
       "        0.05493607, 0.01049112, 0.02349989, 0.02239507, 0.00906095,\n",
       "        0.01794489, 0.01634107, 0.01074078, 0.01168201, 0.00965968,\n",
       "        0.00815986, 0.00801201, 0.04958918, 0.0163357 , 0.01815663,\n",
       "        0.03488084, 0.01022599, 0.01333175, 0.00973506, 0.00564125,\n",
       "        0.00652122, 0.01663209, 0.00248074, 0.01151274, 0.01391837,\n",
       "        0.00798111, 0.0054759 , 0.01445151, 0.00426019, 0.00963886,\n",
       "        0.02132433, 0.00719725, 0.00684654, 0.01040806, 0.01397671,\n",
       "        0.00986383, 0.01187344, 0.00305766, 0.01152613, 0.01451295,\n",
       "        0.0064045 , 0.0066008 , 0.01474013, 0.00743745, 0.00737028,\n",
       "        0.00883514, 0.006045  , 0.01195323, 0.01387614, 0.00318745,\n",
       "        0.02703737, 0.04533981, 0.01039685, 0.01456368, 0.01855737,\n",
       "        0.0034968 , 0.01319594, 0.01560737, 0.00797213, 0.00757971,\n",
       "        0.00924039, 0.00349798, 0.00541694, 0.01428227, 0.00609861,\n",
       "        0.00580743, 0.0234821 , 0.01154761, 0.01031912, 0.04774996,\n",
       "        0.00652806, 0.00960109, 0.02008758, 0.00319912, 0.06473472,\n",
       "        0.01945964, 0.00878305, 0.01733199, 0.02504847, 0.00676194,\n",
       "        0.01182068, 0.13427356, 0.06351692, 0.0204836 , 0.0099853 ,\n",
       "        0.01235523, 0.0124955 , 0.00876757, 0.00495207, 0.01384246,\n",
       "        0.01167081, 0.00300017, 0.01847404, 0.02058395, 0.00248319,\n",
       "        0.00844552, 0.01395171, 0.00729466, 0.01342796, 0.01311971,\n",
       "        0.00571161, 0.00285525, 0.01979273, 0.00660976, 0.00887121,\n",
       "        0.008418  , 0.00554912, 0.00821646, 0.01314195, 0.00639153,\n",
       "        0.00242751, 0.01207761, 0.00624621, 0.02687313, 0.01215771,\n",
       "        0.02215675, 0.02031589, 0.02023015, 0.00582891, 0.0094286 ,\n",
       "        0.01802301, 0.00984366, 0.00781995, 0.01777536, 0.00135536,\n",
       "        0.01279214, 0.01153026, 0.00451571, 0.00630344, 0.01310172,\n",
       "        0.00146923, 0.00509764, 0.02341829, 0.00943223, 0.00451506,\n",
       "        0.03757358, 0.02086432, 0.00992485, 0.01681346, 0.00213491,\n",
       "        0.01339135, 0.05067327, 0.03478852, 0.06632824, 0.00775885,\n",
       "        0.00396805, 0.00793761, 0.0154302 , 0.00365416, 0.01028734,\n",
       "        0.01589447, 0.00682718, 0.00609343, 0.02220247, 0.00520735,\n",
       "        0.00605974, 0.02774017, 0.00641595, 0.01724346, 0.02032174,\n",
       "        0.00392002, 0.01620057, 0.00957247, 0.00978494, 0.01391057,\n",
       "        0.01098142, 0.00503395, 0.02364431, 0.01968656, 0.01025284,\n",
       "        0.0062777 , 0.0319867 , 0.02835081, 0.05490062, 0.0114403 ,\n",
       "        0.00278339, 0.00876881, 0.01500148, 0.00440739, 0.00578092,\n",
       "        0.0241364 , 0.00352627, 0.03958253, 0.03888959, 0.00193752,\n",
       "        0.01081522, 0.01936686, 0.00751833, 0.00518417, 0.07797005,\n",
       "        0.02181933, 0.07500112, 0.02626447, 0.00634119, 0.00532606,\n",
       "        0.00877609, 0.00395811, 0.01116511, 0.01722531, 0.01360618,\n",
       "        0.01192825, 0.01518558, 0.00519952, 0.00709322, 0.06852277,\n",
       "        0.03333943, 0.02478411, 0.00825772, 0.01075409, 0.00594409,\n",
       "        0.01353681, 0.00338061, 0.00604462, 0.00587771, 0.00171993,\n",
       "        0.01734999, 0.0157298 , 0.00462021, 0.00614918, 0.01182823,\n",
       "        0.00453295, 0.00680941, 0.01691363, 0.00804606, 0.01072967,\n",
       "        0.01328703, 0.00886021, 0.0112307 , 0.00770679, 0.00342925,\n",
       "        0.00915226, 0.01611983, 0.0048395 , 0.00664106, 0.01335469,\n",
       "        0.00966047, 0.00374085, 0.01542754, 0.00854261, 0.00788643,\n",
       "        0.00737686, 0.00556234, 0.00895387, 0.02230165, 0.01914196,\n",
       "        0.01680689, 0.01671154, 0.00256073, 0.00984801, 0.03340335,\n",
       "        0.01565881, 0.00765476, 0.01481875, 0.0080087 , 0.00404855,\n",
       "        0.01360513, 0.00664848, 0.01136183, 0.00858846, 0.01059096,\n",
       "        0.00640432, 0.00840951, 0.00484054, 0.03404182, 0.01305697,\n",
       "        0.01304082, 0.01343893, 0.00767525, 0.00661206, 0.01054186,\n",
       "        0.04084795, 0.00795936, 0.01330839, 0.01745333, 0.01299878,\n",
       "        0.04589202, 0.06111277, 0.0017426 , 0.01345449, 0.01443765,\n",
       "        0.00512104, 0.0095063 , 0.00768046, 0.00481589, 0.00640136,\n",
       "        0.01384646, 0.00992901, 0.00813593, 0.0572641 , 0.00757609,\n",
       "        0.06375532, 0.03498326, 0.01610244, 0.01261245, 0.01583633,\n",
       "        0.0068851 , 0.01779702, 0.02448085, 0.00365421, 0.00515891,\n",
       "        0.01271797, 0.00475249, 0.00401822, 0.00711798, 0.008298  ,\n",
       "        0.00797155, 0.03947136, 0.0062778 , 0.0177043 , 0.01204024,\n",
       "        0.00792216, 0.01002611, 0.01571604, 0.01038312, 0.01190584,\n",
       "        0.0111128 , 0.0038532 , 0.00355827, 0.013597  , 0.00843363,\n",
       "        0.00796799, 0.0043669 , 0.00742689, 0.00634219, 0.01702297,\n",
       "        0.01253139, 0.00745595, 0.01426565, 0.00479047, 0.00711443,\n",
       "        0.00975406, 0.01208997, 0.00962847, 0.02507166, 0.00556876,\n",
       "        0.0152779 , 0.01445308, 0.00614387, 0.00879336, 0.01746013,\n",
       "        0.00458527, 0.00257731, 0.00995602, 0.00956554, 0.0067098 ,\n",
       "        0.0154292 , 0.00299898, 0.01837519, 0.01718737, 0.00547474,\n",
       "        0.01187227, 0.01806571, 0.00292463, 0.00614173, 0.02381825,\n",
       "        0.0110494 , 0.00711305, 0.01950104, 0.01085856, 0.01134472,\n",
       "        0.01171637, 0.0089714 , 0.02301906, 0.05839072, 0.00973375,\n",
       "        0.0570354 , 0.01243614, 0.00772904, 0.0039279 , 0.01550848,\n",
       "        0.00278477, 0.01064774, 0.00494566, 0.0045589 , 0.01426035,\n",
       "        0.01889542, 0.01284924, 0.00417511, 0.02553502, 0.00846398,\n",
       "        0.00896389, 0.01265998, 0.00750182, 0.01771859, 0.05173794,\n",
       "        0.00348591, 0.01253332, 0.02139777, 0.00421296, 0.00972118,\n",
       "        0.0112894 , 0.00287007, 0.01325435, 0.07104372, 0.01690665,\n",
       "        0.02101226, 0.02524663, 0.0100167 , 0.02112679, 0.0204759 ,\n",
       "        0.0089175 , 0.0075166 , 0.00694349, 0.00549753, 0.00511284,\n",
       "        0.03978815, 0.01535746, 0.00500389, 0.0800135 , 0.04193119,\n",
       "        0.03880328, 0.0231323 , 0.00744326, 0.01586174, 0.00856213,\n",
       "        0.00185423, 0.00599999, 0.01694844, 0.00391786, 0.01064448,\n",
       "        0.02100264, 0.00788153, 0.01329235, 0.08081492, 0.02798407,\n",
       "        0.00824313, 0.01014338, 0.00457597, 0.00791228, 0.01217729]),\n",
       " 'mean_score_time': array([0.00799742, 0.00559835, 0.00679712, 0.00399857, 0.00759635,\n",
       "        0.00799656, 0.00379715, 0.00590119, 0.0067976 , 0.00419884,\n",
       "        0.00559773, 0.00759792, 0.00579748, 0.00559778, 0.00879741,\n",
       "        0.00419893, 0.01079702, 0.00699806, 0.00499845, 0.00659647,\n",
       "        0.00999689, 0.00599718, 0.00759735, 0.00979691, 0.00459843,\n",
       "        0.00599852, 0.00859714, 0.0051981 , 0.00879688, 0.01139679,\n",
       "        0.00679708, 0.01239581, 0.00899792, 0.00519843, 0.00819821,\n",
       "        0.01059756, 0.0055985 , 0.00899711, 0.01201649, 0.00459886,\n",
       "        0.00799723, 0.00999675, 0.00822206, 0.00699763, 0.00939751,\n",
       "        0.00619783, 0.0076025 , 0.00999737, 0.00739737, 0.00559831,\n",
       "        0.00679832, 0.00399852, 0.00599785, 0.00819755, 0.00599766,\n",
       "        0.01131692, 0.0075974 , 0.00419817, 0.00599847, 0.00919762,\n",
       "        0.00419798, 0.00599771, 0.00819769, 0.00470018, 0.00679784,\n",
       "        0.00979733, 0.00519776, 0.00699797, 0.01121397, 0.00619836,\n",
       "        0.00759749, 0.01222754, 0.00702825, 0.00799656, 0.00999708,\n",
       "        0.00660362, 0.00879722, 0.0105968 , 0.00439796, 0.00679712,\n",
       "        0.00899792, 0.00519838, 0.0069973 , 0.01359506, 0.00659924,\n",
       "        0.00659785, 0.00919967, 0.00560412, 0.00741057, 0.01059613,\n",
       "        0.00539989, 0.00659738, 0.00801353, 0.0061934 , 0.00499611,\n",
       "        0.00720615, 0.00440979, 0.0075048 , 0.00799751, 0.00519814,\n",
       "        0.00579844, 0.01009803, 0.00439825, 0.00639777, 0.01149659,\n",
       "        0.00639811, 0.00699682, 0.01739593, 0.00499892, 0.00859709,\n",
       "        0.01039705, 0.00499763, 0.00679812, 0.00979695, 0.00499811,\n",
       "        0.00679779, 0.0087976 , 0.00579672, 0.00750337, 0.0100039 ,\n",
       "        0.00520787, 0.00799813, 0.00939717, 0.00559821, 0.00739784,\n",
       "        0.01079702, 0.00579772, 0.00820279, 0.01099639, 0.00499754,\n",
       "        0.00679789, 0.01019664, 0.00459757, 0.00719781, 0.00939674,\n",
       "        0.00499797, 0.00539827, 0.00699773, 0.00819826, 0.00499701,\n",
       "        0.00699868, 0.00419831, 0.00599737, 0.00859704, 0.00459986,\n",
       "        0.00619721, 0.00719786, 0.00419807, 0.00579844, 0.00999651,\n",
       "        0.00499887, 0.00619702, 0.00979638, 0.00699973, 0.00761218,\n",
       "        0.00899677, 0.00499802, 0.00659809, 0.00979657, 0.00599856,\n",
       "        0.00759754, 0.01040072, 0.00499802, 0.00739889, 0.01019611,\n",
       "        0.00479817, 0.00679727, 0.01039743, 0.00519714, 0.00679746,\n",
       "        0.00879717, 0.00599818, 0.00679674, 0.01319556, 0.00499821,\n",
       "        0.00779743, 0.01139574, 0.00519838, 0.00719719, 0.01359591,\n",
       "        0.00479865, 0.00519757, 0.0095973 , 0.00460048, 0.00679793,\n",
       "        0.00819726, 0.00539846, 0.00659823, 0.00719748, 0.00479698,\n",
       "        0.0061985 , 0.01040688, 0.00692124, 0.00539908, 0.00820117,\n",
       "        0.00682573, 0.00599823, 0.01251287, 0.00479827, 0.00679846,\n",
       "        0.00839744, 0.00419717, 0.00540471, 0.00979719, 0.00519719,\n",
       "        0.00739594, 0.01079617, 0.00559783, 0.00779724, 0.01539555,\n",
       "        0.0055974 , 0.00881491, 0.00999656, 0.00659871, 0.00899768,\n",
       "        0.0114193 , 0.00459824, 0.00799704, 0.0203826 , 0.01562014,\n",
       "        0.00739913, 0.00959716, 0.00539756, 0.00659776, 0.01159654,\n",
       "        0.00439816, 0.00759869, 0.00919781, 0.00499897, 0.00539794,\n",
       "        0.01260743, 0.00399857, 0.01259894, 0.01099715, 0.00499802,\n",
       "        0.00739784, 0.00899673, 0.00639939, 0.00779734, 0.01019659,\n",
       "        0.00819559, 0.00579844, 0.00759773, 0.00439763, 0.00739756,\n",
       "        0.01139717, 0.00579753, 0.0081964 , 0.01139622, 0.00559826,\n",
       "        0.00839739, 0.00959749, 0.00499811, 0.00759778, 0.0115963 ,\n",
       "        0.00479808, 0.0071979 , 0.01799364, 0.00639648, 0.00979619,\n",
       "        0.00919924, 0.00459824, 0.00739737, 0.00959706, 0.00439858,\n",
       "        0.00659819, 0.00919704, 0.00459857, 0.00719738, 0.00859733,\n",
       "        0.00439739, 0.00819879, 0.00739779, 0.00459895, 0.00619783,\n",
       "        0.00739717, 0.00559845, 0.00619755, 0.00759807, 0.0059968 ,\n",
       "        0.00639758, 0.00799656, 0.00579762, 0.00679798, 0.00739784,\n",
       "        0.00479817, 0.00579829, 0.00859652, 0.00459867, 0.00719795,\n",
       "        0.00819707, 0.01099706, 0.00699787, 0.00939617, 0.00519767,\n",
       "        0.01579385, 0.00779791, 0.00479846, 0.0079978 , 0.01159668,\n",
       "        0.00519929, 0.00819769, 0.01119738, 0.00459852, 0.01159711,\n",
       "        0.0087975 , 0.00599747, 0.00819769, 0.01059632, 0.00479798,\n",
       "        0.00659814, 0.01279693, 0.00559821, 0.00919724, 0.00839815,\n",
       "        0.00539846, 0.00519867, 0.00739775, 0.00499902, 0.02569127,\n",
       "        0.00759811, 0.00419803, 0.01079597, 0.01159563, 0.00479927,\n",
       "        0.00999627, 0.01510019, 0.00821037, 0.00639668, 0.00739856,\n",
       "        0.0063982 , 0.0061985 , 0.00719814, 0.0058991 , 0.01000724,\n",
       "        0.00800223, 0.00519872, 0.00961637, 0.00899725, 0.00539751,\n",
       "        0.00639701, 0.01000013, 0.00539804, 0.0067976 , 0.00979686,\n",
       "        0.00719643, 0.0075973 , 0.00940256, 0.00499811, 0.00720272,\n",
       "        0.00920329, 0.00459828, 0.00799751, 0.00892162, 0.00479798,\n",
       "        0.00779743, 0.00899658, 0.00579648, 0.00799923, 0.00979714,\n",
       "        0.00479727, 0.00679688, 0.00839682, 0.00659809, 0.00619798,\n",
       "        0.00819716, 0.00559807, 0.00539784, 0.01459575, 0.00499811,\n",
       "        0.007198  , 0.00819602, 0.00419879, 0.0067976 , 0.0071979 ,\n",
       "        0.00519943, 0.00699749, 0.00859828, 0.00579886, 0.00679836,\n",
       "        0.00919704, 0.00519781, 0.00659838, 0.00979681, 0.00480008,\n",
       "        0.00619807, 0.01119561, 0.01059656, 0.00859795, 0.01059675,\n",
       "        0.00519795, 0.00899658, 0.00939741, 0.00459857, 0.00699768,\n",
       "        0.0089973 , 0.00499783, 0.01160302, 0.01099663, 0.00519791,\n",
       "        0.01301074, 0.01039658, 0.00619707, 0.00939693, 0.00939684,\n",
       "        0.00701637, 0.01079569, 0.00699759, 0.00439858, 0.00679698,\n",
       "        0.00921507, 0.00459819, 0.01139627, 0.00799713, 0.00539918,\n",
       "        0.00659809, 0.00879917, 0.00679808, 0.00559807, 0.0075983 ,\n",
       "        0.00519848, 0.00499721, 0.00859747, 0.0045979 , 0.00739698,\n",
       "        0.01359468, 0.00620885, 0.00759683, 0.01139545, 0.0045989 ,\n",
       "        0.00699759, 0.01119652, 0.00599837, 0.009197  , 0.01580467,\n",
       "        0.00559635, 0.00819688, 0.00879712, 0.00499825, 0.00919781,\n",
       "        0.01079702, 0.0048059 , 0.00760536, 0.01079607, 0.00499783,\n",
       "        0.00879693, 0.00979743, 0.00599856, 0.00679302, 0.01059656,\n",
       "        0.00979657, 0.00559797, 0.00939703, 0.00459938, 0.00619721,\n",
       "        0.00739727, 0.00479813, 0.00579863, 0.00859938, 0.00419884,\n",
       "        0.00619707, 0.00779772, 0.00459857, 0.00579829, 0.00839791,\n",
       "        0.00519762, 0.0057981 , 0.00759687, 0.0057982 , 0.01059842,\n",
       "        0.00959706, 0.00459757, 0.00799747, 0.00919743, 0.00639834,\n",
       "        0.01259503, 0.01012249, 0.00630255, 0.00639791, 0.01019788,\n",
       "        0.00559745, 0.00739641, 0.01199684, 0.0073987 , 0.00639734,\n",
       "        0.00959721, 0.00619707, 0.00999808, 0.01039629, 0.00619788,\n",
       "        0.00799785, 0.01019788, 0.00419841, 0.00679851, 0.01259551,\n",
       "        0.00479932, 0.00619788, 0.0135973 , 0.00499825, 0.00639887,\n",
       "        0.00759726, 0.00379863, 0.00539823, 0.01559348, 0.00519862,\n",
       "        0.0061974 , 0.00819659, 0.00539846, 0.01659498, 0.00800295,\n",
       "        0.00559645, 0.01019855, 0.00799747, 0.00559697, 0.00619841,\n",
       "        0.0179935 , 0.00679812, 0.00959725, 0.01069784, 0.01062894,\n",
       "        0.01000266, 0.0099967 , 0.00439792, 0.00640512, 0.00899725,\n",
       "        0.00479908, 0.00719719, 0.00939746, 0.00479751, 0.00719733,\n",
       "        0.00939856, 0.00439858, 0.00659771, 0.01839123, 0.00499806,\n",
       "        0.01299496, 0.00979729, 0.00659781, 0.00799575, 0.01239657,\n",
       "        0.00439816, 0.00930657, 0.00799794, 0.00519838, 0.00559754,\n",
       "        0.00699873, 0.00439849, 0.00669756, 0.00759764, 0.00479798,\n",
       "        0.00599771, 0.01399755, 0.00559974, 0.00559855, 0.009197  ,\n",
       "        0.00459762, 0.00719786, 0.01019559, 0.00699787, 0.00659781,\n",
       "        0.0100059 , 0.00539746, 0.00640354, 0.01241989, 0.00479741,\n",
       "        0.00639801, 0.01099963, 0.00599589, 0.00639606, 0.00939693,\n",
       "        0.0051981 , 0.00779786, 0.00999742, 0.00439868, 0.00619779,\n",
       "        0.00859756, 0.0073977 , 0.00719819, 0.01139579, 0.00579982,\n",
       "        0.00619855, 0.01099586, 0.00619755, 0.00859652, 0.00839744,\n",
       "        0.00419812, 0.00699797, 0.00799766, 0.00419812, 0.00819421,\n",
       "        0.00940456, 0.00519805, 0.0055975 , 0.00619831, 0.00699787,\n",
       "        0.01259589, 0.00859761, 0.00479894, 0.00679793, 0.0124022 ,\n",
       "        0.00619836, 0.00879679, 0.00939689, 0.0059998 , 0.00679712,\n",
       "        0.00799799, 0.00599761, 0.00599689, 0.0099956 , 0.00460005,\n",
       "        0.0064014 , 0.00801568, 0.00539861, 0.0101984 , 0.01079588,\n",
       "        0.00459847, 0.00879855, 0.00980487, 0.00499854, 0.00639801,\n",
       "        0.01199818, 0.00719757, 0.00739789, 0.01059723, 0.00919628,\n",
       "        0.00819759, 0.01059699, 0.00580897, 0.01099687, 0.01079664,\n",
       "        0.00399923, 0.00939689, 0.00699801, 0.00379944, 0.00600042,\n",
       "        0.00699735, 0.00580621, 0.00639753, 0.01479654, 0.0049993 ,\n",
       "        0.00599804, 0.00959606, 0.01579523, 0.00559816, 0.00819731,\n",
       "        0.00479856, 0.00739512, 0.00739765, 0.00479851, 0.00659766,\n",
       "        0.01819367, 0.00779738, 0.00639701, 0.00879726, 0.00471253,\n",
       "        0.00699792, 0.00799789, 0.0047986 , 0.00899734, 0.0075973 ,\n",
       "        0.00459833, 0.00719581, 0.00819726, 0.00399899, 0.00699792,\n",
       "        0.00919824, 0.00719638, 0.00859718, 0.01279607, 0.00819726,\n",
       "        0.00659814, 0.01179671, 0.00559821, 0.00679812, 0.00859923]),\n",
       " 'std_score_time': array([4.68919138e-03, 1.19910538e-03, 3.99616976e-04, 2.33601546e-07,\n",
       "        1.74233939e-03, 3.03294644e-03, 3.99048651e-04, 1.11224153e-03,\n",
       "        3.99852208e-04, 7.48060812e-04, 4.89804348e-04, 7.99775612e-04,\n",
       "        3.12399871e-03, 4.89454689e-04, 1.93860685e-03, 3.99518052e-04,\n",
       "        6.17523842e-03, 6.32410124e-04, 1.26429165e-03, 8.00623186e-04,\n",
       "        1.78789914e-03, 2.52875297e-03, 3.71788693e-03, 2.71187425e-03,\n",
       "        7.99978381e-04, 6.32786707e-04, 1.19928525e-03, 4.00138367e-04,\n",
       "        1.93820847e-03, 1.85438373e-03, 1.32597179e-03, 3.43904004e-03,\n",
       "        1.09432129e-03, 9.79676249e-04, 1.72192084e-03, 5.23755839e-03,\n",
       "        1.20046143e-03, 1.41374042e-03, 5.16996966e-03, 7.99954188e-04,\n",
       "        1.26417890e-03, 1.41353794e-03, 3.40943387e-03, 6.32862408e-04,\n",
       "        1.02044102e-03, 4.01862931e-03, 2.41540113e-03, 3.89765302e-03,\n",
       "        5.81567614e-03, 1.20019917e-03, 1.59940720e-03, 6.31806859e-04,\n",
       "        1.09527894e-03, 1.16634304e-03, 1.78781891e-03, 8.50445261e-03,\n",
       "        1.49562420e-03, 9.79890772e-04, 6.32485776e-04, 9.79053377e-04,\n",
       "        4.00114074e-04, 6.32032794e-04, 2.03873997e-03, 1.40321275e-03,\n",
       "        7.47958723e-04, 1.59963967e-03, 1.16536170e-03, 1.99930675e-03,\n",
       "        1.31163938e-03, 1.16573806e-03, 1.95861272e-03, 6.07270668e-03,\n",
       "        4.20609616e-03, 1.09427852e-03, 2.09692072e-03, 2.42542333e-03,\n",
       "        4.26028481e-03, 1.85347365e-03, 4.89804534e-04, 9.80114895e-04,\n",
       "        1.09597520e-03, 9.79734910e-04, 8.93456419e-04, 5.60525054e-03,\n",
       "        2.72643556e-03, 4.90057233e-04, 7.44051611e-04, 8.02650919e-04,\n",
       "        1.01741433e-03, 3.37983442e-03, 1.20033637e-03, 8.00658785e-04,\n",
       "        1.25177853e-03, 3.42816421e-03, 6.28420313e-04, 7.52095652e-04,\n",
       "        8.09401587e-04, 1.10195402e-03, 1.78835165e-03, 1.16619597e-03,\n",
       "        7.47717575e-04, 2.94070987e-03, 7.99644052e-04, 1.74318161e-03,\n",
       "        1.55000648e-03, 1.01954298e-03, 1.09523566e-03, 1.14799725e-02,\n",
       "        6.31810746e-04, 1.19966684e-03, 3.38033353e-03, 1.09453894e-03,\n",
       "        1.16313167e-03, 1.32634534e-03, 8.95216049e-04, 7.47691429e-04,\n",
       "        7.47678678e-04, 1.71934273e-03, 8.90231216e-04, 8.94090596e-04,\n",
       "        9.78062159e-04, 1.89700323e-03, 1.19995284e-03, 1.49745966e-03,\n",
       "        8.00515599e-04, 2.31437758e-03, 1.16582797e-03, 7.38543256e-04,\n",
       "        2.75645431e-03, 1.54898919e-03, 7.48162601e-04, 9.79919891e-04,\n",
       "        4.90193529e-04, 9.80358648e-04, 1.35677550e-03, 1.09449518e-03,\n",
       "        4.89979567e-04, 6.32108970e-04, 3.96988486e-03, 6.32186831e-04,\n",
       "        1.09597564e-03, 9.79277315e-04, 1.54849741e-03, 1.49761875e-03,\n",
       "        4.88367422e-04, 1.16536184e-03, 4.00186505e-04, 4.00188607e-04,\n",
       "        3.99756727e-04, 2.89803240e-03, 1.09554020e-03, 1.16621322e-03,\n",
       "        2.55994331e-03, 4.52075336e-03, 2.34122129e-03, 1.26508381e-03,\n",
       "        6.32033064e-04, 8.00061686e-04, 1.83239290e-03, 1.78789853e-03,\n",
       "        1.74346059e-03, 3.56997270e-03, 1.09510470e-03, 1.35600967e-03,\n",
       "        1.72045698e-03, 7.48264607e-04, 7.47487189e-04, 2.41637895e-03,\n",
       "        7.48111861e-04, 7.48047988e-04, 7.48328280e-04, 1.54935841e-03,\n",
       "        1.16550911e-03, 6.96499564e-03, 8.93670476e-04, 1.16584452e-03,\n",
       "        2.57548211e-03, 1.16717765e-03, 7.48124411e-04, 4.62937110e-03,\n",
       "        1.72027992e-03, 7.47997633e-04, 2.33092826e-03, 8.03987105e-04,\n",
       "        1.59983749e-03, 7.48660312e-04, 2.33142683e-03, 2.24523962e-03,\n",
       "        4.00378858e-04, 9.79180571e-04, 1.46937329e-03, 4.95010273e-03,\n",
       "        2.67415214e-03, 4.89906140e-04, 1.46608777e-03, 5.16933964e-03,\n",
       "        3.56832255e-07, 6.93837061e-03, 1.16644155e-03, 4.00046125e-04,\n",
       "        4.89707132e-04, 7.45645626e-04, 4.96979648e-04, 7.48162890e-04,\n",
       "        1.46917239e-03, 1.01971418e-03, 3.48600770e-03, 1.35633973e-03,\n",
       "        1.32665446e-03, 4.07733059e-03, 1.01957143e-03, 3.22088787e-03,\n",
       "        1.67203060e-03, 1.49614882e-03, 1.54858866e-03, 3.92263759e-03,\n",
       "        4.89278600e-04, 1.54914267e-03, 6.01590687e-03, 7.80878671e-03,\n",
       "        1.96243864e-03, 1.35633261e-03, 1.85220346e-03, 8.00073400e-04,\n",
       "        4.75674378e-03, 4.89747405e-04, 3.55752311e-03, 4.44282327e-03,\n",
       "        1.26451841e-03, 7.99584920e-04, 8.16835509e-03, 6.32033172e-04,\n",
       "        1.22430967e-02, 7.23691598e-03, 8.94042719e-04, 1.49587273e-03,\n",
       "        2.27964211e-03, 1.35598848e-03, 2.71224675e-03, 6.39848713e-03,\n",
       "        4.70339348e-03, 7.47397999e-04, 1.19910373e-03, 4.89299219e-04,\n",
       "        2.86910275e-03, 3.55413247e-03, 7.47921820e-04, 2.99179656e-03,\n",
       "        4.62955440e-03, 1.01944010e-03, 3.49785146e-03, 1.35643186e-03,\n",
       "        8.95215605e-04, 1.35643805e-03, 3.19865050e-03, 9.79452560e-04,\n",
       "        1.16572158e-03, 1.07108515e-02, 2.05896826e-03, 5.63230743e-03,\n",
       "        2.99381717e-03, 4.89473136e-04, 7.99322249e-04, 1.19972243e-03,\n",
       "        7.99656099e-04, 4.90039063e-04, 1.46915257e-03, 1.19938866e-03,\n",
       "        7.48074498e-04, 8.00109014e-04, 4.90958263e-04, 3.24969691e-03,\n",
       "        1.95935250e-03, 7.99822955e-04, 2.03920783e-03, 1.85426042e-03,\n",
       "        2.72704362e-03, 1.72024058e-03, 7.99667879e-04, 1.26270838e-03,\n",
       "        7.99370054e-04, 1.26402999e-03, 2.63719139e-03, 1.16608960e-03,\n",
       "        1.01961783e-03, 7.48455913e-04, 1.16600776e-03, 2.33012648e-03,\n",
       "        7.99203010e-04, 7.48303070e-04, 7.47563985e-04, 1.10055519e-02,\n",
       "        1.54871205e-03, 1.49564403e-03, 7.47844218e-04, 1.57602938e-02,\n",
       "        7.48060948e-04, 7.48532212e-04, 1.41340331e-03, 4.22184681e-03,\n",
       "        1.16416119e-03, 2.03888018e-03, 4.01971383e-03, 8.00085814e-04,\n",
       "        6.11806670e-03, 9.79783784e-04, 2.19112337e-03, 7.47334339e-04,\n",
       "        2.41557783e-03, 7.48150107e-04, 7.99608351e-04, 3.96961097e-03,\n",
       "        1.35620606e-03, 3.54240400e-03, 1.35687397e-03, 2.86949479e-03,\n",
       "        7.48213592e-04, 1.85431679e-03, 8.94096483e-04, 2.31559048e-02,\n",
       "        1.35678251e-03, 7.48086434e-04, 6.87736661e-03, 4.71422769e-03,\n",
       "        1.16583624e-03, 5.54738778e-03, 7.97376427e-03, 5.40167017e-03,\n",
       "        1.01945030e-03, 8.00752874e-04, 1.35721889e-03, 7.47538475e-04,\n",
       "        7.47742188e-04, 1.42668449e-03, 2.77707745e-03, 6.39988398e-04,\n",
       "        9.79763901e-04, 4.95799727e-03, 1.67322737e-03, 7.99143518e-04,\n",
       "        4.89785649e-04, 2.28464599e-03, 1.01983327e-03, 7.48787205e-04,\n",
       "        2.13447502e-03, 2.03863743e-03, 2.33106687e-03, 1.20232204e-03,\n",
       "        1.09475686e-03, 1.16935256e-03, 1.47683117e-03, 4.89122827e-04,\n",
       "        8.94202528e-04, 6.63480088e-04, 7.47831354e-04, 1.71994131e-03,\n",
       "        8.20381667e-07, 1.59922867e-03, 1.54994452e-03, 9.79481888e-04,\n",
       "        1.72109081e-03, 9.79287072e-04, 1.85388522e-03, 3.44029282e-03,\n",
       "        9.79316094e-04, 3.42866737e-03, 2.86943845e-03, 4.89745872e-04,\n",
       "        1.52215010e-02, 1.54898892e-03, 1.46962633e-03, 9.78424808e-04,\n",
       "        4.00067029e-04, 1.16565619e-03, 3.99685276e-04, 9.80737322e-04,\n",
       "        1.41414485e-03, 1.19962760e-03, 1.93777584e-03, 7.48545216e-04,\n",
       "        7.48761637e-04, 1.59832251e-03, 1.74317086e-03, 1.72030172e-03,\n",
       "        4.00189544e-04, 3.99518507e-04, 4.57537782e-03, 1.07581303e-02,\n",
       "        1.35587639e-03, 3.38133153e-03, 7.47640329e-04, 2.68122142e-03,\n",
       "        1.85438361e-03, 7.99870667e-04, 8.94255868e-04, 6.31957353e-04,\n",
       "        8.93989211e-04, 5.76862067e-03, 3.03245882e-03, 3.99995145e-04,\n",
       "        1.00273311e-02, 1.49602575e-03, 3.99662469e-04, 3.00721854e-03,\n",
       "        1.02046879e-03, 3.37775089e-03, 9.63885609e-03, 1.09501768e-03,\n",
       "        4.89687315e-04, 1.46901638e-03, 3.34546582e-03, 1.01933741e-03,\n",
       "        1.18215329e-02, 6.32786815e-04, 1.20069283e-03, 7.99882666e-04,\n",
       "        1.16558383e-03, 3.24899844e-03, 7.99655957e-04, 1.19941241e-03,\n",
       "        1.16564793e-03, 6.30827652e-04, 1.01982363e-03, 4.90173941e-04,\n",
       "        2.49678367e-03, 4.02698840e-03, 2.85964442e-03, 1.85230132e-03,\n",
       "        4.36014375e-03, 1.20001663e-03, 8.94415890e-04, 3.96830412e-03,\n",
       "        2.19055755e-03, 2.22646903e-03, 8.76412088e-03, 2.33259779e-03,\n",
       "        3.91728865e-03, 7.47678374e-04, 9.93378957e-07, 2.78577363e-03,\n",
       "        2.40021947e-03, 7.50356240e-04, 1.03038963e-03, 1.46971084e-03,\n",
       "        6.32410062e-04, 3.18504252e-03, 1.46875040e-03, 1.78877810e-03,\n",
       "        1.16944468e-03, 1.49542741e-03, 9.17169123e-03, 1.35592479e-03,\n",
       "        4.02990641e-03, 1.35614324e-03, 7.47691186e-04, 1.35659282e-03,\n",
       "        1.47006130e-03, 9.79335933e-04, 2.33546267e-03, 3.99923822e-04,\n",
       "        4.00140668e-04, 1.16519873e-03, 4.90037810e-04, 1.16621222e-03,\n",
       "        1.01956176e-03, 1.93830191e-03, 3.99947660e-04, 1.01965539e-03,\n",
       "        1.46926948e-03, 3.38254471e-03, 7.99775484e-04, 4.89220903e-04,\n",
       "        2.60685198e-03, 3.99852009e-04, 1.62386824e-03, 3.60997411e-03,\n",
       "        1.74532907e-03, 1.39632606e-03, 4.90312463e-04, 1.60068387e-03,\n",
       "        7.99048457e-04, 4.88657601e-04, 6.54135421e-03, 3.55861994e-03,\n",
       "        7.99012374e-04, 1.19992894e-03, 1.16620412e-03, 5.51167189e-03,\n",
       "        1.49619774e-03, 2.03917956e-03, 1.67251491e-03, 1.16703888e-03,\n",
       "        4.00019104e-04, 9.79715444e-04, 4.96145400e-03, 1.16645840e-03,\n",
       "        1.93878394e-03, 5.91771508e-03, 1.54945096e-03, 1.49683049e-03,\n",
       "        1.49673325e-03, 3.99900744e-04, 7.99799086e-04, 1.21364582e-02,\n",
       "        1.16582785e-03, 7.48022619e-04, 2.13315771e-03, 1.35589684e-03,\n",
       "        1.83421469e-02, 8.99901512e-04, 2.72629547e-03, 7.11151551e-03,\n",
       "        1.09484444e-03, 1.85271276e-03, 4.00901390e-04, 9.54655585e-03,\n",
       "        9.79686742e-04, 2.72734779e-03, 1.88809622e-03, 1.12613440e-02,\n",
       "        2.60522998e-03, 4.04827926e-03, 7.99870610e-04, 8.00794409e-04,\n",
       "        1.54861954e-03, 4.00283065e-04, 2.47946211e-03, 1.01975839e-03,\n",
       "        4.00210918e-04, 9.79579021e-04, 2.57812229e-03, 4.89297964e-04,\n",
       "        4.89259240e-04, 1.49641453e-02, 1.09527882e-03, 7.50617920e-03,\n",
       "        2.22674308e-03, 2.15509859e-03, 1.89645019e-03, 2.57802609e-03,\n",
       "        4.89259472e-04, 3.76922516e-03, 1.09523523e-03, 9.79588888e-04,\n",
       "        1.35527096e-03, 1.09527925e-03, 4.89570468e-04, 1.77849712e-03,\n",
       "        4.89648529e-04, 7.47513241e-04, 8.93935987e-04, 8.87518118e-03,\n",
       "        2.24358345e-03, 8.00073486e-04, 1.93858222e-03, 7.99572846e-04,\n",
       "        1.16613891e-03, 2.48109021e-03, 1.67285701e-03, 1.35617791e-03,\n",
       "        2.09323488e-03, 2.33055574e-03, 1.35441838e-03, 9.82867445e-03,\n",
       "        1.59742841e-03, 7.99870724e-04, 1.67214764e-03, 2.53026170e-03,\n",
       "        4.91543208e-04, 1.01837404e-03, 1.16571337e-03, 1.16604072e-03,\n",
       "        8.94202456e-04, 4.89998977e-04, 7.47793184e-04, 1.01934681e-03,\n",
       "        5.42611958e-03, 9.79841776e-04, 2.33097292e-03, 1.93952313e-03,\n",
       "        3.99995088e-04, 5.01771584e-03, 3.48594224e-03, 2.41463855e-03,\n",
       "        1.01984242e-03, 4.00289029e-04, 2.52703803e-03, 2.27819934e-03,\n",
       "        1.16580340e-03, 2.70874321e-03, 5.05515727e-03, 2.03860902e-03,\n",
       "        8.00086851e-04, 3.99756584e-04, 3.84620307e-03, 4.84009451e-03,\n",
       "        2.33182747e-03, 7.48659644e-04, 1.83301203e-03, 3.60970676e-03,\n",
       "        2.48185098e-03, 2.78496832e-03, 4.45318640e-03, 2.60749526e-03,\n",
       "        7.48213957e-04, 6.32485352e-04, 2.27989283e-03, 6.31354409e-04,\n",
       "        2.89469323e-03, 8.00613039e-04, 7.97165026e-04, 8.95121787e-04,\n",
       "        1.49602569e-03, 4.95383655e-03, 2.78396956e-03, 7.99405651e-04,\n",
       "        4.62029710e-03, 1.45962516e-03, 6.31731507e-04, 7.99810913e-04,\n",
       "        5.82901667e-03, 4.95653275e-03, 4.89979567e-04, 1.85372034e-03,\n",
       "        6.58243353e-03, 7.47741990e-04, 1.19964294e-03, 1.94609281e-03,\n",
       "        5.72547278e-03, 2.03831919e-03, 6.32560607e-04, 5.42427908e-03,\n",
       "        1.26512110e-03, 4.00066631e-04, 1.26606766e-03, 8.94895998e-04,\n",
       "        1.94345515e-03, 1.95910436e-03, 1.00441578e-02, 1.09715137e-03,\n",
       "        6.31958217e-04, 2.86885045e-03, 1.77866648e-02, 4.89570933e-04,\n",
       "        1.59972909e-03, 7.48111801e-04, 2.86432558e-03, 4.90661126e-04,\n",
       "        7.48035130e-04, 1.20000882e-03, 1.79651022e-02, 3.05868193e-03,\n",
       "        4.90370672e-04, 7.48481319e-04, 7.43867639e-04, 6.32258986e-04,\n",
       "        1.09532284e-03, 7.47805753e-04, 3.09740804e-03, 8.00121038e-04,\n",
       "        7.99990617e-04, 7.49452739e-04, 9.79384247e-04, 5.84003864e-07,\n",
       "        8.95855387e-04, 2.13565411e-03, 4.91243286e-03, 2.33114045e-03,\n",
       "        2.03865124e-03, 3.30985955e-03, 4.89998769e-04, 3.18658325e-03,\n",
       "        2.72686882e-03, 7.48035176e-04, 7.93792056e-04]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.83809524, 0.84313725, 0.84313725, 0.85714286,\n",
       "        0.83809524, 0.85436893, 0.85714286, 0.87378641, 0.85148515,\n",
       "        0.85436893, 0.81188119, 0.85148515, 0.85714286, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.8627451 , 0.83495146, 0.84313725,\n",
       "        0.81188119, 0.8627451 , 0.85436893, 0.85436893, 0.82352941,\n",
       "        0.82352941, 0.84      , 0.83495146, 0.82692308, 0.81904762,\n",
       "        0.8627451 , 0.85436893, 0.8627451 , 0.83495146, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.84313725, 0.81553398, 0.85148515,\n",
       "        0.85436893, 0.84615385, 0.83495146, 0.84313725, 0.83495146,\n",
       "        0.84      , 0.85714286, 0.85714286, 0.85148515, 0.86538462,\n",
       "        0.85714286, 0.82352941, 0.87378641, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84313725, 0.85148515, 0.84615385, 0.83495146,\n",
       "        0.83495146, 0.85436893, 0.85436893, 0.82352941, 0.84313725,\n",
       "        0.81188119, 0.85148515, 0.84313725, 0.83168317, 0.82692308,\n",
       "        0.82692308, 0.84615385, 0.83168317, 0.81188119, 0.81553398,\n",
       "        0.84313725, 0.85436893, 0.85436893, 0.83495146, 0.84615385,\n",
       "        0.84313725, 0.84      , 0.85148515, 0.83809524, 0.84313725,\n",
       "        0.8627451 , 0.8627451 , 0.84615385, 0.86538462, 0.86      ,\n",
       "        0.83168317, 0.83809524, 0.86538462, 0.84      , 0.84615385,\n",
       "        0.85714286, 0.84313725, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.85436893, 0.85436893, 0.83673469, 0.8627451 , 0.85714286,\n",
       "        0.82352941, 0.83018868, 0.86538462, 0.81553398, 0.83809524,\n",
       "        0.82692308, 0.83168317, 0.85436893, 0.81553398, 0.84313725,\n",
       "        0.85714286, 0.84615385, 0.82      , 0.84313725, 0.85148515,\n",
       "        0.84      , 0.84      , 0.8627451 , 0.85436893, 0.85714286,\n",
       "        0.82692308, 0.83168317, 0.82      , 0.84313725, 0.83168317,\n",
       "        0.84      , 0.85436893, 0.83168317, 0.83495146, 0.83495146,\n",
       "        0.81632653, 0.84615385, 0.85436893, 0.83333333, 0.85436893,\n",
       "        0.84615385, 0.82828283, 0.84615385, 0.8627451 , 0.81632653,\n",
       "        0.83495146, 0.85714286, 0.81632653, 0.85148515, 0.85436893,\n",
       "        0.80808081, 0.83495146, 0.83809524, 0.81188119, 0.83495146,\n",
       "        0.85436893, 0.82      , 0.85148515, 0.84313725, 0.83168317,\n",
       "        0.83495146, 0.85714286, 0.82      , 0.8627451 , 0.8627451 ,\n",
       "        0.83168317, 0.85148515, 0.84313725, 0.83168317, 0.84615385,\n",
       "        0.8490566 , 0.82352941, 0.84313725, 0.81188119, 0.83168317,\n",
       "        0.85148515, 0.85148515, 0.84313725, 0.84313725, 0.8490566 ,\n",
       "        0.75862069, 0.82828283, 0.82      , 0.75      , 0.83673469,\n",
       "        0.84848485, 0.75555556, 0.82828283, 0.83168317, 0.79120879,\n",
       "        0.81632653, 0.80808081, 0.76923077, 0.82474227, 0.84848485,\n",
       "        0.76923077, 0.81188119, 0.83168317, 0.76086957, 0.82      ,\n",
       "        0.84      , 0.77419355, 0.82      , 0.84848485, 0.79166667,\n",
       "        0.82352941, 0.83495146, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.77894737, 0.83168317, 0.84313725, 0.77894737, 0.82352941,\n",
       "        0.84313725, 0.79166667, 0.81632653, 0.82828283, 0.79591837,\n",
       "        0.84313725, 0.84313725, 0.78350515, 0.83168317, 0.84313725,\n",
       "        0.83168317, 0.83809524, 0.84313725, 0.84313725, 0.85714286,\n",
       "        0.83809524, 0.85436893, 0.85714286, 0.87378641, 0.85148515,\n",
       "        0.85436893, 0.81188119, 0.85148515, 0.85714286, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.8627451 , 0.83495146, 0.84313725,\n",
       "        0.81188119, 0.8627451 , 0.85436893, 0.85436893, 0.82352941,\n",
       "        0.82352941, 0.84      , 0.83495146, 0.82692308, 0.81904762,\n",
       "        0.8627451 , 0.85436893, 0.8627451 , 0.83495146, 0.83495146,\n",
       "        0.84313725, 0.84313725, 0.84313725, 0.81553398, 0.85148515,\n",
       "        0.85436893, 0.84615385, 0.83495146, 0.84313725, 0.83495146,\n",
       "        0.84      , 0.85714286, 0.85714286, 0.85148515, 0.86538462,\n",
       "        0.85714286, 0.82352941, 0.87378641, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84313725, 0.85148515, 0.84615385, 0.83495146,\n",
       "        0.83495146, 0.85436893, 0.85436893, 0.82352941, 0.84313725,\n",
       "        0.81188119, 0.85148515, 0.84313725, 0.83168317, 0.82692308,\n",
       "        0.82692308, 0.84615385, 0.83168317, 0.81188119, 0.81553398,\n",
       "        0.84313725, 0.85436893, 0.85436893, 0.83495146, 0.84615385,\n",
       "        0.84313725, 0.84      , 0.85148515, 0.83809524, 0.84313725,\n",
       "        0.8627451 , 0.8627451 , 0.84615385, 0.86538462, 0.86      ,\n",
       "        0.83168317, 0.83809524, 0.86538462, 0.84      , 0.84615385,\n",
       "        0.85714286, 0.84313725, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.85436893, 0.85436893, 0.83673469, 0.8627451 , 0.85714286,\n",
       "        0.82352941, 0.83018868, 0.86538462, 0.81553398, 0.83809524,\n",
       "        0.82692308, 0.83168317, 0.85436893, 0.81553398, 0.84313725,\n",
       "        0.85714286, 0.84615385, 0.82      , 0.84313725, 0.85148515,\n",
       "        0.84      , 0.84      , 0.8627451 , 0.85436893, 0.85714286,\n",
       "        0.82692308, 0.83168317, 0.82      , 0.84313725, 0.83168317,\n",
       "        0.84      , 0.85436893, 0.83168317, 0.83495146, 0.83495146,\n",
       "        0.81632653, 0.84615385, 0.85436893, 0.83333333, 0.85436893,\n",
       "        0.84615385, 0.82828283, 0.84615385, 0.8627451 , 0.81632653,\n",
       "        0.83495146, 0.85714286, 0.81632653, 0.85148515, 0.85436893,\n",
       "        0.80808081, 0.83495146, 0.83809524, 0.81188119, 0.83495146,\n",
       "        0.85436893, 0.82      , 0.85148515, 0.84313725, 0.83168317,\n",
       "        0.83495146, 0.85714286, 0.82      , 0.8627451 , 0.8627451 ,\n",
       "        0.83168317, 0.85148515, 0.84313725, 0.83168317, 0.84615385,\n",
       "        0.8490566 , 0.82352941, 0.84313725, 0.81188119, 0.83168317,\n",
       "        0.85148515, 0.85148515, 0.84313725, 0.84313725, 0.8490566 ,\n",
       "        0.75862069, 0.82828283, 0.82      , 0.75      , 0.83673469,\n",
       "        0.84848485, 0.75555556, 0.82828283, 0.83168317, 0.79120879,\n",
       "        0.81632653, 0.80808081, 0.76923077, 0.82474227, 0.84848485,\n",
       "        0.76923077, 0.81188119, 0.83168317, 0.76086957, 0.82      ,\n",
       "        0.84      , 0.77419355, 0.82      , 0.84848485, 0.79166667,\n",
       "        0.82352941, 0.83495146, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.77894737, 0.83168317, 0.84313725, 0.77894737, 0.82352941,\n",
       "        0.84313725, 0.79166667, 0.81632653, 0.82828283, 0.79591837,\n",
       "        0.84313725, 0.84313725, 0.78350515, 0.83168317, 0.84313725,\n",
       "        0.84313725, 0.84615385, 0.83809524, 0.85436893, 0.86538462,\n",
       "        0.86538462, 0.84615385, 0.86538462, 0.87378641, 0.84      ,\n",
       "        0.83809524, 0.82692308, 0.86      , 0.86538462, 0.85436893,\n",
       "        0.85148515, 0.85436893, 0.85436893, 0.85148515, 0.85148515,\n",
       "        0.82692308, 0.85148515, 0.83495146, 0.82692308, 0.86868687,\n",
       "        0.84615385, 0.82692308, 0.82692308, 0.83495146, 0.78095238,\n",
       "        0.84313725, 0.84615385, 0.84615385, 0.85148515, 0.83495146,\n",
       "        0.84313725, 0.83168317, 0.82      , 0.78846154, 0.8627451 ,\n",
       "        0.84615385, 0.84615385, 0.84313725, 0.84615385, 0.84313725,\n",
       "        0.85148515, 0.85436893, 0.85436893, 0.85148515, 0.84615385,\n",
       "        0.8490566 , 0.85436893, 0.87378641, 0.86538462, 0.84      ,\n",
       "        0.85436893, 0.84615385, 0.84848485, 0.88235294, 0.85714286,\n",
       "        0.86      , 0.8627451 , 0.85436893, 0.86      , 0.82692308,\n",
       "        0.82692308, 0.86      , 0.84313725, 0.83495146, 0.86      ,\n",
       "        0.8627451 , 0.84313725, 0.82      , 0.83495146, 0.84313725,\n",
       "        0.84848485, 0.84313725, 0.84313725, 0.86      , 0.85714286,\n",
       "        0.84615385, 0.84      , 0.83495146, 0.80769231, 0.84848485,\n",
       "        0.85436893, 0.8627451 , 0.84313725, 0.8490566 , 0.84615385,\n",
       "        0.84536082, 0.84      , 0.87378641, 0.84536082, 0.8627451 ,\n",
       "        0.87378641, 0.84848485, 0.8627451 , 0.87378641, 0.84      ,\n",
       "        0.85148515, 0.85148515, 0.85714286, 0.87128713, 0.84615385,\n",
       "        0.84      , 0.84615385, 0.85436893, 0.83168317, 0.85436893,\n",
       "        0.85436893, 0.83673469, 0.85148515, 0.84313725, 0.84313725,\n",
       "        0.84615385, 0.85714286, 0.82      , 0.84313725, 0.85148515,\n",
       "        0.83673469, 0.86538462, 0.84615385, 0.8627451 , 0.85436893,\n",
       "        0.85714286, 0.84848485, 0.84313725, 0.82352941, 0.84848485,\n",
       "        0.85148515, 0.84615385, 0.83168317, 0.8490566 , 0.85714286,\n",
       "        0.82978723, 0.84313725, 0.83168317, 0.83870968, 0.85148515,\n",
       "        0.8627451 , 0.82828283, 0.85436893, 0.85436893, 0.8125    ,\n",
       "        0.84      , 0.85436893, 0.80851064, 0.85148515, 0.8627451 ,\n",
       "        0.8       , 0.85148515, 0.85148515, 0.77894737, 0.86      ,\n",
       "        0.8627451 , 0.78723404, 0.85148515, 0.8627451 , 0.79166667,\n",
       "        0.8627451 , 0.85436893, 0.80412371, 0.83168317, 0.85436893,\n",
       "        0.8       , 0.85148515, 0.85148515, 0.81632653, 0.84615385,\n",
       "        0.84615385, 0.82474227, 0.84      , 0.84313725, 0.83333333,\n",
       "        0.84313725, 0.86538462, 0.81632653, 0.84615385, 0.84615385,\n",
       "        0.77272727, 0.84210526, 0.81188119, 0.77272727, 0.84210526,\n",
       "        0.83168317, 0.75      , 0.83333333, 0.84615385, 0.75      ,\n",
       "        0.8125    , 0.82      , 0.75      , 0.82105263, 0.83673469,\n",
       "        0.75      , 0.79591837, 0.84      , 0.75555556, 0.81632653,\n",
       "        0.83168317, 0.75      , 0.8125    , 0.84      , 0.74157303,\n",
       "        0.78787879, 0.83495146, 0.74725275, 0.82828283, 0.83168317,\n",
       "        0.75      , 0.8       , 0.84      , 0.74725275, 0.80808081,\n",
       "        0.83495146, 0.75555556, 0.81632653, 0.82      , 0.74157303,\n",
       "        0.83673469, 0.84      , 0.74725275, 0.80808081, 0.83495146]),\n",
       " 'split1_test_score': array([0.72727273, 0.71578947, 0.68686869, 0.70833333, 0.71428571,\n",
       "        0.71428571, 0.70103093, 0.70833333, 0.72164948, 0.69473684,\n",
       "        0.7       , 0.68627451, 0.68085106, 0.70833333, 0.72      ,\n",
       "        0.69473684, 0.72916667, 0.74747475, 0.70833333, 0.72      ,\n",
       "        0.69306931, 0.70103093, 0.74747475, 0.74      , 0.71428571,\n",
       "        0.70833333, 0.74      , 0.72164948, 0.7254902 , 0.69902913,\n",
       "        0.70103093, 0.74      , 0.7254902 , 0.71428571, 0.71428571,\n",
       "        0.74747475, 0.73469388, 0.72727273, 0.7184466 , 0.72727273,\n",
       "        0.74747475, 0.74747475, 0.70833333, 0.71428571, 0.73267327,\n",
       "        0.68085106, 0.71428571, 0.70103093, 0.6875    , 0.73469388,\n",
       "        0.70833333, 0.69473684, 0.70833333, 0.70103093, 0.6875    ,\n",
       "        0.72164948, 0.70103093, 0.70103093, 0.72164948, 0.71578947,\n",
       "        0.70103093, 0.70833333, 0.72164948, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.71428571, 0.72727273, 0.74      , 0.69473684,\n",
       "        0.70833333, 0.71578947, 0.70103093, 0.7254902 , 0.72      ,\n",
       "        0.70103093, 0.71428571, 0.74      , 0.70103093, 0.70103093,\n",
       "        0.74      , 0.72727273, 0.72      , 0.70588235, 0.70833333,\n",
       "        0.73469388, 0.76      , 0.6875    , 0.72916667, 0.74747475,\n",
       "        0.68041237, 0.6875    , 0.70103093, 0.6875    , 0.72727273,\n",
       "        0.73469388, 0.68041237, 0.70833333, 0.70833333, 0.69387755,\n",
       "        0.71578947, 0.71428571, 0.70103093, 0.72916667, 0.74226804,\n",
       "        0.70103093, 0.72164948, 0.72916667, 0.68041237, 0.73469388,\n",
       "        0.7       , 0.71428571, 0.74226804, 0.72916667, 0.6875    ,\n",
       "        0.70103093, 0.70833333, 0.68041237, 0.71428571, 0.71287129,\n",
       "        0.71428571, 0.73469388, 0.73469388, 0.70103093, 0.70103093,\n",
       "        0.71578947, 0.68041237, 0.72727273, 0.73267327, 0.71428571,\n",
       "        0.72164948, 0.73469388, 0.68041237, 0.70103093, 0.70833333,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.70103093,\n",
       "        0.70103093, 0.67346939, 0.70103093, 0.70833333, 0.65263158,\n",
       "        0.70103093, 0.71428571, 0.66666667, 0.71578947, 0.71578947,\n",
       "        0.68041237, 0.70833333, 0.70833333, 0.68041237, 0.70707071,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.72916667, 0.67346939,\n",
       "        0.70833333, 0.71428571, 0.67346939, 0.70833333, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.72164948,\n",
       "        0.70103093, 0.67346939, 0.72164948, 0.73469388, 0.68041237,\n",
       "        0.6875    , 0.73469388, 0.68041237, 0.71428571, 0.70103093,\n",
       "        0.61052632, 0.66666667, 0.69387755, 0.60869565, 0.67368421,\n",
       "        0.72164948, 0.61538462, 0.67368421, 0.6875    , 0.64583333,\n",
       "        0.67368421, 0.6875    , 0.63917526, 0.66666667, 0.70833333,\n",
       "        0.63157895, 0.68041237, 0.70103093, 0.67346939, 0.67346939,\n",
       "        0.69387755, 0.65263158, 0.69387755, 0.6875    , 0.64583333,\n",
       "        0.69387755, 0.70833333, 0.65979381, 0.68041237, 0.69387755,\n",
       "        0.65263158, 0.68041237, 0.6875    , 0.65263158, 0.69387755,\n",
       "        0.70833333, 0.68686869, 0.69387755, 0.70103093, 0.65979381,\n",
       "        0.67368421, 0.67368421, 0.65263158, 0.70103093, 0.70833333,\n",
       "        0.72727273, 0.71578947, 0.68686869, 0.70833333, 0.71428571,\n",
       "        0.71428571, 0.70103093, 0.70833333, 0.72164948, 0.69473684,\n",
       "        0.7       , 0.68627451, 0.68085106, 0.70833333, 0.72      ,\n",
       "        0.69473684, 0.72916667, 0.74747475, 0.70833333, 0.72      ,\n",
       "        0.69306931, 0.70103093, 0.74747475, 0.74      , 0.71428571,\n",
       "        0.70833333, 0.74      , 0.72164948, 0.7254902 , 0.69902913,\n",
       "        0.70103093, 0.74      , 0.7254902 , 0.71428571, 0.71428571,\n",
       "        0.74747475, 0.73469388, 0.72727273, 0.7184466 , 0.72727273,\n",
       "        0.74747475, 0.74747475, 0.70833333, 0.71428571, 0.73267327,\n",
       "        0.68085106, 0.71428571, 0.70103093, 0.6875    , 0.73469388,\n",
       "        0.70833333, 0.69473684, 0.70833333, 0.70103093, 0.6875    ,\n",
       "        0.72164948, 0.70103093, 0.70103093, 0.72164948, 0.71578947,\n",
       "        0.70103093, 0.70833333, 0.72164948, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.71428571, 0.72727273, 0.74      , 0.69473684,\n",
       "        0.70833333, 0.71578947, 0.70103093, 0.7254902 , 0.72      ,\n",
       "        0.70103093, 0.71428571, 0.74      , 0.70103093, 0.70103093,\n",
       "        0.74      , 0.72727273, 0.72      , 0.70588235, 0.70833333,\n",
       "        0.73469388, 0.76      , 0.6875    , 0.72916667, 0.74747475,\n",
       "        0.68041237, 0.6875    , 0.70103093, 0.6875    , 0.72727273,\n",
       "        0.73469388, 0.68041237, 0.70833333, 0.70833333, 0.69387755,\n",
       "        0.71578947, 0.71428571, 0.70103093, 0.72916667, 0.74226804,\n",
       "        0.70103093, 0.72164948, 0.72916667, 0.68041237, 0.73469388,\n",
       "        0.7       , 0.71428571, 0.74226804, 0.72916667, 0.6875    ,\n",
       "        0.70103093, 0.70833333, 0.68041237, 0.71428571, 0.71287129,\n",
       "        0.71428571, 0.73469388, 0.73469388, 0.70103093, 0.70103093,\n",
       "        0.71578947, 0.68041237, 0.72727273, 0.73267327, 0.71428571,\n",
       "        0.72164948, 0.73469388, 0.68041237, 0.70103093, 0.70833333,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.70103093,\n",
       "        0.70103093, 0.67346939, 0.70103093, 0.70833333, 0.65263158,\n",
       "        0.70103093, 0.71428571, 0.66666667, 0.71578947, 0.71578947,\n",
       "        0.68041237, 0.70833333, 0.70833333, 0.68041237, 0.70707071,\n",
       "        0.70103093, 0.66666667, 0.70103093, 0.72916667, 0.67346939,\n",
       "        0.70833333, 0.71428571, 0.67346939, 0.70833333, 0.70103093,\n",
       "        0.66666667, 0.70103093, 0.71428571, 0.68041237, 0.72164948,\n",
       "        0.70103093, 0.67346939, 0.72164948, 0.73469388, 0.68041237,\n",
       "        0.6875    , 0.73469388, 0.68041237, 0.71428571, 0.70103093,\n",
       "        0.61052632, 0.66666667, 0.69387755, 0.60869565, 0.67368421,\n",
       "        0.72164948, 0.61538462, 0.67368421, 0.6875    , 0.64583333,\n",
       "        0.67368421, 0.6875    , 0.63917526, 0.66666667, 0.70833333,\n",
       "        0.63157895, 0.68041237, 0.70103093, 0.67346939, 0.67346939,\n",
       "        0.69387755, 0.65263158, 0.69387755, 0.6875    , 0.64583333,\n",
       "        0.69387755, 0.70833333, 0.65979381, 0.68041237, 0.69387755,\n",
       "        0.65263158, 0.68041237, 0.6875    , 0.65263158, 0.69387755,\n",
       "        0.70833333, 0.68686869, 0.69387755, 0.70103093, 0.65979381,\n",
       "        0.67368421, 0.67368421, 0.65263158, 0.70103093, 0.70833333,\n",
       "        0.70707071, 0.73469388, 0.71428571, 0.70103093, 0.70833333,\n",
       "        0.72      , 0.70103093, 0.70103093, 0.70103093, 0.6875    ,\n",
       "        0.70103093, 0.6875    , 0.70103093, 0.71428571, 0.70707071,\n",
       "        0.70103093, 0.69473684, 0.70833333, 0.70103093, 0.73267327,\n",
       "        0.70588235, 0.70103093, 0.66666667, 0.71287129, 0.69387755,\n",
       "        0.69387755, 0.72      , 0.70707071, 0.71287129, 0.69306931,\n",
       "        0.70103093, 0.72727273, 0.73267327, 0.70103093, 0.70103093,\n",
       "        0.74      , 0.68686869, 0.73267327, 0.70707071, 0.70103093,\n",
       "        0.7       , 0.7184466 , 0.69387755, 0.70707071, 0.74      ,\n",
       "        0.68041237, 0.71428571, 0.72727273, 0.6875    , 0.70103093,\n",
       "        0.70833333, 0.68041237, 0.70707071, 0.72916667, 0.69387755,\n",
       "        0.72916667, 0.70707071, 0.70103093, 0.70103093, 0.72916667,\n",
       "        0.70103093, 0.70833333, 0.70833333, 0.68041237, 0.72164948,\n",
       "        0.7254902 , 0.70103093, 0.71428571, 0.71428571, 0.70103093,\n",
       "        0.69473684, 0.72164948, 0.67346939, 0.73469388, 0.72      ,\n",
       "        0.70103093, 0.71428571, 0.69387755, 0.69387755, 0.71428571,\n",
       "        0.72727273, 0.7       , 0.74      , 0.72      , 0.70833333,\n",
       "        0.72164948, 0.73267327, 0.69387755, 0.72164948, 0.72164948,\n",
       "        0.67346939, 0.71428571, 0.72727273, 0.67346939, 0.71428571,\n",
       "        0.72164948, 0.6875    , 0.70103093, 0.70833333, 0.68686869,\n",
       "        0.70833333, 0.71428571, 0.68041237, 0.70103093, 0.72727273,\n",
       "        0.70103093, 0.70103093, 0.70833333, 0.67346939, 0.70833333,\n",
       "        0.70103093, 0.70103093, 0.71428571, 0.70103093, 0.70103093,\n",
       "        0.70103093, 0.70833333, 0.65979381, 0.72164948, 0.71428571,\n",
       "        0.67346939, 0.70833333, 0.69473684, 0.69387755, 0.70103093,\n",
       "        0.70103093, 0.65979381, 0.72164948, 0.7184466 , 0.67346939,\n",
       "        0.72164948, 0.6875    , 0.68041237, 0.70103093, 0.70833333,\n",
       "        0.67346939, 0.72727273, 0.71428571, 0.66      , 0.70103093,\n",
       "        0.70103093, 0.66      , 0.70103093, 0.70103093, 0.63917526,\n",
       "        0.68041237, 0.69473684, 0.65306122, 0.70103093, 0.70103093,\n",
       "        0.65979381, 0.70103093, 0.70103093, 0.65979381, 0.68041237,\n",
       "        0.71428571, 0.65979381, 0.70833333, 0.70833333, 0.68041237,\n",
       "        0.70103093, 0.70103093, 0.65979381, 0.68041237, 0.71428571,\n",
       "        0.67346939, 0.72164948, 0.70833333, 0.68041237, 0.70103093,\n",
       "        0.6875    , 0.65306122, 0.68041237, 0.72916667, 0.67346939,\n",
       "        0.72164948, 0.72164948, 0.67346939, 0.70103093, 0.70103093,\n",
       "        0.61538462, 0.65306122, 0.65979381, 0.59340659, 0.65306122,\n",
       "        0.6875    , 0.59340659, 0.66666667, 0.6875    , 0.60215054,\n",
       "        0.63917526, 0.65979381, 0.57777778, 0.64583333, 0.66666667,\n",
       "        0.59340659, 0.65979381, 0.70103093, 0.61702128, 0.65306122,\n",
       "        0.66666667, 0.61702128, 0.65306122, 0.68686869, 0.61702128,\n",
       "        0.66666667, 0.70103093, 0.60215054, 0.65306122, 0.66666667,\n",
       "        0.61702128, 0.65306122, 0.68041237, 0.62365591, 0.67346939,\n",
       "        0.70103093, 0.61702128, 0.65306122, 0.66666667, 0.61702128,\n",
       "        0.63917526, 0.67346939, 0.63157895, 0.67346939, 0.70103093]),\n",
       " 'split2_test_score': array([0.75471698, 0.76923077, 0.74074074, 0.75      , 0.75      ,\n",
       "        0.73584906, 0.75728155, 0.77669903, 0.76190476, 0.74285714,\n",
       "        0.75      , 0.72897196, 0.76190476, 0.73076923, 0.75471698,\n",
       "        0.75      , 0.75471698, 0.75471698, 0.76190476, 0.75471698,\n",
       "        0.75471698, 0.76923077, 0.74766355, 0.74766355, 0.75728155,\n",
       "        0.76923077, 0.74074074, 0.73584906, 0.74074074, 0.72727273,\n",
       "        0.75      , 0.75471698, 0.74074074, 0.75      , 0.74074074,\n",
       "        0.73394495, 0.75471698, 0.73394495, 0.71559633, 0.75728155,\n",
       "        0.75471698, 0.73394495, 0.75      , 0.75471698, 0.72072072,\n",
       "        0.78431373, 0.74285714, 0.76923077, 0.75728155, 0.73786408,\n",
       "        0.75      , 0.75728155, 0.74509804, 0.75728155, 0.76923077,\n",
       "        0.73584906, 0.74285714, 0.76923077, 0.74509804, 0.75      ,\n",
       "        0.75728155, 0.76470588, 0.76190476, 0.75      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75471698, 0.75471698, 0.75      ,\n",
       "        0.76470588, 0.75      , 0.75      , 0.74766355, 0.74766355,\n",
       "        0.75      , 0.76190476, 0.75471698, 0.75      , 0.75728155,\n",
       "        0.73394495, 0.75      , 0.73394495, 0.74074074, 0.75      ,\n",
       "        0.74766355, 0.74766355, 0.75      , 0.76190476, 0.72727273,\n",
       "        0.77227723, 0.75      , 0.75471698, 0.76      , 0.74509804,\n",
       "        0.75728155, 0.77227723, 0.75728155, 0.76923077, 0.76470588,\n",
       "        0.76923077, 0.73584906, 0.76470588, 0.75      , 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.76190476, 0.75728155, 0.75471698,\n",
       "        0.73584906, 0.75728155, 0.73786408, 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.74285714, 0.74285714, 0.74074074,\n",
       "        0.75      , 0.75      , 0.74766355, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.74285714, 0.74766355, 0.74766355, 0.75      ,\n",
       "        0.74285714, 0.75471698, 0.75      , 0.75471698, 0.76923077,\n",
       "        0.77227723, 0.77669903, 0.73076923, 0.76      , 0.76470588,\n",
       "        0.73786408, 0.77227723, 0.76470588, 0.75728155, 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.77227723, 0.75728155, 0.74509804,\n",
       "        0.75      , 0.75728155, 0.75728155, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.75      , 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.73076923, 0.73786408, 0.73584906,\n",
       "        0.75      , 0.75      , 0.73786408, 0.74285714, 0.74285714,\n",
       "        0.74285714, 0.75      , 0.73584906, 0.74074074, 0.74285714,\n",
       "        0.74285714, 0.76190476, 0.73584906, 0.75      , 0.75471698,\n",
       "        0.76470588, 0.74747475, 0.74747475, 0.76470588, 0.74747475,\n",
       "        0.76      , 0.76470588, 0.76      , 0.77227723, 0.75247525,\n",
       "        0.77227723, 0.77227723, 0.76470588, 0.76      , 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.75728155, 0.76470588, 0.76470588,\n",
       "        0.74509804, 0.78431373, 0.73786408, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.75247525, 0.75728155, 0.74509804,\n",
       "        0.77227723, 0.75247525, 0.73786408, 0.72897196, 0.75      ,\n",
       "        0.75      , 0.74509804, 0.75      , 0.72380952, 0.76470588,\n",
       "        0.73786408, 0.73786408, 0.73584906, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.76923077, 0.74074074, 0.75      , 0.75      ,\n",
       "        0.73584906, 0.75728155, 0.77669903, 0.76190476, 0.74285714,\n",
       "        0.75      , 0.72897196, 0.76190476, 0.73076923, 0.75471698,\n",
       "        0.75      , 0.75471698, 0.75471698, 0.76190476, 0.75471698,\n",
       "        0.75471698, 0.76923077, 0.74766355, 0.74766355, 0.75728155,\n",
       "        0.76923077, 0.74074074, 0.73584906, 0.74074074, 0.72727273,\n",
       "        0.75      , 0.75471698, 0.74074074, 0.75      , 0.74074074,\n",
       "        0.73394495, 0.75471698, 0.73394495, 0.71559633, 0.75728155,\n",
       "        0.75471698, 0.73394495, 0.75      , 0.75471698, 0.72072072,\n",
       "        0.78431373, 0.74285714, 0.76923077, 0.75728155, 0.73786408,\n",
       "        0.75      , 0.75728155, 0.74509804, 0.75728155, 0.76923077,\n",
       "        0.73584906, 0.74285714, 0.76923077, 0.74509804, 0.75      ,\n",
       "        0.75728155, 0.76470588, 0.76190476, 0.75      , 0.75471698,\n",
       "        0.74285714, 0.75      , 0.75471698, 0.75471698, 0.75      ,\n",
       "        0.76470588, 0.75      , 0.75      , 0.74766355, 0.74766355,\n",
       "        0.75      , 0.76190476, 0.75471698, 0.75      , 0.75728155,\n",
       "        0.73394495, 0.75      , 0.73394495, 0.74074074, 0.75      ,\n",
       "        0.74766355, 0.74766355, 0.75      , 0.76190476, 0.72727273,\n",
       "        0.77227723, 0.75      , 0.75471698, 0.76      , 0.74509804,\n",
       "        0.75728155, 0.77227723, 0.75728155, 0.76923077, 0.76470588,\n",
       "        0.76923077, 0.73584906, 0.76470588, 0.75      , 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.76190476, 0.75728155, 0.75471698,\n",
       "        0.73584906, 0.75728155, 0.73786408, 0.74285714, 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.74285714, 0.74285714, 0.74074074,\n",
       "        0.75      , 0.75      , 0.74766355, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.74285714, 0.74766355, 0.74766355, 0.75      ,\n",
       "        0.74285714, 0.75471698, 0.75      , 0.75471698, 0.76923077,\n",
       "        0.77227723, 0.77669903, 0.73076923, 0.76      , 0.76470588,\n",
       "        0.73786408, 0.77227723, 0.76470588, 0.75728155, 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.77227723, 0.75728155, 0.74509804,\n",
       "        0.75      , 0.75728155, 0.75728155, 0.75      , 0.75      ,\n",
       "        0.76190476, 0.75      , 0.75728155, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.73076923, 0.73786408, 0.73584906,\n",
       "        0.75      , 0.75      , 0.73786408, 0.74285714, 0.74285714,\n",
       "        0.74285714, 0.75      , 0.73584906, 0.74074074, 0.74285714,\n",
       "        0.74285714, 0.76190476, 0.73584906, 0.75      , 0.75471698,\n",
       "        0.76470588, 0.74747475, 0.74747475, 0.76470588, 0.74747475,\n",
       "        0.76      , 0.76470588, 0.76      , 0.77227723, 0.75247525,\n",
       "        0.77227723, 0.77227723, 0.76470588, 0.76      , 0.77227723,\n",
       "        0.75728155, 0.75728155, 0.75728155, 0.76470588, 0.76470588,\n",
       "        0.74509804, 0.78431373, 0.73786408, 0.74509804, 0.74285714,\n",
       "        0.75728155, 0.75728155, 0.75247525, 0.75728155, 0.74509804,\n",
       "        0.77227723, 0.75247525, 0.73786408, 0.72897196, 0.75      ,\n",
       "        0.75      , 0.74509804, 0.75      , 0.72380952, 0.76470588,\n",
       "        0.73786408, 0.73786408, 0.73584906, 0.75      , 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.75      , 0.76190476,\n",
       "        0.74285714, 0.75728155, 0.76923077, 0.76923077, 0.73076923,\n",
       "        0.72897196, 0.74766355, 0.75      , 0.76923077, 0.75471698,\n",
       "        0.75      , 0.76923077, 0.76190476, 0.73076923, 0.72897196,\n",
       "        0.73394495, 0.75      , 0.75471698, 0.75471698, 0.75728155,\n",
       "        0.76923077, 0.76190476, 0.72380952, 0.71028037, 0.71559633,\n",
       "        0.75      , 0.74074074, 0.73394495, 0.75      , 0.76923077,\n",
       "        0.74766355, 0.73584906, 0.74074074, 0.72727273, 0.75      ,\n",
       "        0.75471698, 0.74074074, 0.75      , 0.75471698, 0.74766355,\n",
       "        0.77227723, 0.77358491, 0.76190476, 0.76470588, 0.74285714,\n",
       "        0.75728155, 0.76470588, 0.76923077, 0.75      , 0.76470588,\n",
       "        0.75      , 0.74285714, 0.75728155, 0.73786408, 0.75      ,\n",
       "        0.75728155, 0.76923077, 0.76190476, 0.74285714, 0.74074074,\n",
       "        0.72897196, 0.75728155, 0.76923077, 0.74766355, 0.76470588,\n",
       "        0.77669903, 0.76923077, 0.73076923, 0.73584906, 0.72222222,\n",
       "        0.75      , 0.73584906, 0.74074074, 0.75      , 0.76923077,\n",
       "        0.76923077, 0.72380952, 0.72897196, 0.72222222, 0.75      ,\n",
       "        0.74766355, 0.74074074, 0.75728155, 0.76190476, 0.76190476,\n",
       "        0.76      , 0.76923077, 0.75471698, 0.77227723, 0.78095238,\n",
       "        0.75728155, 0.76470588, 0.76923077, 0.75      , 0.76470588,\n",
       "        0.75      , 0.74285714, 0.77227723, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.76923077, 0.77669903, 0.75      , 0.75471698,\n",
       "        0.73584906, 0.75      , 0.76923077, 0.75471698, 0.75728155,\n",
       "        0.75728155, 0.75728155, 0.74285714, 0.74285714, 0.72897196,\n",
       "        0.74285714, 0.75      , 0.74766355, 0.75728155, 0.76923077,\n",
       "        0.76190476, 0.74285714, 0.75471698, 0.73394495, 0.75      ,\n",
       "        0.75471698, 0.74766355, 0.75728155, 0.76923077, 0.76923077,\n",
       "        0.77227723, 0.77227723, 0.75      , 0.77227723, 0.77227723,\n",
       "        0.75728155, 0.77227723, 0.75728155, 0.76923077, 0.74509804,\n",
       "        0.75247525, 0.75      , 0.75728155, 0.76923077, 0.76923077,\n",
       "        0.75728155, 0.76470588, 0.76923077, 0.74509804, 0.75728155,\n",
       "        0.75471698, 0.75728155, 0.75728155, 0.75728155, 0.75      ,\n",
       "        0.75728155, 0.76923077, 0.74509804, 0.73076923, 0.75      ,\n",
       "        0.75728155, 0.75      , 0.75728155, 0.75728155, 0.75      ,\n",
       "        0.76923077, 0.74509804, 0.73076923, 0.74285714, 0.76470588,\n",
       "        0.75      , 0.75728155, 0.76470588, 0.75      , 0.76190476,\n",
       "        0.72916667, 0.75247525, 0.76      , 0.74468085, 0.75247525,\n",
       "        0.76      , 0.74468085, 0.76      , 0.77227723, 0.72164948,\n",
       "        0.75247525, 0.75247525, 0.73684211, 0.74      , 0.76470588,\n",
       "        0.7628866 , 0.75247525, 0.76470588, 0.77227723, 0.75247525,\n",
       "        0.75247525, 0.76      , 0.76470588, 0.76470588, 0.75247525,\n",
       "        0.75728155, 0.75      , 0.76767677, 0.76767677, 0.74509804,\n",
       "        0.76767677, 0.75728155, 0.75      , 0.76470588, 0.75      ,\n",
       "        0.75728155, 0.76      , 0.75247525, 0.74509804, 0.74747475,\n",
       "        0.76470588, 0.75      , 0.77227723, 0.75728155, 0.75728155]),\n",
       " 'split3_test_score': array([0.7961165 , 0.78095238, 0.77358491, 0.78095238, 0.78095238,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.8       ,\n",
       "        0.78095238, 0.76190476, 0.79245283, 0.78095238, 0.78095238,\n",
       "        0.81132075, 0.7961165 , 0.8       , 0.79245283, 0.76190476,\n",
       "        0.77358491, 0.78846154, 0.76190476, 0.79245283, 0.80769231,\n",
       "        0.7961165 , 0.78846154, 0.78504673, 0.76190476, 0.76190476,\n",
       "        0.8       , 0.76923077, 0.76923077, 0.81904762, 0.80769231,\n",
       "        0.78846154, 0.78095238, 0.76923077, 0.76923077, 0.7962963 ,\n",
       "        0.76190476, 0.77358491, 0.80769231, 0.8       , 0.78504673,\n",
       "        0.78431373, 0.78095238, 0.76190476, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.77358491, 0.8       , 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.79245283, 0.76190476,\n",
       "        0.76190476, 0.79245283, 0.78095238, 0.75      , 0.78095238,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.76923077, 0.76923077,\n",
       "        0.80373832, 0.77358491, 0.78095238, 0.81904762, 0.7961165 ,\n",
       "        0.78095238, 0.76923077, 0.76923077, 0.76923077, 0.80769231,\n",
       "        0.75      , 0.78095238, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.76923077, 0.78095238,\n",
       "        0.78095238, 0.76923077, 0.8       , 0.8       , 0.78846154,\n",
       "        0.8       , 0.77358491, 0.78095238, 0.78846154, 0.77669903,\n",
       "        0.78095238, 0.80769231, 0.7961165 , 0.78095238, 0.76923077,\n",
       "        0.76923077, 0.78095238, 0.77669903, 0.77358491, 0.78095238,\n",
       "        0.80769231, 0.7961165 , 0.78846154, 0.76190476, 0.76923077,\n",
       "        0.79245283, 0.76190476, 0.76923077, 0.81132075, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76923077, 0.8       ,\n",
       "        0.77358491, 0.76923077, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.75728155, 0.77669903, 0.78095238, 0.75728155, 0.78846154,\n",
       "        0.78095238, 0.75728155, 0.78846154, 0.78504673, 0.76470588,\n",
       "        0.79245283, 0.78095238, 0.76470588, 0.79245283, 0.78846154,\n",
       "        0.75728155, 0.7961165 , 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.7961165 , 0.80769231, 0.78846154, 0.77669903, 0.76190476,\n",
       "        0.8       , 0.8       , 0.77669903, 0.77669903, 0.80769231,\n",
       "        0.7961165 , 0.79245283, 0.77669903, 0.76923077, 0.8       ,\n",
       "        0.80769231, 0.78095238, 0.77669903, 0.80769231, 0.7961165 ,\n",
       "        0.7628866 , 0.75728155, 0.76923077, 0.7628866 , 0.75728155,\n",
       "        0.76190476, 0.74      , 0.75728155, 0.76190476, 0.78350515,\n",
       "        0.75728155, 0.78846154, 0.78350515, 0.75728155, 0.76923077,\n",
       "        0.71428571, 0.75728155, 0.76923077, 0.78787879, 0.78095238,\n",
       "        0.78846154, 0.7755102 , 0.78095238, 0.78095238, 0.72727273,\n",
       "        0.76923077, 0.78095238, 0.78787879, 0.77669903, 0.80769231,\n",
       "        0.78787879, 0.77669903, 0.8       , 0.74509804, 0.78095238,\n",
       "        0.78846154, 0.80808081, 0.8       , 0.80769231, 0.78      ,\n",
       "        0.77669903, 0.8       , 0.75247525, 0.78846154, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.77358491, 0.78095238, 0.78095238,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.8       ,\n",
       "        0.78095238, 0.76190476, 0.79245283, 0.78095238, 0.78095238,\n",
       "        0.81132075, 0.7961165 , 0.8       , 0.79245283, 0.76190476,\n",
       "        0.77358491, 0.78846154, 0.76190476, 0.79245283, 0.80769231,\n",
       "        0.7961165 , 0.78846154, 0.78504673, 0.76190476, 0.76190476,\n",
       "        0.8       , 0.76923077, 0.76923077, 0.81904762, 0.80769231,\n",
       "        0.78846154, 0.78095238, 0.76923077, 0.76923077, 0.7962963 ,\n",
       "        0.76190476, 0.77358491, 0.80769231, 0.8       , 0.78504673,\n",
       "        0.78431373, 0.78095238, 0.76190476, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.8       , 0.79245283, 0.78846154,\n",
       "        0.78095238, 0.77358491, 0.8       , 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.79245283, 0.76190476,\n",
       "        0.76190476, 0.79245283, 0.78095238, 0.75      , 0.78095238,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.76923077, 0.76923077,\n",
       "        0.80373832, 0.77358491, 0.78095238, 0.81904762, 0.7961165 ,\n",
       "        0.78095238, 0.76923077, 0.76923077, 0.76923077, 0.80769231,\n",
       "        0.75      , 0.78095238, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76923077, 0.78095238, 0.78095238, 0.76923077, 0.78095238,\n",
       "        0.78095238, 0.76923077, 0.8       , 0.8       , 0.78846154,\n",
       "        0.8       , 0.77358491, 0.78095238, 0.78846154, 0.77669903,\n",
       "        0.78095238, 0.80769231, 0.7961165 , 0.78095238, 0.76923077,\n",
       "        0.76923077, 0.78095238, 0.77669903, 0.77358491, 0.78095238,\n",
       "        0.80769231, 0.7961165 , 0.78846154, 0.76190476, 0.76923077,\n",
       "        0.79245283, 0.76190476, 0.76923077, 0.81132075, 0.7961165 ,\n",
       "        0.7961165 , 0.80769231, 0.76923077, 0.76923077, 0.8       ,\n",
       "        0.77358491, 0.76923077, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.75728155, 0.77669903, 0.78095238, 0.75728155, 0.78846154,\n",
       "        0.78095238, 0.75728155, 0.78846154, 0.78504673, 0.76470588,\n",
       "        0.79245283, 0.78095238, 0.76470588, 0.79245283, 0.78846154,\n",
       "        0.75728155, 0.7961165 , 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.78846154, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.7961165 , 0.80769231, 0.78846154, 0.77669903, 0.76190476,\n",
       "        0.8       , 0.8       , 0.77669903, 0.77669903, 0.80769231,\n",
       "        0.7961165 , 0.79245283, 0.77669903, 0.76923077, 0.8       ,\n",
       "        0.80769231, 0.78095238, 0.77669903, 0.80769231, 0.7961165 ,\n",
       "        0.7628866 , 0.75728155, 0.76923077, 0.7628866 , 0.75728155,\n",
       "        0.76190476, 0.74      , 0.75728155, 0.76190476, 0.78350515,\n",
       "        0.75728155, 0.78846154, 0.78350515, 0.75728155, 0.76923077,\n",
       "        0.71428571, 0.75728155, 0.76923077, 0.78787879, 0.78095238,\n",
       "        0.78846154, 0.7755102 , 0.78095238, 0.78095238, 0.72727273,\n",
       "        0.76923077, 0.78095238, 0.78787879, 0.77669903, 0.80769231,\n",
       "        0.78787879, 0.77669903, 0.8       , 0.74509804, 0.78095238,\n",
       "        0.78846154, 0.80808081, 0.8       , 0.80769231, 0.78      ,\n",
       "        0.77669903, 0.8       , 0.75247525, 0.78846154, 0.7961165 ,\n",
       "        0.77227723, 0.78095238, 0.77669903, 0.76923077, 0.78846154,\n",
       "        0.78095238, 0.78095238, 0.7961165 , 0.78846154, 0.79245283,\n",
       "        0.78095238, 0.78095238, 0.81132075, 0.78095238, 0.78095238,\n",
       "        0.81132075, 0.80769231, 0.8       , 0.78846154, 0.76923077,\n",
       "        0.75471698, 0.8       , 0.78846154, 0.78095238, 0.82242991,\n",
       "        0.80769231, 0.80392157, 0.78846154, 0.76190476, 0.75471698,\n",
       "        0.8       , 0.78095238, 0.78504673, 0.79245283, 0.80769231,\n",
       "        0.78846154, 0.78095238, 0.75471698, 0.76190476, 0.8       ,\n",
       "        0.78095238, 0.78504673, 0.81132075, 0.80769231, 0.80769231,\n",
       "        0.75728155, 0.8       , 0.79245283, 0.75728155, 0.78095238,\n",
       "        0.78846154, 0.75728155, 0.78846154, 0.7961165 , 0.77669903,\n",
       "        0.79245283, 0.78095238, 0.79245283, 0.78846154, 0.78846154,\n",
       "        0.78095238, 0.80769231, 0.7961165 , 0.78095238, 0.76190476,\n",
       "        0.77358491, 0.79245283, 0.78095238, 0.78095238, 0.79245283,\n",
       "        0.7961165 , 0.80769231, 0.78846154, 0.76923077, 0.76190476,\n",
       "        0.79245283, 0.78095238, 0.77358491, 0.8       , 0.80769231,\n",
       "        0.80769231, 0.7961165 , 0.76923077, 0.76190476, 0.8       ,\n",
       "        0.76190476, 0.76923077, 0.79245283, 0.80769231, 0.8       ,\n",
       "        0.75728155, 0.80769231, 0.77358491, 0.75728155, 0.79245283,\n",
       "        0.78846154, 0.75728155, 0.79245283, 0.8       , 0.76470588,\n",
       "        0.8       , 0.78504673, 0.75728155, 0.80373832, 0.78846154,\n",
       "        0.75728155, 0.81132075, 0.81132075, 0.76923077, 0.79245283,\n",
       "        0.77358491, 0.76923077, 0.77669903, 0.77358491, 0.76923077,\n",
       "        0.80769231, 0.7961165 , 0.78846154, 0.78095238, 0.76190476,\n",
       "        0.78846154, 0.77669903, 0.75728155, 0.78095238, 0.80769231,\n",
       "        0.80769231, 0.78846154, 0.76190476, 0.76190476, 0.78846154,\n",
       "        0.79245283, 0.76923077, 0.78846154, 0.80769231, 0.8       ,\n",
       "        0.78      , 0.77669903, 0.78504673, 0.76767677, 0.76923077,\n",
       "        0.79245283, 0.75510204, 0.77358491, 0.78504673, 0.78      ,\n",
       "        0.81132075, 0.79245283, 0.77227723, 0.80373832, 0.80373832,\n",
       "        0.77227723, 0.80373832, 0.81132075, 0.78      , 0.80373832,\n",
       "        0.79245283, 0.78      , 0.80373832, 0.78846154, 0.76767677,\n",
       "        0.80373832, 0.81904762, 0.79207921, 0.8       , 0.76923077,\n",
       "        0.79207921, 0.81904762, 0.78095238, 0.76767677, 0.81904762,\n",
       "        0.81904762, 0.78      , 0.78846154, 0.76190476, 0.77227723,\n",
       "        0.8       , 0.76923077, 0.76767677, 0.81904762, 0.80769231,\n",
       "        0.75789474, 0.78      , 0.76923077, 0.75789474, 0.78      ,\n",
       "        0.76923077, 0.75      , 0.78      , 0.76923077, 0.75789474,\n",
       "        0.77227723, 0.76923077, 0.75      , 0.77227723, 0.76923077,\n",
       "        0.74226804, 0.77227723, 0.76923077, 0.76595745, 0.77227723,\n",
       "        0.78095238, 0.75789474, 0.77227723, 0.78095238, 0.75      ,\n",
       "        0.77227723, 0.78095238, 0.75789474, 0.78431373, 0.79245283,\n",
       "        0.75789474, 0.77227723, 0.78846154, 0.7628866 , 0.76      ,\n",
       "        0.78095238, 0.77083333, 0.7961165 , 0.8       , 0.75789474,\n",
       "        0.7961165 , 0.8       , 0.77083333, 0.77227723, 0.79245283]),\n",
       " 'split4_test_score': array([0.7628866 , 0.78      , 0.78      , 0.70707071, 0.7628866 ,\n",
       "        0.75510204, 0.73267327, 0.72727273, 0.7628866 , 0.79207921,\n",
       "        0.78787879, 0.7755102 , 0.74747475, 0.72916667, 0.76767677,\n",
       "        0.74      , 0.73684211, 0.75510204, 0.8       , 0.8       ,\n",
       "        0.79591837, 0.78      , 0.74226804, 0.76767677, 0.76923077,\n",
       "        0.75510204, 0.77227723, 0.76      , 0.80392157, 0.77227723,\n",
       "        0.7961165 , 0.76      , 0.76      , 0.78431373, 0.75510204,\n",
       "        0.77227723, 0.76923077, 0.79207921, 0.76      , 0.7961165 ,\n",
       "        0.76      , 0.75510204, 0.77669903, 0.75510204, 0.77227723,\n",
       "        0.74226804, 0.78      , 0.81188119, 0.73469388, 0.74509804,\n",
       "        0.74747475, 0.72727273, 0.75247525, 0.73469388, 0.7628866 ,\n",
       "        0.80808081, 0.78      , 0.76      , 0.76      , 0.73469388,\n",
       "        0.72164948, 0.72916667, 0.75      , 0.8       , 0.80808081,\n",
       "        0.82      , 0.76767677, 0.75510204, 0.78787879, 0.76767677,\n",
       "        0.7628866 , 0.75510204, 0.77227723, 0.79207921, 0.8       ,\n",
       "        0.77227723, 0.76      , 0.76767677, 0.74747475, 0.75510204,\n",
       "        0.75510204, 0.8       , 0.80392157, 0.79207921, 0.77669903,\n",
       "        0.76      , 0.76767677, 0.78      , 0.75510204, 0.76767677,\n",
       "        0.72164948, 0.80392157, 0.79207921, 0.70833333, 0.77227723,\n",
       "        0.74      , 0.71428571, 0.73267327, 0.72727273, 0.75510204,\n",
       "        0.78787879, 0.80412371, 0.74226804, 0.77227723, 0.75510204,\n",
       "        0.73469388, 0.76      , 0.72916667, 0.75510204, 0.80808081,\n",
       "        0.81632653, 0.74226804, 0.78      , 0.75      , 0.74747475,\n",
       "        0.74226804, 0.75510204, 0.77227723, 0.8       , 0.82      ,\n",
       "        0.75510204, 0.76767677, 0.78787879, 0.73469388, 0.75510204,\n",
       "        0.75510204, 0.77669903, 0.81553398, 0.81188119, 0.76470588,\n",
       "        0.78787879, 0.78      , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.7173913 , 0.76      , 0.78431373, 0.7173913 , 0.74747475,\n",
       "        0.77669903, 0.7173913 , 0.73469388, 0.72727273, 0.72164948,\n",
       "        0.78      , 0.78787879, 0.71428571, 0.74      , 0.76      ,\n",
       "        0.72164948, 0.72727273, 0.73267327, 0.73684211, 0.78787879,\n",
       "        0.80412371, 0.70707071, 0.76470588, 0.78      , 0.71428571,\n",
       "        0.73267327, 0.75510204, 0.74      , 0.79591837, 0.8       ,\n",
       "        0.75247525, 0.77227723, 0.78787879, 0.74      , 0.74      ,\n",
       "        0.75      , 0.76470588, 0.78431373, 0.8       , 0.73267327,\n",
       "        0.8       , 0.78787879, 0.74      , 0.76      , 0.74226804,\n",
       "        0.74157303, 0.75      , 0.74226804, 0.75      , 0.73684211,\n",
       "        0.73469388, 0.74157303, 0.74226804, 0.73469388, 0.72340426,\n",
       "        0.75      , 0.75510204, 0.73684211, 0.74226804, 0.72727273,\n",
       "        0.72916667, 0.75510204, 0.72      , 0.7311828 , 0.75510204,\n",
       "        0.7755102 , 0.74468085, 0.74226804, 0.75510204, 0.72916667,\n",
       "        0.74226804, 0.73469388, 0.75789474, 0.76      , 0.78      ,\n",
       "        0.75      , 0.7628866 , 0.76767677, 0.71578947, 0.75510204,\n",
       "        0.76      , 0.7755102 , 0.78431373, 0.80769231, 0.75      ,\n",
       "        0.74747475, 0.78      , 0.71578947, 0.78787879, 0.76767677,\n",
       "        0.7628866 , 0.78      , 0.78      , 0.70707071, 0.7628866 ,\n",
       "        0.75510204, 0.73267327, 0.72727273, 0.7628866 , 0.79207921,\n",
       "        0.78787879, 0.7755102 , 0.74747475, 0.72916667, 0.76767677,\n",
       "        0.74      , 0.73684211, 0.75510204, 0.8       , 0.8       ,\n",
       "        0.79591837, 0.78      , 0.74226804, 0.76767677, 0.76923077,\n",
       "        0.75510204, 0.77227723, 0.76      , 0.80392157, 0.77227723,\n",
       "        0.7961165 , 0.76      , 0.76      , 0.78431373, 0.75510204,\n",
       "        0.77227723, 0.76923077, 0.79207921, 0.76      , 0.7961165 ,\n",
       "        0.76      , 0.75510204, 0.77669903, 0.75510204, 0.77227723,\n",
       "        0.74226804, 0.78      , 0.81188119, 0.73469388, 0.74509804,\n",
       "        0.74747475, 0.72727273, 0.75247525, 0.73469388, 0.7628866 ,\n",
       "        0.80808081, 0.78      , 0.76      , 0.76      , 0.73469388,\n",
       "        0.72164948, 0.72916667, 0.75      , 0.8       , 0.80808081,\n",
       "        0.82      , 0.76767677, 0.75510204, 0.78787879, 0.76767677,\n",
       "        0.7628866 , 0.75510204, 0.77227723, 0.79207921, 0.8       ,\n",
       "        0.77227723, 0.76      , 0.76767677, 0.74747475, 0.75510204,\n",
       "        0.75510204, 0.8       , 0.80392157, 0.79207921, 0.77669903,\n",
       "        0.76      , 0.76767677, 0.78      , 0.75510204, 0.76767677,\n",
       "        0.72164948, 0.80392157, 0.79207921, 0.70833333, 0.77227723,\n",
       "        0.74      , 0.71428571, 0.73267327, 0.72727273, 0.75510204,\n",
       "        0.78787879, 0.80412371, 0.74226804, 0.77227723, 0.75510204,\n",
       "        0.73469388, 0.76      , 0.72916667, 0.75510204, 0.80808081,\n",
       "        0.81632653, 0.74226804, 0.78      , 0.75      , 0.74747475,\n",
       "        0.74226804, 0.75510204, 0.77227723, 0.8       , 0.82      ,\n",
       "        0.75510204, 0.76767677, 0.78787879, 0.73469388, 0.75510204,\n",
       "        0.75510204, 0.77669903, 0.81553398, 0.81188119, 0.76470588,\n",
       "        0.78787879, 0.78      , 0.74      , 0.76767677, 0.75510204,\n",
       "        0.7173913 , 0.76      , 0.78431373, 0.7173913 , 0.74747475,\n",
       "        0.77669903, 0.7173913 , 0.73469388, 0.72727273, 0.72164948,\n",
       "        0.78      , 0.78787879, 0.71428571, 0.74      , 0.76      ,\n",
       "        0.72164948, 0.72727273, 0.73267327, 0.73684211, 0.78787879,\n",
       "        0.80412371, 0.70707071, 0.76470588, 0.78      , 0.71428571,\n",
       "        0.73267327, 0.75510204, 0.74      , 0.79591837, 0.8       ,\n",
       "        0.75247525, 0.77227723, 0.78787879, 0.74      , 0.74      ,\n",
       "        0.75      , 0.76470588, 0.78431373, 0.8       , 0.73267327,\n",
       "        0.8       , 0.78787879, 0.74      , 0.76      , 0.74226804,\n",
       "        0.74157303, 0.75      , 0.74226804, 0.75      , 0.73684211,\n",
       "        0.73469388, 0.74157303, 0.74226804, 0.73469388, 0.72340426,\n",
       "        0.75      , 0.75510204, 0.73684211, 0.74226804, 0.72727273,\n",
       "        0.72916667, 0.75510204, 0.72      , 0.7311828 , 0.75510204,\n",
       "        0.7755102 , 0.74468085, 0.74226804, 0.75510204, 0.72916667,\n",
       "        0.74226804, 0.73469388, 0.75789474, 0.76      , 0.78      ,\n",
       "        0.75      , 0.7628866 , 0.76767677, 0.71578947, 0.75510204,\n",
       "        0.76      , 0.7755102 , 0.78431373, 0.80769231, 0.75      ,\n",
       "        0.74747475, 0.78      , 0.71578947, 0.78787879, 0.76767677,\n",
       "        0.75510204, 0.8       , 0.79207921, 0.72      , 0.72164948,\n",
       "        0.76      , 0.7       , 0.73267327, 0.73267327, 0.76190476,\n",
       "        0.80808081, 0.7755102 , 0.73584906, 0.73469388, 0.75510204,\n",
       "        0.70588235, 0.72727273, 0.73469388, 0.79207921, 0.81632653,\n",
       "        0.8       , 0.7254902 , 0.73469388, 0.7755102 , 0.73786408,\n",
       "        0.73267327, 0.78431373, 0.76190476, 0.81188119, 0.7755102 ,\n",
       "        0.77358491, 0.76470588, 0.76470588, 0.73786408, 0.74      ,\n",
       "        0.76470588, 0.78095238, 0.8       , 0.76767677, 0.76923077,\n",
       "        0.74509804, 0.76470588, 0.76190476, 0.73267327, 0.76470588,\n",
       "        0.73469388, 0.78787879, 0.82      , 0.72727273, 0.78      ,\n",
       "        0.71428571, 0.71428571, 0.72727273, 0.74      , 0.74747475,\n",
       "        0.78350515, 0.79591837, 0.72      , 0.74509804, 0.74      ,\n",
       "        0.7254902 , 0.7       , 0.74226804, 0.76923077, 0.81632653,\n",
       "        0.82828283, 0.74509804, 0.76      , 0.76      , 0.73786408,\n",
       "        0.71428571, 0.73469388, 0.76923077, 0.8       , 0.82      ,\n",
       "        0.76635514, 0.77227723, 0.79207921, 0.76190476, 0.7254902 ,\n",
       "        0.73267327, 0.79245283, 0.7961165 , 0.79207921, 0.78504673,\n",
       "        0.77669903, 0.76470588, 0.76190476, 0.74      , 0.73267327,\n",
       "        0.72340426, 0.77669903, 0.78431373, 0.72916667, 0.78846154,\n",
       "        0.73469388, 0.70833333, 0.72      , 0.75247525, 0.72      ,\n",
       "        0.78      , 0.79166667, 0.71428571, 0.73267327, 0.74226804,\n",
       "        0.70707071, 0.7254902 , 0.72727273, 0.73267327, 0.77227723,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.78      , 0.72      ,\n",
       "        0.7254902 , 0.72727273, 0.75728155, 0.80392157, 0.82      ,\n",
       "        0.75247525, 0.78846154, 0.78      , 0.73267327, 0.74      ,\n",
       "        0.74226804, 0.78846154, 0.78846154, 0.81188119, 0.77669903,\n",
       "        0.75728155, 0.75728155, 0.74509804, 0.74      , 0.76767677,\n",
       "        0.73913043, 0.74747475, 0.78431373, 0.72527473, 0.73267327,\n",
       "        0.78095238, 0.7311828 , 0.74747475, 0.74509804, 0.72916667,\n",
       "        0.74      , 0.78      , 0.72727273, 0.76923077, 0.76470588,\n",
       "        0.72727273, 0.7254902 , 0.73786408, 0.74747475, 0.75728155,\n",
       "        0.78787879, 0.74747475, 0.75728155, 0.76470588, 0.74      ,\n",
       "        0.7254902 , 0.71287129, 0.74747475, 0.78846154, 0.80392157,\n",
       "        0.73469388, 0.77358491, 0.76923077, 0.73469388, 0.75      ,\n",
       "        0.74509804, 0.72164948, 0.76923077, 0.7961165 , 0.75      ,\n",
       "        0.78095238, 0.76470588, 0.73469388, 0.75      , 0.75247525,\n",
       "        0.66666667, 0.72527473, 0.72164948, 0.68131868, 0.73913043,\n",
       "        0.71428571, 0.65168539, 0.7311828 , 0.72164948, 0.68131868,\n",
       "        0.72164948, 0.72727273, 0.68131868, 0.72164948, 0.72      ,\n",
       "        0.69565217, 0.71428571, 0.74509804, 0.7173913 , 0.72727273,\n",
       "        0.74      , 0.68888889, 0.72727273, 0.7254902 , 0.68888889,\n",
       "        0.72727273, 0.73267327, 0.7173913 , 0.76767677, 0.75      ,\n",
       "        0.7173913 , 0.72727273, 0.75247525, 0.7032967 , 0.73469388,\n",
       "        0.73267327, 0.69565217, 0.76      , 0.76923077, 0.7173913 ,\n",
       "        0.73469388, 0.75728155, 0.7173913 , 0.72727273, 0.74      ]),\n",
       " 'mean_test_score': array([0.7745352 , 0.77681357, 0.76486632, 0.75789874, 0.77305351,\n",
       "        0.76485689, 0.76526141, 0.77388959, 0.78173776, 0.77623167,\n",
       "        0.77464002, 0.75290853, 0.76683371, 0.76127289, 0.77165952,\n",
       "        0.76783897, 0.7719959 , 0.78400777, 0.77952848, 0.7759518 ,\n",
       "        0.76583415, 0.78029367, 0.77073601, 0.78043242, 0.77440395,\n",
       "        0.77046241, 0.7762959 , 0.76749935, 0.77179607, 0.75590629,\n",
       "        0.78197851, 0.77566334, 0.77164136, 0.7805197 , 0.77055445,\n",
       "        0.77705914, 0.77654625, 0.77313298, 0.75576154, 0.78569045,\n",
       "        0.77569308, 0.7712521 , 0.77553523, 0.7734484 , 0.76913388,\n",
       "        0.76634931, 0.77504762, 0.7802381 , 0.76388442, 0.77430043,\n",
       "        0.7702825 , 0.75528109, 0.77593861, 0.77016876, 0.76795241,\n",
       "        0.78018013, 0.76812205, 0.77634937, 0.76992008, 0.76327744,\n",
       "        0.76067499, 0.77285342, 0.7791231 , 0.77340263, 0.77234347,\n",
       "        0.76732862, 0.77518009, 0.77223628, 0.77285579, 0.76405781,\n",
       "        0.77179308, 0.77494753, 0.77253673, 0.76926898, 0.77048566,\n",
       "        0.77403675, 0.77282886, 0.77954301, 0.77050095, 0.77113697,\n",
       "        0.77062733, 0.7773007 , 0.77571649, 0.76920566, 0.77717239,\n",
       "        0.77102051, 0.78380756, 0.77426923, 0.78153492, 0.77667532,\n",
       "        0.7550506 , 0.77209384, 0.77883282, 0.75301282, 0.77435084,\n",
       "        0.77401413, 0.75586867, 0.77053142, 0.77404429, 0.76280564,\n",
       "        0.78545359, 0.77644246, 0.76513839, 0.78053011, 0.7776987 ,\n",
       "        0.75949763, 0.7753624 , 0.77634784, 0.75785647, 0.78096353,\n",
       "        0.76966589, 0.76529417, 0.77824002, 0.76222854, 0.76326919,\n",
       "        0.77308314, 0.77259746, 0.76080166, 0.77243697, 0.77886559,\n",
       "        0.77036812, 0.77085508, 0.78044242, 0.7702829 , 0.77187847,\n",
       "        0.77116717, 0.7678688 , 0.77594021, 0.78091721, 0.77213495,\n",
       "        0.77319406, 0.77860211, 0.76195757, 0.77089853, 0.77121583,\n",
       "        0.74598866, 0.77211657, 0.772938  , 0.74968371, 0.77120841,\n",
       "        0.76854005, 0.74974046, 0.76700921, 0.76813589, 0.74551814,\n",
       "        0.77314335, 0.77950826, 0.7468524 , 0.7714018 , 0.7727436 ,\n",
       "        0.74348484, 0.76479112, 0.76881514, 0.75351944, 0.77217067,\n",
       "        0.78197797, 0.74643978, 0.77339127, 0.7771727 , 0.74779889,\n",
       "        0.76587122, 0.77830089, 0.75054003, 0.77631198, 0.77230597,\n",
       "        0.76016502, 0.77495866, 0.77197297, 0.75433034, 0.77167056,\n",
       "        0.76781224, 0.7608315 , 0.77232971, 0.77130932, 0.75752519,\n",
       "        0.77790692, 0.78338299, 0.75521954, 0.77502306, 0.76863781,\n",
       "        0.7276625 , 0.74994116, 0.75457022, 0.72725763, 0.75040346,\n",
       "        0.76534659, 0.72344382, 0.75230333, 0.75761181, 0.73928536,\n",
       "        0.7539139 , 0.76228432, 0.73869183, 0.75019171, 0.76511978,\n",
       "        0.72030873, 0.75239174, 0.75584528, 0.74362128, 0.75884594,\n",
       "        0.76858947, 0.74626598, 0.75499241, 0.76342746, 0.72735931,\n",
       "        0.75723747, 0.76324252, 0.74739799, 0.75649475, 0.76933358,\n",
       "        0.74834699, 0.76083128, 0.76723562, 0.72428768, 0.76069228,\n",
       "        0.76998643, 0.76144488, 0.76890356, 0.77370158, 0.75008361,\n",
       "        0.75577186, 0.76693711, 0.7280501 , 0.77181088, 0.7716242 ,\n",
       "        0.7745352 , 0.77681357, 0.76486632, 0.75789874, 0.77305351,\n",
       "        0.76485689, 0.76526141, 0.77388959, 0.78173776, 0.77623167,\n",
       "        0.77464002, 0.75290853, 0.76683371, 0.76127289, 0.77165952,\n",
       "        0.76783897, 0.7719959 , 0.78400777, 0.77952848, 0.7759518 ,\n",
       "        0.76583415, 0.78029367, 0.77073601, 0.78043242, 0.77440395,\n",
       "        0.77046241, 0.7762959 , 0.76749935, 0.77179607, 0.75590629,\n",
       "        0.78197851, 0.77566334, 0.77164136, 0.7805197 , 0.77055445,\n",
       "        0.77705914, 0.77654625, 0.77313298, 0.75576154, 0.78569045,\n",
       "        0.77569308, 0.7712521 , 0.77553523, 0.7734484 , 0.76913388,\n",
       "        0.76634931, 0.77504762, 0.7802381 , 0.76388442, 0.77430043,\n",
       "        0.7702825 , 0.75528109, 0.77593861, 0.77016876, 0.76795241,\n",
       "        0.78018013, 0.76812205, 0.77634937, 0.76992008, 0.76327744,\n",
       "        0.76067499, 0.77285342, 0.7791231 , 0.77340263, 0.77234347,\n",
       "        0.76732862, 0.77518009, 0.77223628, 0.77285579, 0.76405781,\n",
       "        0.77179308, 0.77494753, 0.77253673, 0.76926898, 0.77048566,\n",
       "        0.77403675, 0.77282886, 0.77954301, 0.77050095, 0.77113697,\n",
       "        0.77062733, 0.7773007 , 0.77571649, 0.76920566, 0.77717239,\n",
       "        0.77102051, 0.78380756, 0.77426923, 0.78153492, 0.77667532,\n",
       "        0.7550506 , 0.77209384, 0.77883282, 0.75301282, 0.77435084,\n",
       "        0.77401413, 0.75586867, 0.77053142, 0.77404429, 0.76280564,\n",
       "        0.78545359, 0.77644246, 0.76513839, 0.78053011, 0.7776987 ,\n",
       "        0.75949763, 0.7753624 , 0.77634784, 0.75785647, 0.78096353,\n",
       "        0.76966589, 0.76529417, 0.77824002, 0.76222854, 0.76326919,\n",
       "        0.77308314, 0.77259746, 0.76080166, 0.77243697, 0.77886559,\n",
       "        0.77036812, 0.77085508, 0.78044242, 0.7702829 , 0.77187847,\n",
       "        0.77116717, 0.7678688 , 0.77594021, 0.78091721, 0.77213495,\n",
       "        0.77319406, 0.77860211, 0.76195757, 0.77089853, 0.77121583,\n",
       "        0.74598866, 0.77211657, 0.772938  , 0.74968371, 0.77120841,\n",
       "        0.76854005, 0.74974046, 0.76700921, 0.76813589, 0.74551814,\n",
       "        0.77314335, 0.77950826, 0.7468524 , 0.7714018 , 0.7727436 ,\n",
       "        0.74348484, 0.76479112, 0.76881514, 0.75351944, 0.77217067,\n",
       "        0.78197797, 0.74643978, 0.77339127, 0.7771727 , 0.74779889,\n",
       "        0.76587122, 0.77830089, 0.75054003, 0.77631198, 0.77230597,\n",
       "        0.76016502, 0.77495866, 0.77197297, 0.75433034, 0.77167056,\n",
       "        0.76781224, 0.7608315 , 0.77232971, 0.77130932, 0.75752519,\n",
       "        0.77790692, 0.78338299, 0.75521954, 0.77502306, 0.76863781,\n",
       "        0.7276625 , 0.74994116, 0.75457022, 0.72725763, 0.75040346,\n",
       "        0.76534659, 0.72344382, 0.75230333, 0.75761181, 0.73928536,\n",
       "        0.7539139 , 0.76228432, 0.73869183, 0.75019171, 0.76511978,\n",
       "        0.72030873, 0.75239174, 0.75584528, 0.74362128, 0.75884594,\n",
       "        0.76858947, 0.74626598, 0.75499241, 0.76342746, 0.72735931,\n",
       "        0.75723747, 0.76324252, 0.74739799, 0.75649475, 0.76933358,\n",
       "        0.74834699, 0.76083128, 0.76723562, 0.72428768, 0.76069228,\n",
       "        0.76998643, 0.76144488, 0.76890356, 0.77370158, 0.75008361,\n",
       "        0.75577186, 0.76693711, 0.7280501 , 0.77181088, 0.7716242 ,\n",
       "        0.76551745, 0.78236002, 0.76762806, 0.75892613, 0.76914675,\n",
       "        0.77383883, 0.75708374, 0.77288722, 0.77303658, 0.76252536,\n",
       "        0.77142626, 0.76370984, 0.77164015, 0.77290947, 0.77044221,\n",
       "        0.76394384, 0.77066032, 0.77186018, 0.77276521, 0.77973754,\n",
       "        0.76429347, 0.76560125, 0.7558981 , 0.77019479, 0.77602799,\n",
       "        0.76992555, 0.77941263, 0.76163392, 0.76637781, 0.74396904,\n",
       "        0.77355062, 0.77196512, 0.77250494, 0.7665666 , 0.77058109,\n",
       "        0.77679365, 0.76326113, 0.7696262 , 0.7504773 , 0.77660136,\n",
       "        0.76538425, 0.77101876, 0.77204806, 0.76966142, 0.7806398 ,\n",
       "        0.75923004, 0.78602367, 0.79119985, 0.75764906, 0.77019886,\n",
       "        0.76348375, 0.75421089, 0.77316443, 0.77613356, 0.76455144,\n",
       "        0.78189872, 0.77459049, 0.76385003, 0.7709615 , 0.77295421,\n",
       "        0.76495101, 0.7696003 , 0.77259831, 0.76669053, 0.77350892,\n",
       "        0.77665059, 0.77117267, 0.77352122, 0.76757062, 0.77121074,\n",
       "        0.76891664, 0.77528074, 0.75638619, 0.77494503, 0.77345285,\n",
       "        0.77166475, 0.76930033, 0.76868393, 0.77315646, 0.77476837,\n",
       "        0.77660458, 0.77047577, 0.77385414, 0.7607797 , 0.77837298,\n",
       "        0.77245715, 0.77401915, 0.76973079, 0.77606063, 0.77247627,\n",
       "        0.7519032 , 0.78158156, 0.78273495, 0.75551113, 0.78777951,\n",
       "        0.77517457, 0.75326112, 0.76909193, 0.776919  , 0.75525609,\n",
       "        0.7779637 , 0.77706828, 0.75627994, 0.77708573, 0.77467738,\n",
       "        0.75253295, 0.7706453 , 0.77559896, 0.75141132, 0.77642986,\n",
       "        0.77379151, 0.75685382, 0.77283518, 0.77049401, 0.7581361 ,\n",
       "        0.76752977, 0.7692294 , 0.75367881, 0.77850357, 0.77532952,\n",
       "        0.7587996 , 0.7777757 , 0.76516716, 0.76550597, 0.77446459,\n",
       "        0.77400778, 0.76561178, 0.773974  , 0.76994138, 0.76742296,\n",
       "        0.7755172 , 0.76156594, 0.76058733, 0.77340212, 0.78047675,\n",
       "        0.75893286, 0.7733722 , 0.77306587, 0.75278768, 0.76533947,\n",
       "        0.77889256, 0.74936898, 0.76674821, 0.77095508, 0.74118799,\n",
       "        0.76484167, 0.77431172, 0.74368067, 0.77894319, 0.7802902 ,\n",
       "        0.74332506, 0.76929009, 0.77418634, 0.74226279, 0.77174276,\n",
       "        0.78241588, 0.74635683, 0.77562398, 0.77630548, 0.74595116,\n",
       "        0.77005722, 0.77130991, 0.7497139 , 0.76626526, 0.7783614 ,\n",
       "        0.75150481, 0.78315343, 0.77345664, 0.75127822, 0.77324648,\n",
       "        0.77340605, 0.7449102 , 0.76177478, 0.77463647, 0.75875717,\n",
       "        0.77914782, 0.77565046, 0.75137449, 0.77324648, 0.77385142,\n",
       "        0.70836799, 0.75058329, 0.74451105, 0.71000563, 0.75335443,\n",
       "        0.75253993, 0.69795457, 0.75423656, 0.75936227, 0.70260269,\n",
       "        0.73961544, 0.74575451, 0.69918771, 0.74016254, 0.7514676 ,\n",
       "        0.70884268, 0.73895007, 0.76401312, 0.72564056, 0.74428259,\n",
       "        0.75435549, 0.71476098, 0.74596341, 0.75960343, 0.70999169,\n",
       "        0.74227539, 0.75992161, 0.71847322, 0.76020226, 0.75718014,\n",
       "        0.72199682, 0.74197855, 0.76226983, 0.72035957, 0.74524881,\n",
       "        0.76137792, 0.71981247, 0.7555959 , 0.7601991 , 0.71627102,\n",
       "        0.75428524, 0.76415019, 0.72786671, 0.74767634, 0.76514335]),\n",
       " 'std_test_score': array([0.03603841, 0.03889515, 0.05118018, 0.05079308, 0.04737488,\n",
       "        0.04270123, 0.05184125, 0.05308768, 0.05074934, 0.05335622,\n",
       "        0.05049767, 0.04262064, 0.05588253, 0.05355784, 0.03758682,\n",
       "        0.0529106 , 0.04245371, 0.04355105, 0.04251393, 0.04212135,\n",
       "        0.04124022, 0.051517  , 0.0423233 , 0.04118831, 0.03861494,\n",
       "        0.03891457, 0.03688882, 0.0400447 , 0.03815112, 0.04085513,\n",
       "        0.05407964, 0.04047694, 0.0480125 , 0.0442267 , 0.04431704,\n",
       "        0.03808745, 0.03670267, 0.04222239, 0.03681787, 0.04188036,\n",
       "        0.03965489, 0.03958213, 0.04412225, 0.044157  , 0.04066513,\n",
       "        0.05283885, 0.04801511, 0.05224304, 0.0547872 , 0.04948926,\n",
       "        0.05028811, 0.04346709, 0.0569654 , 0.05615554, 0.04686602,\n",
       "        0.04831998, 0.04677124, 0.04938741, 0.0421951 , 0.04170848,\n",
       "        0.04770955, 0.052908  , 0.04674607, 0.04328596, 0.05072569,\n",
       "        0.04454351, 0.04586779, 0.03930686, 0.03351793, 0.04301996,\n",
       "        0.03946553, 0.04617025, 0.04550883, 0.03072291, 0.03455915,\n",
       "        0.04809943, 0.04552048, 0.03980375, 0.04957263, 0.04819846,\n",
       "        0.03972143, 0.0393947 , 0.04780726, 0.04492008, 0.0463901 ,\n",
       "        0.04656584, 0.04092109, 0.05373125, 0.04705288, 0.0454655 ,\n",
       "        0.05110163, 0.05117014, 0.05348996, 0.05322931, 0.04068323,\n",
       "        0.04459046, 0.05565319, 0.05171949, 0.05575918, 0.03970056,\n",
       "        0.04490673, 0.04971567, 0.04474175, 0.04573973, 0.04121935,\n",
       "        0.04147015, 0.03872762, 0.05094914, 0.04441987, 0.03732773,\n",
       "        0.0478596 , 0.03962056, 0.04177388, 0.03029295, 0.0504339 ,\n",
       "        0.05413799, 0.04611775, 0.0472983 , 0.04500158, 0.05072913,\n",
       "        0.04273051, 0.03636976, 0.04499359, 0.05518764, 0.05222449,\n",
       "        0.03779355, 0.05293462, 0.03666617, 0.0409877 , 0.0405318 ,\n",
       "        0.04063496, 0.04081377, 0.05334428, 0.04448017, 0.04141847,\n",
       "        0.05074608, 0.0463109 , 0.04907735, 0.05099555, 0.05048523,\n",
       "        0.04846242, 0.05214868, 0.04924794, 0.05403858, 0.05530944,\n",
       "        0.04402706, 0.04657744, 0.05154059, 0.04719041, 0.04707483,\n",
       "        0.04209597, 0.04594618, 0.04776709, 0.04530388, 0.04241423,\n",
       "        0.05045449, 0.0549605 , 0.04905706, 0.03954774, 0.0539337 ,\n",
       "        0.04510732, 0.04931096, 0.05040752, 0.05282912, 0.05561226,\n",
       "        0.05586783, 0.05015737, 0.04434168, 0.04956748, 0.04728778,\n",
       "        0.05059951, 0.0503893 , 0.04260455, 0.03081779, 0.05307103,\n",
       "        0.05690506, 0.03871942, 0.05365418, 0.04527183, 0.05036289,\n",
       "        0.0591367 , 0.05124847, 0.0409421 , 0.05960397, 0.0521128 ,\n",
       "        0.04427511, 0.05479651, 0.04927186, 0.0472461 , 0.05255644,\n",
       "        0.04625151, 0.04129577, 0.05200877, 0.05048459, 0.04832176,\n",
       "        0.04847282, 0.04189317, 0.0452153 , 0.03943804, 0.04809579,\n",
       "        0.04832386, 0.04868823, 0.04263853, 0.0523682 , 0.04695823,\n",
       "        0.04189756, 0.04317728, 0.04570451, 0.0421207 , 0.04567981,\n",
       "        0.04946913, 0.04860663, 0.05301927, 0.04159209, 0.04235271,\n",
       "        0.04470495, 0.04269643, 0.04345031, 0.05110791, 0.04766945,\n",
       "        0.0551336 , 0.05763957, 0.04375146, 0.04383065, 0.0459627 ,\n",
       "        0.03603841, 0.03889515, 0.05118018, 0.05079308, 0.04737488,\n",
       "        0.04270123, 0.05184125, 0.05308768, 0.05074934, 0.05335622,\n",
       "        0.05049767, 0.04262064, 0.05588253, 0.05355784, 0.03758682,\n",
       "        0.0529106 , 0.04245371, 0.04355105, 0.04251393, 0.04212135,\n",
       "        0.04124022, 0.051517  , 0.0423233 , 0.04118831, 0.03861494,\n",
       "        0.03891457, 0.03688882, 0.0400447 , 0.03815112, 0.04085513,\n",
       "        0.05407964, 0.04047694, 0.0480125 , 0.0442267 , 0.04431704,\n",
       "        0.03808745, 0.03670267, 0.04222239, 0.03681787, 0.04188036,\n",
       "        0.03965489, 0.03958213, 0.04412225, 0.044157  , 0.04066513,\n",
       "        0.05283885, 0.04801511, 0.05224304, 0.0547872 , 0.04948926,\n",
       "        0.05028811, 0.04346709, 0.0569654 , 0.05615554, 0.04686602,\n",
       "        0.04831998, 0.04677124, 0.04938741, 0.0421951 , 0.04170848,\n",
       "        0.04770955, 0.052908  , 0.04674607, 0.04328596, 0.05072569,\n",
       "        0.04454351, 0.04586779, 0.03930686, 0.03351793, 0.04301996,\n",
       "        0.03946553, 0.04617025, 0.04550883, 0.03072291, 0.03455915,\n",
       "        0.04809943, 0.04552048, 0.03980375, 0.04957263, 0.04819846,\n",
       "        0.03972143, 0.0393947 , 0.04780726, 0.04492008, 0.0463901 ,\n",
       "        0.04656584, 0.04092109, 0.05373125, 0.04705288, 0.0454655 ,\n",
       "        0.05110163, 0.05117014, 0.05348996, 0.05322931, 0.04068323,\n",
       "        0.04459046, 0.05565319, 0.05171949, 0.05575918, 0.03970056,\n",
       "        0.04490673, 0.04971567, 0.04474175, 0.04573973, 0.04121935,\n",
       "        0.04147015, 0.03872762, 0.05094914, 0.04441987, 0.03732773,\n",
       "        0.0478596 , 0.03962056, 0.04177388, 0.03029295, 0.0504339 ,\n",
       "        0.05413799, 0.04611775, 0.0472983 , 0.04500158, 0.05072913,\n",
       "        0.04273051, 0.03636976, 0.04499359, 0.05518764, 0.05222449,\n",
       "        0.03779355, 0.05293462, 0.03666617, 0.0409877 , 0.0405318 ,\n",
       "        0.04063496, 0.04081377, 0.05334428, 0.04448017, 0.04141847,\n",
       "        0.05074608, 0.0463109 , 0.04907735, 0.05099555, 0.05048523,\n",
       "        0.04846242, 0.05214868, 0.04924794, 0.05403858, 0.05530944,\n",
       "        0.04402706, 0.04657744, 0.05154059, 0.04719041, 0.04707483,\n",
       "        0.04209597, 0.04594618, 0.04776709, 0.04530388, 0.04241423,\n",
       "        0.05045449, 0.0549605 , 0.04905706, 0.03954774, 0.0539337 ,\n",
       "        0.04510732, 0.04931096, 0.05040752, 0.05282912, 0.05561226,\n",
       "        0.05586783, 0.05015737, 0.04434168, 0.04956748, 0.04728778,\n",
       "        0.05059951, 0.0503893 , 0.04260455, 0.03081779, 0.05307103,\n",
       "        0.05690506, 0.03871942, 0.05365418, 0.04527183, 0.05036289,\n",
       "        0.0591367 , 0.05124847, 0.0409421 , 0.05960397, 0.0521128 ,\n",
       "        0.04427511, 0.05479651, 0.04927186, 0.0472461 , 0.05255644,\n",
       "        0.04625151, 0.04129577, 0.05200877, 0.05048459, 0.04832176,\n",
       "        0.04847282, 0.04189317, 0.0452153 , 0.03943804, 0.04809579,\n",
       "        0.04832386, 0.04868823, 0.04263853, 0.0523682 , 0.04695823,\n",
       "        0.04189756, 0.04317728, 0.04570451, 0.0421207 , 0.04567981,\n",
       "        0.04946913, 0.04860663, 0.05301927, 0.04159209, 0.04235271,\n",
       "        0.04470495, 0.04269643, 0.04345031, 0.05110791, 0.04766945,\n",
       "        0.0551336 , 0.05763957, 0.04375146, 0.04383065, 0.0459627 ,\n",
       "        0.04435273, 0.03924802, 0.047024  , 0.05322084, 0.0559312 ,\n",
       "        0.04996245, 0.05459616, 0.05637238, 0.05868041, 0.05203002,\n",
       "        0.05027311, 0.04582115, 0.05677495, 0.05201356, 0.04827569,\n",
       "        0.05904516, 0.05661696, 0.0512165 , 0.05239409, 0.04772642,\n",
       "        0.04386804, 0.05400174, 0.05607686, 0.03711502, 0.06212152,\n",
       "        0.05368164, 0.03665607, 0.04333658, 0.05060194, 0.03427513,\n",
       "        0.04765629, 0.04149652, 0.04175097, 0.05152566, 0.04752902,\n",
       "        0.03711082, 0.04877451, 0.03429945, 0.02929777, 0.05374214,\n",
       "        0.04809681, 0.04373216, 0.05156675, 0.05060928, 0.03906581,\n",
       "        0.05567635, 0.04510191, 0.04419554, 0.05421948, 0.04798465,\n",
       "        0.05185959, 0.05863693, 0.05804944, 0.05011897, 0.04717305,\n",
       "        0.04281145, 0.04729184, 0.05271315, 0.06223977, 0.04660135,\n",
       "        0.05475784, 0.0611963 , 0.04980724, 0.0581867 , 0.04143484,\n",
       "        0.04492938, 0.05317114, 0.04150106, 0.04003033, 0.05369223,\n",
       "        0.06015207, 0.04520005, 0.05054098, 0.0384963 , 0.05027464,\n",
       "        0.04862132, 0.04414095, 0.04998294, 0.05515495, 0.05288988,\n",
       "        0.04521396, 0.05122232, 0.03849775, 0.03558477, 0.04719856,\n",
       "        0.04479962, 0.04647342, 0.04873345, 0.04645494, 0.04575006,\n",
       "        0.05620118, 0.04195139, 0.04947251, 0.05618199, 0.04709584,\n",
       "        0.05426715, 0.05578124, 0.05719962, 0.05646619, 0.05153631,\n",
       "        0.04801147, 0.0467476 , 0.05991602, 0.05889179, 0.04153582,\n",
       "        0.04982942, 0.05337261, 0.05355755, 0.05135982, 0.04786871,\n",
       "        0.05319124, 0.04598338, 0.04484709, 0.04571869, 0.04912104,\n",
       "        0.05308337, 0.05303421, 0.05394964, 0.04320333, 0.05263217,\n",
       "        0.05390568, 0.05173857, 0.04920944, 0.05651601, 0.05309706,\n",
       "        0.05389917, 0.06265363, 0.04061477, 0.04157563, 0.05697461,\n",
       "        0.04410184, 0.05081586, 0.05002092, 0.0515172 , 0.04848471,\n",
       "        0.05164955, 0.03917976, 0.03922546, 0.05890778, 0.05037266,\n",
       "        0.05243823, 0.05495926, 0.05000795, 0.05043291, 0.05858016,\n",
       "        0.05604895, 0.05233323, 0.05231974, 0.04926686, 0.05290716,\n",
       "        0.04791703, 0.0539286 , 0.05300877, 0.04383193, 0.0592833 ,\n",
       "        0.04897411, 0.04564332, 0.04846907, 0.05047029, 0.03717271,\n",
       "        0.057688  , 0.05922199, 0.05072906, 0.05392754, 0.04779937,\n",
       "        0.04561485, 0.04675694, 0.04618478, 0.04434691, 0.05236241,\n",
       "        0.05581043, 0.05758782, 0.05379246, 0.04097139, 0.05124544,\n",
       "        0.04168542, 0.04788891, 0.04692485, 0.05236241, 0.04954863,\n",
       "        0.05901273, 0.06228366, 0.05116516, 0.06610298, 0.06139791,\n",
       "        0.04958118, 0.06429917, 0.05505131, 0.05363469, 0.05693881,\n",
       "        0.05823286, 0.05261086, 0.06578952, 0.05799193, 0.0564671 ,\n",
       "        0.06201839, 0.04777669, 0.04500277, 0.05755265, 0.05413284,\n",
       "        0.05401094, 0.055446  , 0.05376053, 0.05176771, 0.0519456 ,\n",
       "        0.04277084, 0.04556489, 0.06055649, 0.05797804, 0.05510914,\n",
       "        0.05513723, 0.05028167, 0.05229744, 0.05318106, 0.04347809,\n",
       "        0.04534032, 0.05769833, 0.05634292, 0.05332577, 0.05137736,\n",
       "        0.06676898, 0.05569348, 0.05210626, 0.04531919, 0.04565825]),\n",
       " 'rank_test_score': array([159,  92, 439, 514, 216, 441, 430, 181,  23, 114, 155, 571, 407,\n",
       "        483, 279, 389, 261,   8,  52, 119, 418,  42, 313,  40, 162, 330,\n",
       "        112, 396, 271, 532,  18, 129, 281,  35, 320,  89, 100, 211, 541,\n",
       "          4, 127, 292, 134, 194, 366, 413, 144,  45, 453, 167, 337, 545,\n",
       "        123, 341, 385,  47, 383, 105, 348, 460, 494, 226,  58, 197, 244,\n",
       "        399, 141, 250, 224, 449, 273, 150, 237, 360, 327, 174, 229,  50,\n",
       "        324, 302, 317,  81, 125, 363,  85, 304,  10, 169,  26,  95, 550,\n",
       "        258,  64, 569, 164, 177, 535, 322, 172, 467,   6, 102, 434,  33,\n",
       "         79, 503, 137, 107, 516,  28, 351, 428,  73, 473, 462, 213, 235,\n",
       "        489, 242,  62, 333, 311,  38, 335, 266, 300, 387, 121,  30, 254,\n",
       "        205,  66, 475, 309, 294, 618, 256, 220, 601, 297, 379, 598, 403,\n",
       "        381, 623, 209,  54, 611, 287, 232, 633, 444, 372, 565, 252,  20,\n",
       "        613, 200,  83, 606, 416,  71, 587, 109, 248, 499, 148, 263, 557,\n",
       "        276, 391, 485, 246, 290, 521,  76,  12, 548, 146, 375, 650, 596,\n",
       "        554, 654, 590, 425, 659, 578, 519, 642, 562, 470, 645, 592, 436,\n",
       "        663, 576, 537, 631, 509, 377, 616, 552, 458, 652, 523, 465, 609,\n",
       "        528, 356, 604, 487, 401, 657, 492, 344, 480, 370, 187, 594, 539,\n",
       "        405, 647, 269, 284, 159,  92, 439, 514, 216, 441, 430, 181,  23,\n",
       "        114, 155, 571, 407, 483, 279, 389, 261,   8,  52, 119, 418,  42,\n",
       "        313,  40, 162, 330, 112, 396, 271, 532,  18, 129, 281,  35, 320,\n",
       "         89, 100, 211, 541,   4, 127, 292, 134, 194, 366, 413, 144,  45,\n",
       "        453, 167, 337, 545, 123, 341, 385,  47, 383, 105, 348, 460, 494,\n",
       "        226,  58, 197, 244, 399, 141, 250, 224, 449, 273, 150, 237, 360,\n",
       "        327, 174, 229,  50, 324, 302, 317,  81, 125, 363,  85, 304,  10,\n",
       "        169,  26,  95, 550, 258,  64, 569, 164, 177, 535, 322, 172, 467,\n",
       "          6, 102, 434,  33,  79, 503, 137, 107, 516,  28, 351, 428,  73,\n",
       "        473, 462, 213, 235, 489, 242,  62, 333, 311,  38, 335, 266, 300,\n",
       "        387, 121,  30, 254, 205,  66, 475, 309, 294, 618, 256, 220, 601,\n",
       "        297, 379, 598, 403, 381, 623, 209,  54, 611, 287, 232, 633, 444,\n",
       "        372, 565, 252,  20, 613, 200,  83, 606, 416,  71, 587, 109, 248,\n",
       "        499, 148, 263, 557, 276, 391, 485, 246, 290, 521,  76,  12, 548,\n",
       "        146, 375, 650, 596, 554, 654, 590, 425, 659, 578, 519, 642, 562,\n",
       "        470, 645, 592, 436, 663, 576, 537, 631, 509, 377, 616, 552, 458,\n",
       "        652, 523, 465, 609, 528, 356, 604, 487, 401, 657, 492, 344, 480,\n",
       "        370, 187, 594, 539, 405, 647, 269, 284, 422,  17, 393, 508, 365,\n",
       "        185, 526, 223, 218, 469, 286, 456, 283, 222, 332, 452, 315, 268,\n",
       "        231,  49, 447, 421, 534, 340, 118, 347,  56, 478, 412, 629, 189,\n",
       "        265, 239, 411, 319,  94, 464, 354, 589,  99, 424, 306, 260, 353,\n",
       "         32, 506,   3,   1, 518, 339, 457, 561, 207, 116, 446,  22, 158,\n",
       "        455, 307, 219, 438, 355, 234, 410, 191,  97, 299, 190, 394, 296,\n",
       "        369, 140, 530, 152, 193, 278, 358, 374, 208, 153,  98, 329, 183,\n",
       "        491,  69, 241, 176, 350, 117, 240, 580,  25,  15, 544,   2, 143,\n",
       "        568, 368,  91, 547,  75,  88, 531,  87, 154, 575, 316, 133, 583,\n",
       "        104, 186, 527, 228, 326, 513, 395, 362, 564,  68, 139, 511,  78,\n",
       "        432, 423, 161, 179, 420, 180, 346, 398, 136, 479, 496, 199,  37,\n",
       "        507, 202, 215, 573, 427,  61, 603, 409, 308, 639, 443, 166, 630,\n",
       "         60,  44, 635, 359, 171, 637, 275,  16, 615, 132, 111, 621, 343,\n",
       "        289, 600, 415,  70, 581,  14, 192, 585, 203, 196, 626, 477, 157,\n",
       "        512,  57, 131, 584, 203, 184, 672, 586, 627, 669, 567, 574, 675,\n",
       "        560, 505, 673, 641, 622, 674, 640, 582, 671, 644, 451, 656, 628,\n",
       "        556, 668, 620, 502, 670, 636, 501, 666, 497, 525, 661, 638, 472,\n",
       "        662, 625, 482, 665, 543, 498, 667, 559, 448, 649, 608, 433])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.4,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 1,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911998502810007"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789238\n",
      "F1: 0.675862\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a267f25f8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
