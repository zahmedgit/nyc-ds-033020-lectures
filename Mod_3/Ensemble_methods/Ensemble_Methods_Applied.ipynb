{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "How does a random forest algorithm try to correct for the short comings of a decision tree?\n",
    "\n",
    "Your answer should specifically mention the shortcomings of a decision tree and be explicit in how it overcomes that shortcoming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods Applied\n",
    "\n",
    "Agenda:\n",
    "- Review code for Voting Classifier, Bagging Classifier, and Random Forest\n",
    "- Practice finding optimal hyperparameter for  Random Forest with gridsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Prep Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestRegressor, ExtraTreesRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/nyc-ds-033020-lectures/master/Mod_3/decision_trees/cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use x and y variables to split the training data into train and test set then scale that data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7770700636942675\n"
     ]
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_preds = knn.predict(X_test)\n",
    "\n",
    "knn_f1 = metrics.f1_score(y_test, knn_preds)\n",
    "\n",
    "\n",
    "print(knn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8066298342541436\n"
     ]
    }
   ],
   "source": [
    "lr_preds = lr.predict(X_test)\n",
    "\n",
    "lr_f1 = metrics.f1_score(y_test, lr_preds)\n",
    "\n",
    "print(lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8047337278106509\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=5, class_weight='balanced')\n",
    "\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "dtc_preds  = dtc.predict(X_test)\n",
    "\n",
    "dtc_f1 = metrics.f1_score(y_test, dtc_preds)\n",
    "\n",
    "print(dtc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine three models using Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the estimators, we must provide a list of tuples. The first value in the tuple is is the name given to the model/estimator in the second value. SKlearn requires this because there is additional functionality where you can access information about the specific models, so you need to name the models to access them later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8255813953488372\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "                estimators=[('logreg', lr), ('knneighbors', knn), ('decisiontree', dtc)], \n",
    "                voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "vc_preds = voting_clf.predict(X_test)\n",
    "\n",
    "vc_f1 = metrics.f1_score(y_test, vc_preds)\n",
    "\n",
    "print(vc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a voting classifier with multiple Logistic regression models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10]\n",
    "titles = ['lr_0_001', 'lr_0_01', 'lr_0_1', 'lr_1', 'lr_10']\n",
    "\n",
    "params = dict(zip(titles, C_param_range)) \n",
    "models = {}\n",
    "\n",
    "table = pd.DataFrame(columns = ['C_parameter','F1'])\n",
    "table['C_parameter'] = C_param_range\n",
    "j = 0\n",
    "\n",
    "for k , v  in params.items():\n",
    "    \n",
    "    # Create model using different value for c  \n",
    "    lr = LogisticRegression(penalty = 'l2', C = v, random_state = 1, class_weight='balanced')\n",
    "    \n",
    "    #save the model to a dictionary to use later in our voting classifiers\n",
    "    models[k]= lr\n",
    "    \n",
    "    #the steps below this point are unnecessary in order to create a voting classifier, \n",
    "    #but it is easy to fit the model and see how performance changes for different levels of regularization\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict using model\n",
    "    y_preds = lr.predict(X_test)\n",
    "\n",
    "    # Saving accuracy score in table\n",
    "    table.iloc[j,1] = metrics.f1_score(y_test, y_preds)\n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_parameter</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.735135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.751381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.804469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.80663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000</td>\n",
       "      <td>0.80663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   C_parameter        F1\n",
       "0        0.001  0.735135\n",
       "1        0.010  0.751381\n",
       "2        0.100  0.804469\n",
       "3        1.000   0.80663\n",
       "4       10.000   0.80663"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review performance for different levels of C\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lr_0_001',\n",
       "  LogisticRegression(C=0.001, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_0_01',\n",
       "  LogisticRegression(C=0.01, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_0_1',\n",
       "  LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_1',\n",
       "  LogisticRegression(C=1, class_weight='balanced', dual=False, fit_intercept=True,\n",
       "                     intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                     multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False)),\n",
       " ('lr_10',\n",
       "  LogisticRegression(C=10, class_weight='balanced', dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                     max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                     random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                     warm_start=False))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invesitgate the models D=dictionary\n",
    "list(models.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have programmatically created multiple logistic regression models, let's use them in an ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8255813953488372\n"
     ]
    }
   ],
   "source": [
    "lr_voting = VotingClassifier(estimators=list(models.items()), \n",
    "                              voting='hard')\n",
    "\n",
    "lr_voting.fit(X_train, y_train)\n",
    "\n",
    "lrv_preds = voting_clf.predict(X_test)\n",
    "\n",
    "lrv_f1 = metrics.f1_score(y_test, lrv_preds)\n",
    "\n",
    "print(vc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Bagging Classifier for a Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_lr = BaggingClassifier(\n",
    "            base_estimator=LogisticRegression(random_state = 1, class_weight='balanced'), \n",
    "            n_estimators= 100,\n",
    "            max_samples= .7,\n",
    "            max_features= 6,\n",
    "            oob_score= True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1.0,\n",
       "                                                    class_weight='balanced',\n",
       "                                                    dual=False,\n",
       "                                                    fit_intercept=True,\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None, max_iter=100,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l2',\n",
       "                                                    random_state=1,\n",
       "                                                    solver='lbfgs', tol=0.0001,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=6,\n",
       "                  max_samples=0.7, n_estimators=100, n_jobs=None,\n",
       "                  oob_score=True, random_state=None, verbose=0,\n",
       "                  warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_lr.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7762762762762763"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the oob_score to get some idea of how the model performs on a validation set\n",
    "\n",
    "bc_lr.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7826086956521741\n"
     ]
    }
   ],
   "source": [
    "# See how the model performs on the test set\n",
    "\n",
    "bc_lr_preds = bc_lr.predict(X_test)\n",
    "\n",
    "bc_lr_f1 = metrics.f1_score(y_test, bc_lr_preds)\n",
    "\n",
    "print(bc_lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the difference in the `VotingClassifier` algorithm and the `BaggingClassifier` algorithm?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(random_state = 1, n_estimators=100, max_depth=1, max_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=1, max_features=4,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at all the different default features\n",
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=1, max_features=4,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model to the training data\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:  0.7074829931972789\n"
     ]
    }
   ],
   "source": [
    "#use the fitted model to predict on the test data\n",
    "rfc_preds = rfc.predict(X_test)\n",
    "\n",
    "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', rfc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Increase the number of trees and see how the model performs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridsearchCV with Random Forest\n",
    "\n",
    "Let's use grid search to identify the best tuning parameters to use for a random forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of all the parameters you want to tune\n",
    "param_grid = { \n",
    "    param1: list_of_options,\n",
    "    param2: list_of_options,\n",
    "    param3: list_of_options,\n",
    "    param4: list_of_options,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a grid search object and fit it to the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Identify the best params \n",
    "\n",
    "\n",
    "\n",
    "#Identify the best score during fitting with cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test set\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "\n",
    "\n",
    "# checking accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
