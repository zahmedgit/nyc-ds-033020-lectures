{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features for use in model construction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Top reasons to use feature selection are:\n",
    "* It enables the machine learning algorithm to train faster.\n",
    "* It reduces the complexity of a model and makes it easier to interpret.\n",
    "* It improves the accuracy of a model if the right subset is chosen.\n",
    "* It reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import the packages we will use for this notebook***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 300)\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set up or initial dataframe by removing variable and transforming others.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/cleaned_movie_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['color', 'director_name','actor_2_name','language', 'country',\n",
    "                 'movie_imdb_link', 'movie_title', 'num_voted_users', \n",
    "                 'movie_facebook_likes','actor_1_name', 'actor_3_name',\n",
    "                 'content_rating', 'rating', 'genres'], \n",
    "                 axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yr_old'] =  df['title_year'].map(lambda x: 2016-x )\n",
    "\n",
    "df = df[df['yr_old']<20]\n",
    "\n",
    "df = df[df['budget']<300000000]\n",
    "\n",
    "df = df[(df['gross']>1000000) & (df['gross']<500000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['gross_log'] = np.log(df['gross'])\n",
    "df['gross_sqrt'] = np.sqrt(df['gross'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols  = ['duration','director_facebook_likes', 'actor_3_facebook_likes', \n",
    "       'actor_1_facebook_likes', 'gross', 'cast_total_facebook_likes',\n",
    "       'facenumber_in_poster', 'budget'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break the colums in to groups to plot 4 on a row at a time\n",
    "n = 4\n",
    "row_groups= [cols[i:i+n] for i in range(0, len(cols), n) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in row_groups:\n",
    "    pp = sns.pairplot(data=df, y_vars=['gross'],x_vars=i, kind=\"reg\", height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['num_critic_for_reviews', 'duration', 'director_facebook_likes',\n",
    "       'actor_3_facebook_likes', 'actor_1_facebook_likes',\n",
    "       'cast_total_facebook_likes', 'facenumber_in_poster',\n",
    "       'num_user_for_reviews', 'budget', 'title_year',\n",
    "       'actor_2_facebook_likes', 'imdb_score', 'G', 'PG', 'PG-13',\n",
    "       'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['gross_sqrt']\n",
    "features = df[feature_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.fillna(features.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=23,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train =pd.DataFrame(data=scaler.transform(X_train), columns=feature_columns)\n",
    "X_test =pd.DataFrame(data=scaler.transform(X_test), columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a linear regression object\n",
    "lm = LinearRegression()\n",
    "\n",
    "#fit the linear regression to the data\n",
    "lm = lm.fit(X_train, y_train)\n",
    "\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "\n",
    "print('Training Root Mean Squared Error:' , train_rmse)\n",
    "\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('Testing Root Mean Squared Error:' , test_rmse)\n",
    "\n",
    "\n",
    "print('Training: ', int(train_rmse), \"vs. Testing: \", int(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "y_train_pred = lm.predict(X_train)\n",
    "y_test_pred = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot the residuals after fitting a linear model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.residplot(y_train_pred, y_train, lowess=True, color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on this residual plot how might we want to transform our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Types of Feature Selection\n",
    "\n",
    "* Filter Methods\n",
    "* Wrapper Methods\n",
    "* Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Filter Methods\n",
    "Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./resources/Filter_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Examples:** \n",
    "* F-Test\n",
    "* Chi squared test \n",
    "* Information gain \n",
    "* Correlation coefficient scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./resources/FS1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Variables based on correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = X_train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "X_train.drop(columns=to_drop, inplace=True)\n",
    "X_test.drop(columns=to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.corrwith(y_train).abs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflation Factor (VIF) \n",
    "\n",
    "VIFis a measure of colinearity among predictor variables within a multiple regression. The variance inflation factor for the estimated regression coefficient $b_j$ — denoted $VIF_j$ —is just the factor by which the variance of $b_j$ is \"inflated\" by the existence of correlation among the predictor variables in the model.In particular, the variance inflation factor for the jth predictor is:\n",
    "\n",
    "$$VIF_j=\\frac{1}{1-R_{j}^{2}}$$\n",
    "\n",
    "\n",
    "where $R^2_j$  is the $R^2$-value obtained by regressing the jth predictor on the remaining predictors. \n",
    "\n",
    "\n",
    "Inspect the factors for each predictor variable, if the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable.\n",
    "\n",
    "https://online.stat.psu.edu/stat462/node/180/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif[\"features\"] = X_train.columns\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  F Test\n",
    "\n",
    "F Test is a statistical test used to compare between models and check if the difference is significant between the model.\n",
    "\n",
    "F-Test does a hypothesis testing model X and Y where X is a model created by just a constant and Y is the model created by a constant and a feature.\n",
    "\n",
    "The least square errors in both the models are compared and checks if the difference in errors between model X and Y are significant or introduced by chance.\n",
    "\n",
    "F-Test is useful in feature selection as we get to know the significance of each feature in improving the model.\n",
    "\n",
    "Scikit learn provides the Selecting K best features using F-Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I am using the F-test to select the 10 top varaibles for this model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression,mutual_info_regression\n",
    "\n",
    "selector = SelectKBest(f_regression, k=10)\n",
    "\n",
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "removed_columns = X_train.columns[~selector.get_support()]\n",
    "# X_train = X_train[selected_columns]\n",
    "# X_test = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(removed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a linear regression object\n",
    "lm_kbest = LinearRegression()\n",
    "\n",
    "#fit the linear regression to the data\n",
    "lm_kbest = lm_kbest.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "trainK_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "\n",
    "print('Training Root Mean Squared Error:' , trainK_rmse)\n",
    "\n",
    "y_pred = lm_kbest.predict(X_test[selected_columns])\n",
    "\n",
    "testK_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('Testing Root Mean Squared Error:' , testK_rmse)\n",
    "\n",
    "\n",
    "print('Original: ', int(test_rmse), \"vs. KBest: \", int(testK_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrapper Methods\n",
    "\n",
    "Wrapper methods consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy.\n",
    "\n",
    "The search process may be methodical such as a best-first search, it may stochastic such as a random hill-climbing algorithm, or it may use heuristics, like forward and backward passes to add and remove features.\n",
    "\n",
    "Wrapper Methods promises you a best set of features with a extensive greedy search.\n",
    "\n",
    "But the main drawbacks of wrapper methods is the sheer amount of models that needs to be trained. It is computationally very expensive and is infeasible with large number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./resources/Wrapper_1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "An example if a wrapper method is the recursive feature elimination algorithm.\n",
    "\n",
    "As the name suggests, this method eliminates worst performing features on a particular model one after the other until the best subset of features are known.\n",
    "\n",
    "\n",
    "Recursive elimination eliminates the least explaining features one after the other.\n",
    "For data with n features,\n",
    "\n",
    "- On first round ‘n-1’ models are created with combination of all features except one. The least performing feature is removed\n",
    "\n",
    "- On second round ‘n-2’ models are created by removing another feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./resources/rfe_graph.png' width=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create recursive feature eliminator that scores features by mean squared errors\n",
    "selector = RFECV(estimator=ols, step=1, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit recursive feature eliminator \n",
    "selector.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_train.columns[selector.support_]\n",
    "removed_columns = X_train.columns[~selector.support_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(removed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(selected_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a linear regression object\n",
    "lm_rfe = LinearRegression()\n",
    "\n",
    "#fit the linear regression to the data\n",
    "lm_rfe = lm_rfe.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "trainRFE_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "\n",
    "print('Training Root Mean Squared Error:' , trainRFE_rmse)\n",
    "\n",
    "y_pred = lm_rfe.predict(X_test[selected_columns])\n",
    "\n",
    "testRFE_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('Testing Root Mean Squared Error:' , testRFE_rmse)\n",
    "\n",
    "\n",
    "print('Original: ', int(test_rmse), \"vs. KBest: \", int(testK_rmse), \"vs. RFE: \", int(testRFE_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embedded Methods\n",
    "\n",
    "Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.\n",
    "\n",
    "Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "Examples of regularization algorithms are the LASSO, Elastic Net and Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./resources/Embedded_1.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now we will create polynomial and interaction terms***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now we will scale those terms***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot to compare the size of all of our coefficients***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(data=lm.coef_ ).T\n",
    "coef.columns = feature_columns\n",
    "\n",
    "model_coef = coef.T.sort_values(by=0).T\n",
    "model_coef.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01, normalize=False)\n",
    "\n",
    "lasso.fit(X_train,y_train)\n",
    "\n",
    "y_train_pred = lasso.predict(X_train)\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "train_rmse = metrics.mean_absolute_error(y_train, y_train_pred)\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Training Error: '+ str(train_rmse) )\n",
    "print('Testing Error: '+ str(test_rmse) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef01 = pd.DataFrame(data=lasso.coef_).T\n",
    "lasso_coef01.columns = X_train.columns\n",
    "lasso_coef01 = lasso_coef01.T.sort_values(by=0).T\n",
    "lasso_coef01.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef01.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=10, normalize=False)\n",
    "\n",
    "lasso.fit(X_train,y_train)\n",
    "\n",
    "y_train_pred = lasso.predict(X_train)\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "train_rmse = metrics.mean_absolute_error(y_train, y_train_pred)\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Training Error: '+ str(train_rmse) )\n",
    "print('Testing Error: '+ str(test_rmse) )\n",
    "\n",
    "lasso_coef01 = pd.DataFrame(data=lasso.coef_).T\n",
    "lasso_coef01.columns = X_train.columns\n",
    "lasso_coef01 = lasso_coef01.T.sort_values(by=0).T\n",
    "lasso_coef01.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef01.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
