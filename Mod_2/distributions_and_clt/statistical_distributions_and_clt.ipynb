{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Statistical Distributions and the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fundamental distinction among kinds of distributions is the distinction between discrete and continuous distributions. A discrete distribution (or variable) takes on countable values, like integers, while a continuous distribution takes on a continuum of values, like real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Mass Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\bf{probability\\ mass\\ function\\ (pmf)}$ for a random variable gives, at any value $k$, the probability that the random variable takes the value $k$. Suppose, for example, that I have a jar full of lottery balls containing:\n",
    "- 60 \"1\"s,\n",
    "- 30 \"2\"s, and\n",
    "- 10 \"3\"s.\n",
    "\n",
    "We might then represent this function pictorially as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot here!\n",
    "\n",
    "x = range(1, 4)\n",
    "lotto_dict = {1: 0.6, 2: 0.3, 3: 0.1}\n",
    "y = [lotto_dict[num] for num in x]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(x, y, 'bo', ms=8, label='lotto pmf')\n",
    "ax.vlines(x, 0, y, 'r', lw=5)\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several functions that commonly appear; the shapes of their graphs should become familiar to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial\n",
    "\n",
    "The binomial distribution applies when I have a process that has only two outcomes. Suppose some baseball team has a 70% chance of winning any game that it plays. The binomial distribution can tell me what the probability is that the team win exactly $k$ out of $n$ games ($k < n$).\n",
    "\n",
    "$\\Large f(x) = {n \\choose k}p^k(1 - p)^{n - k}$\n",
    "\n",
    "Note: ${n\\choose k} = \\frac{n!}{k!(n - k)!}$, the number of ways of choosing $k$ objects from a total of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "p = 0.95\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "x = np.arange(stats.binom.ppf(0.01, n, p),\n",
    "              stats.binom.ppf(0.99, n, p))\n",
    "\n",
    "ax.plot(x, stats.binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\n",
    "ax.vlines(x, 0, stats.binom.pmf(x, n, p), 'r', linewidth=5,\n",
    "          label='pmf')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fair coin 5 times. Exactly 3 heads.\n",
    "\n",
    "stats.binom.pmf(3, 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct calculation using the Binomial Theorem:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose I flip a fair coin 300 times.\n",
    "# What are the chances that I get exactly 162 heads?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the standard deviation of a binomial distribution of 10 repeated trials,\n",
    "# where each trial has a probability of success of 45%?\n",
    "\n",
    "stats.binom.std(10, 0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a binomial variable representing 500 repeated trials,\n",
    "# where each has a probability of success of 90%,\n",
    "# what interval will contain 50% of my distribution?\n",
    "\n",
    "stats.binom.interval(0.5, 500, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .rvs() to generate random numbers drawn from the distribution\n",
    "\n",
    "stats.binom.rvs(n=2, p=0.5, size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric\n",
    "\n",
    "The geometric distribution is for discrete distributions what the exponential distribution is for continuous ones (see below).\n",
    "\n",
    "Suppose I'm thinking about flipping a fair coin and wondering about how long I'll need to wait before it lands \"heads\". There's a 50-50 chance that I'll get \"heads\" on the first flip. But there is only one chance in four that the first \"heads\" would come on the second flip, and only one chance in eight that it would come on the third. Etc.\n",
    "\n",
    "$\\Large f(x) = (1 - p)^{k - 1}p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "x = np.arange(stats.geom.ppf(0.01, p),\n",
    "              stats.geom.ppf(0.99, p))\n",
    "y = stats.geom.pmf(x, p)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "ax.plot(x, y, 'bo', ms=8, label='geom pmf')\n",
    "ax.vlines(x, 0, y, 'r', linewidth = 5)\n",
    "ax.legend(loc='best', frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the mean of the distribution above? Why?\n",
    "\n",
    "stats.geom(p=0.5, loc=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can confirm this by taking samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for _ in range(1000):\n",
    "    samples.append(np.mean(stats.geom(p=0.5, loc=0).rvs(size=1000)))\n",
    "\n",
    "np.mean(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the median of the distribution above? Why?\n",
    "\n",
    "stats.geom(p=0.5, loc=0).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Binomial\n",
    "\n",
    "The negative binomial distribution describes the probability of getting the $r^{th}$ success (of a binary process) on the $n^{th}$ trial.\n",
    "\n",
    "Its pdf looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "p = 1 / 6\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "prob = stats.nbinom(n, p)\n",
    "x = np.arange(stats.nbinom.ppf(0.01, n, p),\n",
    "              stats.nbinom.ppf(0.99, n, p))\n",
    "ax.plot(x, stats.nbinom.pmf(x, n, p), 'bo', ms=8, label='nbinom pmf')\n",
    "ax.vlines(x, 0, prob.pmf(x), colors='k', linestyles='-', lw=1)\n",
    "ax.legend(loc='best', frameon=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, what are the chances that it takes me exactly twenty rolls of a fair die before finally rolling two 6's?\n",
    "\n",
    "The equation is given by the following:\n",
    "\n",
    "$\\large P_{negbinom}(n, r, p) = $$\\large(n - 1)\\choose\\large(r - 1)$$\\large p^r(1 - p)^{n - r}$.\n",
    "\n",
    "Let's explain this:\n",
    "\n",
    "If the $r^{th}$ success is going to come on the $n^{th}$ trial, then how many successes should I have on the first $n - 1$ trials?\n",
    "\n",
    "How many failures will I have in the first $n - 1$ trials?\n",
    "\n",
    "So, to answer our example question, we have:\n",
    "\n",
    "$\\large P_{negbinom}(20, 2, 1/6) = $$\\large19\\choose\\large 1$$\\large (1/6)^2(5/6)^{18}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate this here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our math against scipy's!\n",
    "\n",
    "# We'll use `stats.nbinom()`, specifying:\n",
    "    # p, the probability of success on a single trial;\n",
    "    # n, the number of successes; and, inside the .pmf() method, we'll specify:\n",
    "    \n",
    "    # k, the number of failures\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that the geometric distribution is a special case of the negative binomial distribution.)\n",
    "\n",
    "There is only a 2% chance that this will happen!\n",
    "\n",
    "The mean of this distribution is $\\large\\mu = \\frac{r}{p}$ (does this make sense?), <br/>\n",
    "\n",
    "and the std of this distribution is $\\large\\sigma = \\frac{\\sqrt{r(1 - p)}}{p}$\n",
    "\n",
    "### Your turn!\n",
    "\n",
    "Suppose I roll a 100-sided die. What are the chances that I roll my 3rd '98' on the 27th roll of the die?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center, Spread, Shape\n",
    "\n",
    "Of _any_ distribution it is natural to ask about its center, its spread, and its shape. The 'center' refers loosely to the middle-values of a distribution, and is measured more precisely by notions like the mean, the median, and the mode.\n",
    "\n",
    "For a discrete distribution:\n",
    "\n",
    "mean = $\\Large\\mu = \\frac{\\Sigma^n_{i = 1}x_i}{n}$\n",
    "\n",
    "For a continuous distribution:\n",
    "\n",
    "mean = $\\Large\\mu = \\int_Xxp(x)dx$, <br/> where $p(x)$ is the probability density function associated with the distribution $X$ (see below).\n",
    "\n",
    "**Question: How does this formula for continuous-distribution means relate to the formula for discrete-distribution means?**\n",
    "\n",
    "**Question: How would we make sense of the notions of median and mode for continuous random variables?**\n",
    "\n",
    "The 'spread' refers loosely to how far away the more extreme values are from the center, and is measured more precisely by the notion of the standard deviation, which is effectively a measure of the *average distance away from the mean*.\n",
    "\n",
    "For a discrete distribution:\n",
    "\n",
    "std = $\\Large\\sigma = \\sqrt{\\frac{\\Sigma^n_{i = 1}(x_i - \\mu)^2}{n}}$\n",
    "\n",
    "For a continuous distribution:\n",
    "\n",
    "std = $\\Large\\sigma = \\sqrt{\\int_X(x - \\mu)^2p(x)dx}$, <br/> where again $p(x)$ is the probability density function of X.\n",
    "\n",
    "The 'shape' refers loosely to how the probability of obtaining certain values changes as a function of where we are in the distribution. Are middle or extreme values more probable? Is the distribution symmetric about the middle or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Function and Cumulative Distribution Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\bf{probability\\ density\\ function\\ (pdf)}$ plays the same role for continuous distributions that the pmf plays for discrete distributions.\n",
    "\n",
    "Since a continuous random variable can take any of an _infinite_ number of values, it doesn't make sense to speak of the probability that the variable take any particular value. Think of throwing a dart at a dartboard. The chance that the dart hit any particular point on the board is _zero_. But we can still make sense of the idea of the probability of the variable taking a value _in a particular range_, and this is why we use the word 'density' for continuous distributions.\n",
    "\n",
    "The $\\bf{cumulative\\ distribution\\ function\\ (cdf)}$ gives, at any value $x$, the probability that a continuous variable take a value that is _less than or equal to $x$_.\n",
    "\n",
    "The cdf will therefore be, for any distribution, a monotonically increasing (or, strictly, nondecreasing) function. That is, $cdf(x_2) \\geq cdf(x_1)$ if $x_2 \\geq x_1$.\n",
    "\n",
    "[Here's](http://www.mas.ncl.ac.uk/~nmf16/teaching/mar1002/lect07.pdf) a helpful document on continuous random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform\n",
    "\n",
    "The uniform distribution applies when all possible values of the variable are _equally probable_. If I'm running a random number generator to select a number between 0 and 1, then the generator won't be any good unless all such numbers are equally likely to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the random module's RNG:\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How could we use a histogram to test whether this RNG is working?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large f(x) = \\frac{1}{b - a}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(stats.uniform.ppf(0),\n",
    "               stats.uniform.ppf(1), 10)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, stats.uniform.pdf(x),\n",
    "         'bo', ms=8, label = 'uniform pmf')\n",
    "ax.vlines(x, 0, stats.uniform.pdf(x), 'r', lw=5, label='pmf')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a derivation of the variance for a uniform distribution:\n",
    "\n",
    "We have, in the general case, that:\n",
    "\n",
    "$\\sigma^2 = \\int(x - \\mu)^2\\rho(x)dx$\n",
    "\n",
    "In our case $\\mu = \\frac{a+b}{2}$ and $\\rho(x) = \\frac{1}{b-a}$\n",
    "\n",
    "So we have:\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{b-a}\\int\\left(x - \\frac{a+b}{2}\\right)^2dx$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{b-a}\\int\\left(x^2 - (a+b)x + \\frac{(a+b)^2}{4}\\right)dx$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{b-a}\\left[\\frac{x^3}{3} - \\frac{(a+b)x^2}{2} + \\frac{(a+b)^2x}{4}\\right]\\bigg|^b_a$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{b-a}\\left[\\frac{b^3}{3} - \\frac{ab^2}{2} - \\frac{b^3}{2} + \\frac{a^2b}{4} + \\frac{2ab^2}{4} + \\frac{b^3}{4} - \\frac{a^3}{3} + \\frac{a^3}{2} + \\frac{a^2b}{2} - \\frac{a^3}{4} - \\frac{2a^2b}{4} - \\frac{ab^2}{4}\\right]$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{b-a}\\left[\\frac{b^3}{12} -\\frac{ab^2}{4} + \\frac{a^2b}{4} - \\frac{a^3}{12}\\right]$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{b-a}\\frac{(b-a)^3}{12}$\n",
    "\n",
    "$\\sigma^2 = \\frac{(b-a)^2}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal\n",
    "\n",
    "The normal distribution describes many phenomena. Think of anything that has a typical range:\n",
    "- human body temperatures\n",
    "- sizes of elephants\n",
    "- sizes of stars\n",
    "- populations of cities\n",
    "\n",
    "Among human beings, 98.6 degrees Fahrenheit is an _average_ body temperature. Many folks' temperatures won't measure _exactly_ 98.6 degrees, but most measurements will be _close_. It is much more common to have a body temperature close to 98.6 (whether slightly more or slightly less) than it is to have a body temperature far from 98.6 (whether significantly more or significantly less). This is a hallmark of a normally distributed variable.\n",
    "\n",
    "Similarly, there are large elephants and there are small elephants, but most elephants are near the average size.\n",
    "\n",
    "The normal distribution is _very_ common in nature (**Why?**) and will arise often in your work. Get to know it well!\n",
    "\n",
    "$\\Large f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp\\left[\\frac{-(x - \\mu)^2}{2\\sigma^2}\\right]$\n",
    "\n",
    "You can get Euler's number $e$ from `numpy.e` or (the method) `numpy.exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 5000)\n",
    "mu = 0\n",
    "sigma = 1\n",
    "\n",
    "y_pdf = stats.norm.pdf(x, mu, sigma) # the normal pdf\n",
    "y_cdf = stats.norm.cdf(x, mu, sigma) # the normal cdf\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(x, y_pdf, 'r', label='pdf', linewidth=5)\n",
    "ax.plot(x, y_cdf, 'k', label='cdf', linewidth=5)\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binomial distribution tends to the normal distribution in the limit as $n\\rightarrow \\infty$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "p = 0.5\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "x = np.arange(stats.binom.ppf(0.01, n, p),\n",
    "              stats.binom.ppf(0.99, n, p))\n",
    "\n",
    "ax.plot(x, stats.binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\n",
    "ax.vlines(x, 0, stats.binom.pmf(x, n, p), 'r', linewidth=5,\n",
    "          label='pmf')\n",
    "ax.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential\n",
    "\n",
    "The exponential distribution is like the continuous analog of the geometric distribution. It often answers the question: \"How long will I have to wait before ... ?\"\n",
    "\n",
    "Think, for example, of the rate at which you receive spam emails. There is no regular pattern to the incoming of spam emails, but we can think about how what the probability is that you will have to wait a certain amount of time for the next to come in. And, in particular, the _longer_ you wait, the higher the probability will be that the next spam email will come soon.\n",
    "\n",
    "Exponential distributions also describe the decay of radioactive materials. There is a high probability that you'll have to wait a very short time before a chunk of radioactive material decays, but there is a very small probability that you'll have to wait a long time.\n",
    "\n",
    "$\\Large f(x) = \\lambda e^{-\\lambda x}$, for $x \\geq 0$.\n",
    "Otherwise $\\large f(x) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 5000)\n",
    "mu = 0\n",
    "sigma = 1\n",
    "\n",
    "y_pdf = stats.expon.pdf(x, mu, sigma) # the exponential pdf\n",
    "y_cdf = stats.expon.cdf(x, mu, sigma) # the exponential cdf\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax.plot(x, y_pdf, 'r', label='pdf', linewidth=5)\n",
    "ax.plot(x, y_cdf, 'k', label='cdf', linewidth=5)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and the Central Limit Theorem\n",
    "\n",
    "Recall the difference between sample and population. Statisticians (and data scientists!) are often in the position of making predictions about some population based on a sample drawn from that population:\n",
    "\n",
    "- I interview 100 Washingtonians and take their responses to be representative of state opinions generally.\n",
    "\n",
    "- I measure the lengths of 200 fish in Puget Sound and speculate about the lengths of _all_ the fish in the Sound.\n",
    "\n",
    "Question: Are the _statistics_ I calculate on my sample representative of the statistics of the population?\n",
    "\n",
    "Note: The difference between a sample statistic and the population statistic is called the **sampling error**.\n",
    "\n",
    "Reflection: What factors might be responsible for a large (or small) sampling error?\n",
    "\n",
    "Suppose I take many samples and calculate statistics on each. Would _those_ illuminate the population statistics?\n",
    "\n",
    "The Central Limit Theorem (CLT) says that, under suitable conditions, the sum of independent random variables will converge to a normal distribution in the limit as the number of variables increases infinitely. The CLT will often hold _even when_ the underlying random variables are _not_ themselves normally distributed!\n",
    "\n",
    "Let's look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 1000 draws from an exponential distribution.\n",
    "\n",
    "test = stats.expon.rvs(size=1000, random_state=42)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(test, bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the means of samples from this exponential distribution.\n",
    "# Let's also experiment with the number of samples we draw.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a histogram. Suppose we choose about 50 bins.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try the same thing for binomial random variates.\n",
    "# We'll use 100 single trials. We can tinker with p.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it for standard deviations of a geometric distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So why do we, as data scientists, care about the Central Limit Theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
