{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting off\n",
    "\n",
    "Define write down a definition for the following two terms:\n",
    "\n",
    "Bias - \n",
    "\n",
    "Variance - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Agenda:\n",
    "- R^2 \n",
    "- Bias versus Variance\n",
    "- Train Test Split\n",
    "- Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Coefficient of Determination ($R^2$)\n",
    "\n",
    "The _coefficient of determination_, is a measure of how well the model fits the data.\n",
    "\n",
    "It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model\n",
    "\n",
    "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n",
    "\n",
    "\n",
    "$$\\text{TSS }= \\text{ESS} + \\text{RSS }$$\n",
    "\n",
    "- TSS or SST = Total Sum of Squares \n",
    "- ESS or SSE = Explained Sum of Squares\n",
    "- RSS or SSR = Residual Sum of Squares\n",
    "\n",
    "The actual calculation of $R^2$ is: <br/> $$\\Large R^2= \\frac{\\Sigma_i(\\bar{y} - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}=1- \\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$$.\n",
    "\n",
    "$R^2$ takes values between 0 and 1.\n",
    "\n",
    "$R^2$ is a measure of how much variation in the dependent variable your model explains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://pbs.twimg.com/media/D-Gu7E0WsAANhLY.png' width =\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple linear regression in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in car data\n",
    "df = sns.load_dataset('mpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Applying R^2\n",
    "\n",
    "Let's look at the r^2 value of our SLR model for cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear regression model using statsmodel \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "lr_model = ols(formula='mpg~weight', data=df).fit()\n",
    "\n",
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the R^2 when we add more variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~weight+horsepower+displacement+cylinders+acceleration', data=df).fit()\n",
    "mlr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare it to a model without displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~weight+horsepower+cylinders+acceleration', data=df).fit()\n",
    "mlr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is the Adjusted R-squared?\n",
    "\n",
    "The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n",
    "\n",
    "Suppose you compare a five-predictor model with a higher R-squared to a one-predictor model. Does the five predictor model have a higher R-squared because it’s better? Or is the R-squared higher because it has more predictors? Simply compare the adjusted R-squared values to find out!\n",
    "\n",
    "$$Adjusted R^2=1-\\left(\\frac{n-1}{n-p}\\right)(1-R^2)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "n = sample size   \n",
    "\n",
    "p  = the number of independent variables in the regression equation\n",
    "\n",
    "\n",
    "- The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "\n",
    "- The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. \n",
    "\n",
    "- It is always lower than the R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Model Selection\n",
    "Probabilistic model selection (or “information criteria”) provides an analytical technique for scoring and choosing among candidate models.\n",
    "\n",
    "Models are scored both on their performance on the training dataset and based on the complexity of the model.\n",
    "\n",
    "- **Model Performance:** How well a candidate model has performed on the training dataset.\n",
    "- **Model Complexity:** How complicated the trained candidate model is after training.\n",
    "\n",
    "Model performance may be evaluated using a probabilistic framework, such as log-likelihood under the framework of maximum likelihood estimation. Model complexity may be evaluated as the number of degrees of freedom or parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Akaike Information Criterion vs. Bayesian Information Criterion\n",
    "\n",
    "The model with the lower AIC or BIC should be selected. \n",
    "\n",
    "Despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily.\n",
    "\n",
    "Compared to the BIC method (below), the AIC statistic penalizes complex models less, meaning that it may put more emphasis on model performance on the training dataset, and, in turn, select more complex models.\n",
    "\n",
    "A downside of BIC is that for smaller, less representative training datasets, it is more likely to choose models that are too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/probabilistic-model-selection-measures/\n",
    "\n",
    "https://www.methodology.psu.edu/resources/AIC-vs-BIC/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Machine Learning Process\n",
    "\n",
    "1. Look at the big picture. \n",
    "2. Get the data. \n",
    "3. Discover and visualize the data to gain insights. \n",
    "4. Prepare the data for Machine Learning algorithms. \n",
    "5. Select a model and train it. \n",
    "6. Fine-tune your model. \n",
    "7. Present your solution. \n",
    "8. Launch, monitor, and maintain your system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.kdnuggets.com/wp-content/uploads/crisp-dm-4-problems-fig1.png' width =\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**A proper machine learning workflow includes:**\n",
    "\n",
    "* Separate training and test sets\n",
    "* Trying appropriate algorithms (No Free Lunch)\n",
    "* Fitting model parameters\n",
    "* Tuning impactful hyperparameters\n",
    "* Proper performance metrics\n",
    "* Systematic cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias - Variance \n",
    "\n",
    "There are 3 types of prediction error: bias, variance, and irreducible error.\n",
    "\n",
    "\n",
    "**Total Error = Bias + Variance + Irreducible Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "\n",
    "**Let's do a thought experiment:**\n",
    "\n",
    "1. Imagine you've collected 5 different training sets for the same problem.\n",
    "2. Now imagine using one algorithm to train 5 models, one for each of your training sets.\n",
    "3. Bias vs. variance refers to the accuracy vs. consistency of the models trained by your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='Bias-vs.-Variance-v5-2-darts.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**High bias** algorithms tend to be less complex, with simple or rigid underlying structure.\n",
    "\n",
    "+ They train models that are consistent, but inaccurate on average.\n",
    "+ These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "\n",
    "On the other hand, **high variance** algorithms tend to be more complex, with flexible underlying structure.\n",
    "\n",
    "+ They train models that are accurate on average, but inconsistent.\n",
    "+ These include non-linear or non-parametric algorithms such as decision trees and nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias-Variance Tradeoff\n",
    "\n",
    "This tradeoff in complexity is why there's a tradeoff in bias and variance - an algorithm cannot simultaneously be more complex and less complex.\n",
    "\n",
    "**Total Error = Bias^2 + Variance + Irreducible Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src='Bias-vs.-Variance-v4-chart.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error from Bias\n",
    "\n",
    "**Bias** is the difference between your model's expected predictions and the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='noisy-sine-linear.png' width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error from Variance\n",
    "\n",
    "**Variance** refers to your algorithm's sensitivity to specific sets of training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='noisy-sine-decision-tree.png' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one is overfit and which one is underfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try to find the proper balance of variance and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src='noisy-sine-third-order-polynomial.png' width=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train Test Split\n",
    "\n",
    "**How do we know if our model is overfitting or underfitting?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If our model is not performing well on the training  data, we are probably underfitting it.  \n",
    "\n",
    "\n",
    "To know if our  model is overfitting the data, we need  to test our model on unseen data. \n",
    "We then measure our performance on the unseen data. \n",
    "\n",
    "If the model performs way worse on the  unseen data, it is probably  overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The previous module introduced the idea of dividing your data set into two subsets:\n",
    "\n",
    "* **training set** —a subset to train a model.\n",
    "* **test set**—a subset to test the trained model.\n",
    "\n",
    "You could imagine slicing the single data set as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='testtrainsplit.png' width =550 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Never train on test data.** If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src='https://developers.google.com/machine-learning/crash-course/images/WorkflowWithTestSet.svg' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation Metrics for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](mae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Mean Squared Error** (MSE) is the mean of the squared errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](mse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](rmse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MSE is more popular than MAE because MSE \"punishes\" larger errors. \n",
    "\n",
    "But, RMSE is even more popular than MSE because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "Additionally, I like to divide the RMSE by the standard deviation to  convert it to something similiar to a Z-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practicum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in cleaned movie data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv('cleaned_movie_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature exploration¶\n",
    "\n",
    "In this section I will be investigating different features by plotting them to determine the relationship to SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing data\n",
    "total = df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify my features and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = movie_df.gross\n",
    "\n",
    "features = movie_df[['____', '____', '____', '____', '____', '____']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Split\n",
    "\n",
    "The random state variable makes it so you can always have the same 'random' split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#improt train_test_split from sklearn package\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#call train_test_split on the data and capture the results\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=34,test_size=0.2)\n",
    "\n",
    "#check the shape of the results\n",
    "print(\"Training set - Features: \", X_train.shape, \"Target: \", y_train.shape)\n",
    "print(\"Training set - Features: \", X_test.shape, \"Target: \",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "from sklearn import linear_model\n",
    "\n",
    "#instantiate a linear regression object\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "#fit the linear regression to the data\n",
    "lm = lm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(lm.intercept_)\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well did our model perform\n",
    "\n",
    "Previously we have looked at the R^2 of the model  to  determine  how good of a model this is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"R^2 Score:\", lm.score(X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the metrics module from sklearn\n",
    "from sklearn import metrics\n",
    "\n",
    "train_mae = metrics.mean_absolute_error(y_train, y_train_pred)\n",
    "train_mse = metrics.mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error:', train_mae )\n",
    "print('Mean Squared Error:',  train_mse)\n",
    "print('Root Mean Squared Error:' , train_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_std = target.std()\n",
    "\n",
    "print('Mean Absolute Error:', train_mae/price_std )\n",
    "print('Root Mean Squared Error:' , train_rmse/price_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## The line / model\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(y_pred, y_test, lowess=True, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print (\"Score:\", lm.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "print('Mean Absolute Error:' + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print('Mean Squared Error:' + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('Root Mean Squared Error:' + str(np.sqrt(metrics.mean_squared_error(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error  Z:', test_mae/price_std )\n",
    "print('Root Mean Squared Error Z:' , test_rmse/price_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing our Model's performance on training data versus test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training: ', int(train_rmse), \"vs. Testing: \", int(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
